[
    {
        "content": "<p>Hello,</p>\n<p>I'd like to reduce instantiation time, which is currently ~10s for a 2MB .wasm file.<br>\nAre there safety checks at instantiation (or during execution for that matter) that I could disable to improve performance?</p>\n<p>I'm working with the WasmCert-Coq formalization and have a (WIP) formal proof that the module instantiates according to the spec.</p>",
        "id": 419334445,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1706822028
    },
    {
        "content": "<p>Do you know about <code>wasmtime compile</code>?</p>",
        "id": 419335591,
        "sender_full_name": "Pat Hickey",
        "timestamp": 1706822552
    },
    {
        "content": "<p>also using <code>InstancePre</code> and maybe the pooling allocator will help: <a href=\"https://docs.rs/wasmtime/latest/wasmtime/struct.Linker.html#method.instantiate_pre\">https://docs.rs/wasmtime/latest/wasmtime/struct.Linker.html#method.instantiate_pre</a></p>\n<p>but yeah if you are seeing 10 <em>second</em> \"instantiations\" then I am pretty sure you are measuring compile time as well, and compiling your Wasm modules ahead of time will be the biggest single improvement you can do</p>",
        "id": 419372187,
        "sender_full_name": "fitzgen (he/him)",
        "timestamp": 1706840838
    },
    {
        "content": "<p>Sorry for the late response.</p>\n<p>Thanks for the suggestions!<br>\nYou're right about that it's compilation and not instantiation (which I thought at first) that takes so long,<br>\nso <code>wasmtime compile</code> indeed speeds up things a lot.<br>\nI thus measure startup time now (loading file + compilation + instantiation).</p>\n<p>As we finally have a proper testing setup now, I included some numbers below:</p>\n<ul>\n<li>we measure wasmtime (19.0.1) against wasmtime run (.cwasm file) (19.0.1) and node (v20.11.1).</li>\n<li>we've done a bunch of other improvements, so we're down from the 10s (which I mentioned above) to 2s for the <code>color</code> benchmark</li>\n<li>note that with<code>wasmtime-compile</code>, we can't pretty print the result as that would require importing a function <code>write_char</code>, which we can't provide for <code>wasmtime run</code> it seems. (we don't support wasi-io).</li>\n<li>sizes of the wasm binaries: [demo1: 45KB, demo2: 6KB, list_sum: 8KB, vs_easy: 225KB, vs_hard: 225KB, binom: 228KB, color:1.1MB, sha_fast: 422KB, even_10000: 33KB, ack_3_9: 1KB, sm_gauss_nat: 63KB, sm_gauss_N: 18KB)</li>\n<li>average of 10 runs, we run <code>wasm-opt --coalesce-locals</code> on our binaries first</li>\n</ul>\n<p>thanks!</p>\n<p><a href=\"/user_uploads/15107/S6g9-CcmBxPRvUUzFhcC5u67/benchmarks.png\">benchmarks.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/15107/S6g9-CcmBxPRvUUzFhcC5u67/benchmarks.png\" title=\"benchmarks.png\"><img src=\"/user_uploads/15107/S6g9-CcmBxPRvUUzFhcC5u67/benchmarks.png\"></a></div>",
        "id": 432703136,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1712841152
    },
    {
        "content": "<p>I'd be quite interested to learn why Node is quite a bit better on our benchmarks...?</p>",
        "id": 432705434,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1712841935
    },
    {
        "content": "<p>Node uses V8, which has a multi-phase compilation pipeline: <a href=\"https://v8.dev/docs/wasm-compilation-pipeline\">https://v8.dev/docs/wasm-compilation-pipeline</a></p>",
        "id": 432715952,
        "sender_full_name": "Lann Martin",
        "timestamp": 1712844967
    },
    {
        "content": "<p>This includes a very quick startup baseline compiler (Liftoff) that has \"pretty good\" performance</p>",
        "id": 432716516,
        "sender_full_name": "Lann Martin",
        "timestamp": 1712845132
    },
    {
        "content": "<p>There is a very work-in-progress backend for wasmtime called winch which I believe has similar goals</p>",
        "id": 432716652,
        "sender_full_name": "Lann Martin",
        "timestamp": 1712845174
    },
    {
        "content": "<p>Would you be able to share what you're benchmarking in terms of code/setup/etc? Sounds like the lion's share of improvements, separating compile time from what you're benchmarking, worked well but further improvements may require a bit more careful analysis of what exactly node is doing and how Wasmtime is setup and/or configured.</p>",
        "id": 432717847,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1712845478
    },
    {
        "content": "<p>Sure!<br>\nEverything for testing is in <a href=\"https://github.com/womeier/certicoqwasm-testing\">this repo</a>, this includes the binaries generated by different versions of our compiler (<a href=\"https://github.com/womeier/certicoqwasm-testing/tree/master/evaluation/binaries/non-cps-grow-mem-func-mrch-24-24\">best</a>), <a href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/benchmark.py#L242\">benchmark.py</a> to provide a CLI for benchmarking.<br>\nIt calls <a href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/run-wasmtime.py\">run-wasmtime.py</a> and <a href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/run-node.js\">run-node.js</a> in the same folder, which measure one run. Multiple runs are aggregated by <code>benchmark.py</code>.</p>\n<p>I obtained the above numbers like this (in the folder <code>evaluation</code>):<br>\n<code>$./benchmark.py --folder binaries/non-cps-grow-mem-func-mrch-24-24/ --engine=node --wasm-opt --coalesce-locals</code><br>\n<code>$./benchmark.py --folder binaries/non-cps-grow-mem-func-mrch-24-24/ --engine=wasmtime --wasm-opt --coalesce-locals</code></p>\n<p>Some more background information:</p>\n<ul>\n<li>See <a href=\"https://womeier.de/files/certicoqwasm-coqpl24-abstract.pdf\">here</a> for a short high-level description, most of the limitations are removed by now.</li>\n<li>we generate code in SSA form, that's why you should use <code>--coalesce-locals</code>, in particular for <code>color</code>, the main function has &gt;20k locals</li>\n<li><a href=\"https://github.com/womeier/certicoqwasm/blob/benchmarks_non-cps-grow-mem-func-mrch-24-24/theories/CodegenWasm/LambdaANF_to_Wasm.v#L690\">this</a> is the part of the main function of our compiler that builds the wasm module, every function body is generated by <a href=\"https://github.com/womeier/certicoqwasm/blob/benchmarks_non-cps-grow-mem-func-mrch-24-24/theories/CodegenWasm/LambdaANF_to_Wasm.v#L423\">this</a> function</li>\n<li>the linked setup does <em>not</em> work with --engine=<code>wasmtime-compile</code></li>\n</ul>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/womeier/certicoqwasm-testing\" style=\"background-image: url(https\\:\\/\\/uploads\\.zulipusercontent\\.net\\/824805963a140f28aa60d659b224dc177c932347\\/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f373265386638323230366432313263373138623839303262636336643137333730313334336134623164333035653066383564306239646330643065633131392f776f6d656965722f6365727469636f717761736d2d74657374696e67)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/womeier/certicoqwasm-testing\" title=\"GitHub - womeier/certicoqwasm-testing\">GitHub - womeier/certicoqwasm-testing</a></div><div class=\"message_embed_description\">Contribute to womeier/certicoqwasm-testing development by creating an account on GitHub.</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/womeier/certicoqwasm-testing/tree/master/evaluation/binaries/non-cps-grow-mem-func-mrch-24-24\" style=\"background-image: url(https\\:\\/\\/uploads\\.zulipusercontent\\.net\\/824805963a140f28aa60d659b224dc177c932347\\/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f373265386638323230366432313263373138623839303262636336643137333730313334336134623164333035653066383564306239646330643065633131392f776f6d656965722f6365727469636f717761736d2d74657374696e67)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/womeier/certicoqwasm-testing/tree/master/evaluation/binaries/non-cps-grow-mem-func-mrch-24-24\" title=\"certicoqwasm-testing/evaluation/binaries/non-cps-grow-mem-func-mrch-24-24 at master · womeier/certicoqwasm-testing\">certicoqwasm-testing/evaluation/binaries/non-cps-grow-mem-func-mrch-24-24 at master · womeier/certicoqwasm-testing</a></div><div class=\"message_embed_description\">Contribute to womeier/certicoqwasm-testing development by creating an account on GitHub.</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/benchmark.py#L242\" style=\"background-image: url(https\\:\\/\\/uploads\\.zulipusercontent\\.net\\/824805963a140f28aa60d659b224dc177c932347\\/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f373265386638323230366432313263373138623839303262636336643137333730313334336134623164333035653066383564306239646330643065633131392f776f6d656965722f6365727469636f717761736d2d74657374696e67)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/benchmark.py#L242\" title=\"certicoqwasm-testing/evaluation/benchmark.py at master · womeier/certicoqwasm-testing\">certicoqwasm-testing/evaluation/benchmark.py at master · womeier/certicoqwasm-testing</a></div><div class=\"message_embed_description\">Contribute to womeier/certicoqwasm-testing development by creating an account on GitHub.</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/run-wasmtime.py\" style=\"background-image: url(https\\:\\/\\/uploads\\.zulipusercontent\\.net\\/824805963a140f28aa60d659b224dc177c932347\\/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f373265386638323230366432313263373138623839303262636336643137333730313334336134623164333035653066383564306239646330643065633131392f776f6d656965722f6365727469636f717761736d2d74657374696e67)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/run-wasmtime.py\" title=\"certicoqwasm-testing/evaluation/run-wasmtime.py at master · womeier/certicoqwasm-testing\">certicoqwasm-testing/evaluation/run-wasmtime.py at master · womeier/certicoqwasm-testing</a></div><div class=\"message_embed_description\">Contribute to womeier/certicoqwasm-testing development by creating an account on GitHub.</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/run-node.js\" style=\"background-image: url(https\\:\\/\\/uploads\\.zulipusercontent\\.net\\/824805963a140f28aa60d659b224dc177c932347\\/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f373265386638323230366432313263373138623839303262636336643137333730313334336134623164333035653066383564306239646330643065633131392f776f6d656965722f6365727469636f717761736d2d74657374696e67)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/womeier/certicoqwasm-testing/blob/master/evaluation/run-node.js\" title=\"certicoqwasm-testing/evaluation/run-node.js at master · womeier/certicoqwasm-testing\">certicoqwasm-testing/evaluation/run-node.js at master · womeier/certicoqwasm-testing</a></div><div class=\"message_embed_description\">Contribute to womeier/certicoqwasm-testing development by creating an account on GitHub.</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/womeier/certicoqwasm/blob/benchmarks_non-cps-grow-mem-func-mrch-24-24/theories/CodegenWasm/LambdaANF_to_Wasm.v#L690\" style=\"background-image: url(https\\:\\/\\/uploads\\.zulipusercontent\\.net\\/7a596c1d4640250b67ddae3846738191b6593bcc\\/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f323561646565313231613235323862633931306637643730363732613765666431323239346363653539656533333231393930353565333632646465393136342f776f6d656965722f6365727469636f717761736d)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/womeier/certicoqwasm/blob/benchmarks_non-cps-grow-mem-func-mrch-24-24/theories/CodegenWasm/LambdaANF_to_Wasm.v#L690\" title=\"certicoqwasm/theories/CodegenWasm/LambdaANF_to_Wasm.v at benchmarks_non-cps-grow-mem-func-mrch-24-24 · womeier/certicoqwasm\">certicoqwasm/theories/CodegenWasm/LambdaANF_to_Wasm.v at benchmarks_non-cps-grow-mem-func-mrch-24-24 · womeier/certicoqwasm</a></div><div class=\"message_embed_description\">Contribute to womeier/certicoqwasm development by creating an account on GitHub.</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/womeier/certicoqwasm/blob/benchmarks_non-cps-grow-mem-func-mrch-24-24/theories/CodegenWasm/LambdaANF_to_Wasm.v#L423\" style=\"background-image: url(https\\:\\/\\/uploads\\.zulipusercontent\\.net\\/7a596c1d4640250b67ddae3846738191b6593bcc\\/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f323561646565313231613235323862633931306637643730363732613765666431323239346363653539656533333231393930353565333632646465393136342f776f6d656965722f6365727469636f717761736d)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/womeier/certicoqwasm/blob/benchmarks_non-cps-grow-mem-func-mrch-24-24/theories/CodegenWasm/LambdaANF_to_Wasm.v#L423\" title=\"certicoqwasm/theories/CodegenWasm/LambdaANF_to_Wasm.v at benchmarks_non-cps-grow-mem-func-mrch-24-24 · womeier/certicoqwasm\">certicoqwasm/theories/CodegenWasm/LambdaANF_to_Wasm.v at benchmarks_non-cps-grow-mem-func-mrch-24-24 · womeier/certicoqwasm</a></div><div class=\"message_embed_description\">Contribute to womeier/certicoqwasm development by creating an account on GitHub.</div></div></div>",
        "id": 432841480,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1712905579
    },
    {
        "content": "<p>I have a separate but related, concrete performance issue as well:</p>\n<p>If we, instead of this fragment</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">some_check</span>\n<span class=\"k\">if</span>\n<span class=\"w\">  </span><span class=\"k\">return</span>\n<span class=\"n\">end</span>\n<span class=\"o\">..</span><span class=\"p\">.</span>\n</code></pre></div>\n<p>generate</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">some_check</span>\n<span class=\"n\">br_if</span><span class=\"w\"> </span><span class=\"n\">i</span>\n<span class=\"o\">..</span><span class=\"p\">.</span>\n</code></pre></div>\n<p>we measure the following:</p>\n<ul>\n<li>color benchmark: Node.js is 2x faster with br_if</li>\n<li>color benchmark: wasmtime is &gt;4x slower with br_if</li>\n<li>see numbers below</li>\n<li>the depth <code>i</code> is quite small, typically &lt;= 5</li>\n<li>this is literally the only difference (I checked the diff of the wat files)</li>\n<li>with the br_if, the binaries are (a few KB) smaller</li>\n</ul>\n<p>I am of the opinion that generating the <code>br_if</code> is the correct way to do it, but I don't want to include this change if it makes <code>wasmtime</code> that much slower.</p>\n<p><a href=\"/user_uploads/15107/ocYUH8-tQ7cOQZUBaCLPArdr/br_if.png\">br_if.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/15107/ocYUH8-tQ7cOQZUBaCLPArdr/br_if.png\" title=\"br_if.png\"><img src=\"/user_uploads/15107/ocYUH8-tQ7cOQZUBaCLPArdr/br_if.png\"></a></div>",
        "id": 432874382,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1712917340
    },
    {
        "content": "<p>Ah ok this is interesting! Cranelift is known to not do the same degree of optimizations of other compilers, for example LLVM and v8, and it's generally expected that Cranelift's performance will only be on-par if the input code has been run through an optimizer beforehand. For LLVM-generated code that's typically the case, but for hand-generated code we recommend running through an optimizer like <code>wasm-opt</code> first (with all of its bits and pieces turned on). </p>\n<p>Not to say that what you're finding with <code>br_if</code> vs <code>if return end</code> isn't a bug of course. That'd still be good to fix, but in general it's expected that Cranelift will lose-out performance wise against v8 if the input wasm wasn't itself optimized</p>",
        "id": 432919442,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1712933063
    },
    {
        "content": "<p>FWIW (for Wolfgang), Cranelift has been gaining a bunch of optimization infrastructure over the past few years (and in some benchmarks is seen to be ~at parity with V8); so \"much simpler and needs pre-optimization\" is becoming less true. Strictly speaking the only missing \"fundamental optimization\" is inlining; most everything else one expects from e.g. <code>-O2</code> is there (GVN, LICM, constant prop, alias analysis and related transforms, a bunch of simplification rules).</p>\n<p>That is to say: we're definitely not in the realm of \"extremely simple and limited compiler that will fall down with trivially different branch patterns\", at least that's the expectation! So I'm very curious what's going on above though with the <code>br_if</code> -- <span class=\"user-mention\" data-user-id=\"686788\">@Wolfgang Meier</span> are you sure that the target (<code>i</code> to <code>br_if</code>) is to the outermost block? Or is the branch actually to some tail code? The control-flow graph is technically more complex (many edges into one return-block) so it wouldn't surprise me to see some slowdown due to additional processing, but zeroing in on this could be useful...</p>",
        "id": 432950617,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1712943370
    },
    {
        "content": "<p>Reading over the results, it looks like <code>ack</code> might be the one with the largest discrepancy? I notice as well that you're enabling tail-call and if <code>ack</code> stands for \"ackermann\" then it's known that tail-call historically has had a perf hit with function calls in wasmtime (even non-tail-call ones). That was fixed recently (I believe at least) so recent versions of wasmtime should perform better. Locally with what I think was a development build I saw only very small differences between v8 and wasmtime. </p>\n<p>Do you know what version of wasmtime-py you're using?</p>\n<p>Also, I'll note that with the <code>--preload</code> argument you can get <code>write_{int,char}</code> working with the <code>wasmtime</code> CLI, although you're only able to invoke one function so you can't implement the driver script you've got as part of the wasmtime CLI.</p>\n<p>And finally, are there other benchmarks you're particularly interested in? For example is there another one that you see a large discrepancy on for wasmtime and v8?</p>",
        "id": 432977183,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1712953773
    },
    {
        "content": "<p>Also if you've got a branch with the <code>br_if</code> vs <code>if return end</code> change I can try to poke around that as well and see if I can't see any low-hanging fruit for wasmtime</p>",
        "id": 432980115,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1712955237
    },
    {
        "content": "<p>Thanks for looking into it, much appreciated.</p>\n<p>Quick reply:</p>\n<ul>\n<li>indeed, the br_ifs didn't target the outermost block due to a mistake of mine, however (judging from a quick evaluation)  that was <em>not</em> the reason for the performance drop</li>\n<li>for the evaluations above, I used wasmtime-py 19.0.0</li>\n<li>I haven't experimented too much with wasm-opt, but have a few more ideas to try</li>\n</ul>\n<p>I'll experiment some more and report again later in the week...</p>",
        "id": 433059531,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1713025553
    },
    {
        "content": "<p>Quick update, regarding <em>br_if</em> vs <em>return</em>.<br>\nI tried a bunch of things in our code generation, that didn't amount to anything, also:</p>\n<ul>\n<li><em>wasm-opt -O2</em> didn't help</li>\n<li>updating to wasmtime 21.0.0 didn't help</li>\n</ul>\n<p>We indeed have branches for these two versions, but currently no documentation on how to set them up.<br>\nI'll have some time in a month to look into it some more.</p>",
        "id": 443526630,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1717883544
    },
    {
        "content": "<p>We spent a bit of time optimizing our compiler, here is a bar plot comparing Wasmtime against Node.js.</p>\n<ul>\n<li>Wasmtime 24.0.0 (same for Wasmtime-compile)</li>\n<li>Wasmtime is unfortunately still much slower than Node.js (we care about Wasmtime performance more than Node.js :/)</li>\n<li>The Ackermann benchmark is just 1 KB, so compilation time is negligible</li>\n<li>We didn't include the change discussed above, we still have return instructions instead of br_if. (but we don't intend to add it for now)</li>\n<li>color and sha_fast are larger than our typical programs, but it'd still be nice if they weren't that bad</li>\n</ul>\n<p>Was hoping for some more ideas, on what we could try?<br>\n(figured a bar plot would be more helpful than the table with raw data)</p>\n<p><a href=\"/user_uploads/15107/s1Tn3nlSiBjQLokRoonISwET/wasmtime_nodejs.png\">wasmtime_nodejs.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/15107/s1Tn3nlSiBjQLokRoonISwET/wasmtime_nodejs.png\" title=\"wasmtime_nodejs.png\"><img data-original-dimensions=\"537x760\" src=\"/user_uploads/thumbnail/15107/s1Tn3nlSiBjQLokRoonISwET/wasmtime_nodejs.png/840x560.webp\"></a></div>",
        "id": 472570200,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1727210457
    },
    {
        "content": "<p>What is the y-axis unit (there is no label)?</p>",
        "id": 472571555,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1727211027
    },
    {
        "content": "<p>And what is the distinction between wasmtime and wasmtime-compile? Are you doing separate AOT compilation in the former? (Why isn't the non-hatched part of the bar exactly equivalent between the two in that case?)</p>",
        "id": 472571700,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1727211105
    },
    {
        "content": "<p>as far as \"why is it slow to compile\", this question is fairly impossible to answer without more detail -- have you tried profiling, and observing where the time is going within Wasmtime? If Cranelift, where in Cranelift?</p>",
        "id": 472571862,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1727211154
    },
    {
        "content": "<blockquote>\n<p>Wasmtime is unfortunately still much slower than Node.js</p>\n</blockquote>\n<p>are you still measuring all of process creation, wasm compilation, and wasm instantiation?</p>\n<p>in general, no one has put in work to optimize how long it takes a wasmtime process to start up because no using wasmtime in production has needed that yet. doesn't mean we wouldn't appreciate PRs improving the current state of things, just that no one has really looked at it or has any need to improve it.</p>\n<p>fwiw, wasmtime's happy path for low latency start up, which has been optimized a <em>ton</em> because it is what production users actually do, is roughly the following:</p>\n<ul>\n<li>compile <code>.cwasm</code> offline</li>\n<li>daemon embedding wasmtime constantly running</li>\n<li>load <code>wasmtime::Module</code>s from the offline-compiled <code>.cwasm</code>s</li>\n<li>create <code>wasmtime::InstancePre</code>s for those modules, early-binding their imports so that we don't have to do string lookups at instantiation time</li>\n<li>when new work comes in, eg an http request, instantiate via <code>instance_pre.instantiate(...)</code></li>\n</ul>\n<p>we also have <a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/benches/instantiation.rs\">benchmarks</a> in the repo that you can run via <code>cargo bench --bench instantiate</code>. these benchmarks roughly reflect this shape of workload. last time I ran them on my laptop, I was seeing ~5 microseconds per instantiation, regardless of the size of the wasm binary</p>\n<p>on the other hand, if you want to reduce <em>compile</em> time, I'd suggest looking into using Winch (e.g. <code>-C compiler=winch</code> in the CLI). it is a single-pass \"baseline\" compiler comparable to V8's Liftoff tier. (as mentioned before, if you are comparing node and wasmtime in compile-and-instantiate, then you are comparing apples and oranges unless you switch wasmtime to using Winch or force node to skip Liftoff and go straight to its optimizing tier, somehow)</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/bytecodealliance/wasmtime/blob/main/benches/instantiation.rs\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/1d541bfde0b8fdb53a2ca71dc9fb0eab98a4be3e/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f653837343735393935366364393662656565623236336636653166376166393866663764636636343765653035363063373464616335316666643532663839352f62797465636f6465616c6c69616e63652f7761736d74696d65&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/benches/instantiation.rs\" title=\"wasmtime/benches/instantiation.rs at main · bytecodealliance/wasmtime\">wasmtime/benches/instantiation.rs at main · bytecodealliance/wasmtime</a></div><div class=\"message_embed_description\">A fast and secure runtime for WebAssembly. Contribute to bytecodealliance/wasmtime development by creating an account on GitHub.</div></div></div>",
        "id": 472573197,
        "sender_full_name": "fitzgen (he/him)",
        "timestamp": 1727211702
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"254389\">@Chris Fallin</span>  time in ms. Yes, they are separate: <code>Wasmtime</code> uses the python api, <code>Wasmtime-compile</code> is a pre-compiled <code>.cwasm</code> file, that we run with <code>wasmtime run</code>.</p>\n<p>Good point, I'll look more into profiling, thanks!</p>\n<p><span class=\"user-mention\" data-user-id=\"253990\">@fitzgen (he/him)</span> </p>\n<ul>\n<li>\n<p>I'm happy with pre-compiling to <code>.cwasm</code>, that makes total sense.<br>\n   We just couldn't use <code>wasmtime run</code> previously because we had some custom function imports (which we got rid of <br>\n   now.)<br>\n   And the python bindings don't (yet?) support <code>.cwasm</code> files</p>\n</li>\n<li>\n<p>The title of this thread is perhaps misleading, it's just compilation time, not instantiation time.</p>\n</li>\n<li>I'll look into winch again, thanks!</li>\n</ul>",
        "id": 472577601,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1727213809
    },
    {
        "content": "<blockquote>\n<p>The title of this thread is perhaps misleading, it's just compilation time, not instantiation time.</p>\n</blockquote>\n<p>I renamed the thread to reflect this</p>",
        "id": 472577755,
        "sender_full_name": "fitzgen (he/him)",
        "timestamp": 1727213889
    },
    {
        "content": "<p>For benchmarking purposes, you should be able to invoke <code>wasmtime compile</code> independently of the Python API. If you have surprisingly slow compilations, we're definitely interested; for any report to be actionable by us, though, it would either need profiling output and general info about the input (\"this callstack in Cranelift is slow with input of this shape\"), or ideally an actual .wasm file we can reproduce the issue with</p>",
        "id": 472577863,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1727213976
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"686788\">Wolfgang Meier</span> <a href=\"#narrow/stream/206238-general/topic/reduce.20compilation.20time/near/472577601\">said</a>:</p>\n<blockquote>\n<p>And the python bindings don't (yet?) support <code>.cwasm</code> files</p>\n</blockquote>\n<p>For this I think you can use <code>Module.deserialize{,_file}</code> methods, but if you run into issues with those feel free to file an issue and/or feature request on wasmtime-py!</p>",
        "id": 472577982,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1727214033
    },
    {
        "content": "<p>It's worth mentioning though that most optimization work in Wasmtime has been focused on the Rust-based <code>wasmtime</code> crate, so for example <code>InstancePre</code> isn't part of the Python API (yet)</p>",
        "id": 472578036,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1727214075
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"254389\">Chris Fallin</span> <a href=\"#narrow/stream/206238-general/topic/reduce.20compilation.20time/near/472577863\">said</a>:</p>\n<blockquote>\n<p>For benchmarking purposes, you should be able to invoke <code>wasmtime compile</code> independently of the Python API.</p>\n</blockquote>\n<p>Yes, that's what we do.<br>\nWe're mostly happy with <code>wasmtime compile</code> and <code>wasmtime run</code> for now, I think.</p>",
        "id": 472578442,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1727214313
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"253994\">Alex Crichton</span> <a href=\"#narrow/stream/206238-general/topic/reduce.20compilation.20time/near/472577982\">said</a>:</p>\n<blockquote>\n<p>For this I think you can use <code>Module.deserialize{,_file}</code> methods, but if you run into issues with those feel free to file an issue and/or feature request on wasmtime-py!</p>\n</blockquote>\n<p>I'll look into this.</p>\n<p>But I see now that our benchmarking setup is quite different from what you'd actually use in production</p>",
        "id": 472578747,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1727214485
    },
    {
        "content": "<p>Thank you so much for your help!</p>",
        "id": 472578827,
        "sender_full_name": "Wolfgang Meier",
        "timestamp": 1727214552
    },
    {
        "content": "<p>In the future, if you're curious, custom host functions can sort of be supported through <code>--preload</code> on the CLI. That'll load a module and use its exports as imports, so you could define your own custom functions in terms of WASI doing that, for example. That doesn't work well for passing chunks of memory (like strings) to imports though. Also if you've otherwise removed the need for host functions that's additionally not as applicable, but figured I could note here at least.</p>",
        "id": 472578981,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1727214638
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"253994\">@Alex Crichton</span>  a quick question: is 100ms of compilation time (without parallel compilation, None optimization level) typical for a 160KB WASM (generated from Rust code in release mode) that does simple SIMD bitunpacking? My use case <em>cannot</em> use AOT compilation so I just want to know if this overhead is expected and is there any way to optimize it. Thanks</p>",
        "id": 480304441,
        "sender_full_name": "Xinyu Zeng",
        "timestamp": 1730637536
    },
    {
        "content": "<p>And I did not find the info in this thread workable for me :(. But the number in figure by <span class=\"user-mention\" data-user-id=\"686788\">@Wolfgang Meier</span> matches my experiments. (the vs_easy and vs_hard)</p>",
        "id": 480304643,
        "sender_full_name": "Xinyu Zeng",
        "timestamp": 1730637731
    },
    {
        "content": "<p>Whether or not it's typical depends on a lot of factors, e.g. how powerful the cpu is and how big the functions are. I'm not sure many of us are benchmarking on single-threaded compiles ourselves. Would you be able to share the module (or a similar-ish module) so we can test locally? Also have you tried using Winch?</p>",
        "id": 480312220,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1730644840
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"253994\">Alex Crichton</span> <a href=\"#narrow/channel/206238-general/topic/reduce.20compilation.20time/near/480312220\">said</a>:</p>\n<blockquote>\n<p>Whether or not it's typical depends on a lot of factors, e.g. how powerful the cpu is and how big the functions are. I'm not sure many of us are benchmarking on single-threaded compiles ourselves. Would you be able to share the module (or a similar-ish module) so we can test locally? Also have you tried using Winch?</p>\n</blockquote>\n<p>Thanks for reply! The code contains V128 and winch does not support it yet. The code is a wrapper around <a href=\"https://github.com/spiraldb/fastlanes\">https://github.com/spiraldb/fastlanes</a> and the machine is Intel(R) Xeon(R) Platinum 8474C</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/spiraldb/fastlanes\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/cee0768bcea55328a5d25e3d5296e734c7ec0286/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f376665373766613863643039386336313939353839356139356662326337333764623734333631616566386434323832376566623836356631333138653465642f73706972616c64622f666173746c616e6573&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/spiraldb/fastlanes\" title=\"GitHub - spiraldb/fastlanes: Rust implementation of the FastLanes compression library\">GitHub - spiraldb/fastlanes: Rust implementation of the FastLanes compression library</a></div><div class=\"message_embed_description\">Rust implementation of the FastLanes compression library - spiraldb/fastlanes</div></div></div>",
        "id": 480396894,
        "sender_full_name": "Xinyu Zeng",
        "timestamp": 1730711418
    },
    {
        "content": "<p>Ah ok yeah our general solution for \"you want fast compiles\" is indeed not applicable here since Winch doesn't fully support simd yet.</p>\n<p>Would you be able to share the wasm binary you're working with?</p>",
        "id": 480500546,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1730744037
    },
    {
        "content": "<p>I have a copy here: <a href=\"https://drive.google.com/file/d/1jtWsDfDEh_uADDqhem4besFAi8NaMnzA/view?usp=sharing\">https://drive.google.com/file/d/1jtWsDfDEh_uADDqhem4besFAi8NaMnzA/view?usp=sharing</a></p>",
        "id": 480639087,
        "sender_full_name": "Xinyu Zeng",
        "timestamp": 1730791390
    },
    {
        "content": "<p>Poking around at this I don't see anything out of the ordinary myself, so what you're seeing is probably expected.</p>",
        "id": 480749743,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1730821301
    },
    {
        "content": "<p>Ok, thanks!</p>",
        "id": 480833037,
        "sender_full_name": "Xinyu Zeng",
        "timestamp": 1730860241
    }
]