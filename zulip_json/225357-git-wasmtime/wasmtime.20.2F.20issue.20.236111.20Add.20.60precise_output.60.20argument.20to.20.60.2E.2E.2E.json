[
    {
        "content": "<p>jameysharp <a href=\"https://github.com/bytecodealliance/wasmtime/pull/6111#issuecomment-1486085107\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/6111\">issue #6111</a>:</p>\n<blockquote>\n<p>This is neat! It's clearly handy to be able to regenerate test expectations like this.</p>\n<p>I don't know for sure that precise output tests are a good idea for optimizations though. Writing assertions about only the specific aspects we're trying to test makes the tests more resilient to unrelated changes in the optimizer.</p>\n<p>I'm curious if @cfallin or @fitzgen have opinions on this.</p>\n</blockquote>",
        "id": 345006313,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1679967620
    },
    {
        "content": "<p>Kmeakin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/6111#issuecomment-1486105007\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/6111\">issue #6111</a>:</p>\n<blockquote>\n<blockquote>\n<p>I don't know for sure that precise output tests are a good idea for optimizations though. Writing assertions about only the specific aspects we're trying to test makes the tests more resilient to unrelated changes in the optimizer.</p>\n</blockquote>\n<p>Could we do a script to automatically insert the correct <code>filecheck</code> commands? LLVM has something similar: <a href=\"https://github.com/llvm/llvm-project/blob/main/llvm/utils/update_any_test_checks.py\">https://github.com/llvm/llvm-project/blob/main/llvm/utils/update_any_test_checks.py</a></p>\n</blockquote>",
        "id": 345009207,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1679969489
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/6111#issuecomment-1487260845\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/6111\">issue #6111</a>:</p>\n<blockquote>\n<blockquote>\n<p>I don't know for sure that precise output tests are a good idea for optimizations though. Writing assertions about only the specific aspects we're trying to test makes the tests more resilient to unrelated changes in the optimizer.</p>\n<p>I'm curious if @cfallin or @fitzgen have opinions on this.</p>\n</blockquote>\n<p>Yes, I think we want to retain this property: many of the tests are checking just \"this one value becomes X\" rather than freezing the entire output. There is still some unnecessary capture of implementation details (e.g. the value number in the output exposes how many rewrites we went through) but less coupling is better here.</p>\n<p>It would be nice to try to find ways to automate updates though!</p>\n</blockquote>",
        "id": 345187314,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1680021562
    },
    {
        "content": "<p>fitzgen <a href=\"https://github.com/bytecodealliance/wasmtime/pull/6111#issuecomment-1487318746\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/6111\">issue #6111</a>:</p>\n<blockquote>\n<p>Yeah for tests that are asserting one particular peephole applied, it is nice to not have the full precise output. There are certainly other tests that are written in a less targeted way, and could be migrated to precise output tests on a case-by-case basis. Because of this, it would be nice to support, even if we don't blanket enable it for every <code>test optimize</code> test.</p>\n</blockquote>",
        "id": 345196455,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1680024048
    }
]