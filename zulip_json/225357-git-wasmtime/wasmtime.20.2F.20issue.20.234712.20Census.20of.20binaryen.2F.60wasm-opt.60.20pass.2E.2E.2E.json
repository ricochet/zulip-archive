[
    {
        "content": "<p>fitzgen labeled <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712\">issue #4712</a>:</p>\n<blockquote>\n<p>What is Cranelift's job (in the context of Wasmtime)? To take Wasm that is produced by LLVM and already optimized 99% of the time and do the architecture-specific code generation that LLVM cannot do when targeting Wasm (e.g. instruction selection, regalloc). We don't want to duplicate <em>all</em> of LLVM's mid-end optimizations, only the ones that are beneficial for cleaning up and improving code after we've lowered Wasm memory operations into raw base + offset memory operations, etc. This is an interesting place for a compiler to be, and it means the set of passes and trade offs we have are different from what one might assume by default.</p>\n<p>There is another compiler that is in a similar space, at least as far as consuming already-optimized-by-LLVM Wasm binaries and attempting to further optimize them: binaryen and <code>wasm-opt</code>. The big difference is that <code>wasm-opt</code> is emitting another Wasm binary while we are emitting machine code. But maybe they have passes that are not specific to targeting Wasm and which are beneficial to run on already-optimized-by-LLVM Wasm binaries? AIUI, the LLVM IR to Wasm lowering introduces some suboptimal code patterns.</p>\n<p>So I did an informal census of what passes are run by <code>wasm-opt</code>, filtering out anything that looked overly specific to targeting Wasm. Results are summarized in the table below, and might give us some food for thought as we start looking into Cranelift's code quality some more. (FWIW, I wasn't 100% sure about some things below, so if you see something that you know is incorrect, feel free to edit this issue and correct it!)</p>\n<table>\n<thead>\n<tr>\n<th>Pass</th>\n<th>Description</th>\n<th>Cranelift has equivalent?</th>\n<th>Discussion</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>local-cse</td>\n<td>Perform common-subexpression elimination within a block</td>\n<td><strong>Yes</strong></td>\n<td>Our GVN should cover all of this.</td>\n</tr>\n<tr>\n<td>dce</td>\n<td>Perform dead code elimination to remove unreachable blocks and unused expressions</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>optimize-instructions</td>\n<td>Peephole optimizations</td>\n<td><strong>Partial</strong></td>\n<td>We have some peepholes, but not as many as <code>wasm-opt</code>, and could definitely add more. Although, they care a lot about Wasm encoding tricks for peepholes where we do not. Probably better to look at LLVM itself here for inspiration. Finally, they also have some Souper-synthesized peepholes, and we should really add some of our own once the e-graphs work lands.</td>\n</tr>\n<tr>\n<td>pick-load-signs</td>\n<td>Look at uses of a load to determine whether to use sign extension or zero extension for the load (e.g. if the load instruction is <code>i32.load8_u</code> but a majority of uses are prefixed with <code>i32.extend8_s</code>, then change the load to <code>i32.load8_s</code>, remove the now-redundant sign extends from the majority of uses, and insert zero extends for the other uses.)</td>\n<td><strong>No</strong></td>\n<td>Unclear how much this is worth it in practice, especially if our primary goal is code speed rather than code size.</td>\n</tr>\n<tr>\n<td>precompute[-propagate]</td>\n<td>Constant propagation and folding</td>\n<td><strong>Partial</strong></td>\n<td>We have a couple peepholes in <code>simple_preopt</code> that do some of this, but only 2 levels deep. Should investigate doing this more completely once the WIP e-graphs work merges.</td>\n</tr>\n<tr>\n<td>code-pushing</td>\n<td>Push defs down towards uses. Might move the def into a block on the other side of a conditional, making it so that it is not executed unless needed.</td>\n<td><strong>No</strong></td>\n<td>Unsure whether their pass is aware of loop boundaries, and whether this might \"undo\" some manual LICM the programmer/LLVM did (this comes before their LICM in their phase ordering; our LICM won't create partially dead code, fwiw.) Although maybe we start (or can start) doing this with the new e-graphs work?</td>\n</tr>\n<tr>\n<td>code-folding</td>\n<td>Merge common tails of all of a block's predecessors into the block itself.</td>\n<td><strong>No</strong></td>\n<td>I don't believe we do any block-level optimizations that look at multiple predecessors or multiple successors at the same time (i.e. we can merge one successor block into its sole predecessor). FWIW, I don't see a dual pass for merging common heads of successor blocks into their predecessor block in <code>wasm-opt</code>, but that seems like an obvious thing to implement if you've implemented merging common tails of successor blocks. Totally possible it exists and I missed it.</td>\n</tr>\n<tr>\n<td>merge-blocks</td>\n<td>Sort of Wasm-specific, but essentially merge a block into its sole predecessor.</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>duplicate-function-elimination</td>\n<td>Interprocedural optimization to deduplicate identical functions.</td>\n<td><strong>No</strong></td>\n<td>When the new incremental caching infra is enabled, I <em>guess</em> we <em>could</em> get this for free? But also depends on implementation (which I am not personally familiar with) and order of function compilation scheduling in the face of our parallelism.</td>\n</tr>\n<tr>\n<td>inlining</td>\n<td>Inline a callee function into its caller, removing function call overhead and, more importantly, providing opportunity for more optimization based on the actual arguments to the call.</td>\n<td><strong>No</strong></td>\n<td>We probably don't want this for regular core Wasm modules right now, since LLVM did all the profitable inlining already and has way better heuristics than anything we are going to come up with on the first try. If something is both small and not inlined into callers by the time we see it, then it was probably either marked no-inline or cold or something like that and we just don't have those annotations anymore. However, with the component model this is going to change: callees will remain a black box until after component linking time (so LLVM won't ever have had a chance to inline these calls) and we will have lots of oppotunities to do some nice cross-module inlining ourselves.</td>\n</tr>\n<tr>\n<td>directize</td>\n<td>Turn <code>call_indirect</code>s into <code>call</code>s. Devirtualization.</td>\n<td><strong>No</strong></td>\n<td>Probably not profitable for us to do this, since LLVM already does it, but could be very profitable when done optimistically in concert with PGO data and then inline the callee into the caller.</td>\n</tr>\n</tbody>\n</table>\n</blockquote>",
        "id": 293610253,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660592857
    },
    {
        "content": "<p>fitzgen opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712\">issue #4712</a>:</p>\n<blockquote>\n<p>What is Cranelift's job (in the context of Wasmtime)? To take Wasm that is produced by LLVM and already optimized 99% of the time and do the architecture-specific code generation that LLVM cannot do when targeting Wasm (e.g. instruction selection, regalloc). We don't want to duplicate <em>all</em> of LLVM's mid-end optimizations, only the ones that are beneficial for cleaning up and improving code after we've lowered Wasm memory operations into raw base + offset memory operations, etc. This is an interesting place for a compiler to be, and it means the set of passes and trade offs we have are different from what one might assume by default.</p>\n<p>There is another compiler that is in a similar space, at least as far as consuming already-optimized-by-LLVM Wasm binaries and attempting to further optimize them: binaryen and <code>wasm-opt</code>. The big difference is that <code>wasm-opt</code> is emitting another Wasm binary while we are emitting machine code. But maybe they have passes that are not specific to targeting Wasm and which are beneficial to run on already-optimized-by-LLVM Wasm binaries? AIUI, the LLVM IR to Wasm lowering introduces some suboptimal code patterns.</p>\n<p>So I did an informal census of what passes are run by <code>wasm-opt</code>, filtering out anything that looked overly specific to targeting Wasm. Results are summarized in the table below, and might give us some food for thought as we start looking into Cranelift's code quality some more. (FWIW, I wasn't 100% sure about some things below, so if you see something that you know is incorrect, feel free to edit this issue and correct it!)</p>\n<table>\n<thead>\n<tr>\n<th>Pass</th>\n<th>Description</th>\n<th>Cranelift has equivalent?</th>\n<th>Discussion</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>local-cse</td>\n<td>Perform common-subexpression elimination within a block</td>\n<td><strong>Yes</strong></td>\n<td>Our GVN should cover all of this.</td>\n</tr>\n<tr>\n<td>dce</td>\n<td>Perform dead code elimination to remove unreachable blocks and unused expressions</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>optimize-instructions</td>\n<td>Peephole optimizations</td>\n<td><strong>Partial</strong></td>\n<td>We have some peepholes, but not as many as <code>wasm-opt</code>, and could definitely add more. Although, they care a lot about Wasm encoding tricks for peepholes where we do not. Probably better to look at LLVM itself here for inspiration. Finally, they also have some Souper-synthesized peepholes, and we should really add some of our own once the e-graphs work lands.</td>\n</tr>\n<tr>\n<td>pick-load-signs</td>\n<td>Look at uses of a load to determine whether to use sign extension or zero extension for the load (e.g. if the load instruction is <code>i32.load8_u</code> but a majority of uses are prefixed with <code>i32.extend8_s</code>, then change the load to <code>i32.load8_s</code>, remove the now-redundant sign extends from the majority of uses, and insert zero extends for the other uses.)</td>\n<td><strong>No</strong></td>\n<td>Unclear how much this is worth it in practice, especially if our primary goal is code speed rather than code size.</td>\n</tr>\n<tr>\n<td>precompute[-propagate]</td>\n<td>Constant propagation and folding</td>\n<td><strong>Partial</strong></td>\n<td>We have a couple peepholes in <code>simple_preopt</code> that do some of this, but only 2 levels deep. Should investigate doing this more completely once the WIP e-graphs work merges.</td>\n</tr>\n<tr>\n<td>code-pushing</td>\n<td>Push defs down towards uses. Might move the def into a block on the other side of a conditional, making it so that it is not executed unless needed.</td>\n<td><strong>No</strong></td>\n<td>Unsure whether their pass is aware of loop boundaries, and whether this might \"undo\" some manual LICM the programmer/LLVM did (this comes before their LICM in their phase ordering; our LICM won't create partially dead code, fwiw.) Although maybe we start (or can start) doing this with the new e-graphs work?</td>\n</tr>\n<tr>\n<td>code-folding</td>\n<td>Merge common tails of all of a block's predecessors into the block itself.</td>\n<td><strong>No</strong></td>\n<td>I don't believe we do any block-level optimizations that look at multiple predecessors or multiple successors at the same time (i.e. we can merge one successor block into its sole predecessor). FWIW, I don't see a dual pass for merging common heads of successor blocks into their predecessor block in <code>wasm-opt</code>, but that seems like an obvious thing to implement if you've implemented merging common tails of successor blocks. Totally possible it exists and I missed it.</td>\n</tr>\n<tr>\n<td>merge-blocks</td>\n<td>Sort of Wasm-specific, but essentially merge a block into its sole predecessor.</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>duplicate-function-elimination</td>\n<td>Interprocedural optimization to deduplicate identical functions.</td>\n<td><strong>No</strong></td>\n<td>When the new incremental caching infra is enabled, I <em>guess</em> we <em>could</em> get this for free? But also depends on implementation (which I am not personally familiar with) and order of function compilation scheduling in the face of our parallelism.</td>\n</tr>\n<tr>\n<td>inlining</td>\n<td>Inline a callee function into its caller, removing function call overhead and, more importantly, providing opportunity for more optimization based on the actual arguments to the call.</td>\n<td><strong>No</strong></td>\n<td>We probably don't want this for regular core Wasm modules right now, since LLVM did all the profitable inlining already and has way better heuristics than anything we are going to come up with on the first try. If something is both small and not inlined into callers by the time we see it, then it was probably either marked no-inline or cold or something like that and we just don't have those annotations anymore. However, with the component model this is going to change: callees will remain a black box until after component linking time (so LLVM won't ever have had a chance to inline these calls) and we will have lots of oppotunities to do some nice cross-module inlining ourselves.</td>\n</tr>\n<tr>\n<td>directize</td>\n<td>Turn <code>call_indirect</code>s into <code>call</code>s. Devirtualization.</td>\n<td><strong>No</strong></td>\n<td>Probably not profitable for us to do this, since LLVM already does it, but could be very profitable when done optimistically in concert with PGO data and then inline the callee into the caller.</td>\n</tr>\n</tbody>\n</table>\n</blockquote>",
        "id": 293610255,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660592858
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712#issuecomment-1215742483\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712\">issue #4712</a>:</p>\n<blockquote>\n<p>Thanks for this in-depth look! This is a really valuable comparison.</p>\n<p>I agree that Cranelift's goal in context as a backend for Wasmtime, with today's Wasm ecosystem, largely results in an optimization design-space that discounts traditional heavyweight analysis and calls for more directed work. I sort of touched on this recently <a href=\"https://github.com/bytecodealliance/rfcs/pull/27#issuecomment-1210040665\">in this comment</a> with a from-first-principles breakdown of how <em>should</em> expect Cranelift to be able to optimize further as coming from (i) Wasm-to-CLIF semantic gap, (ii) CLIF-to-machine semantic gap, and (iii) regalloc quality. The meat of the (remaining) issue is in (i) and that's what you're pointing to with \"cleaning up and improving code after we've lowered Wasm memory operations\"; so, fully agreed.</p>\n<p>I do also want to emphasize your point about inlining and the component model: late-binding two modules together, that have not met before and were compiled separately, <em>completely</em> changes the tradeoffs in that it means all of the traditional LLVM-style heavyweight analyses will find low-hanging fruit when inlining across that boundary. For that reason, and for the reason that Cranelift is also used (and hopefully used more in the future) in non-Wasm contexts, I think it's valuable to keep thinking about the heavyweight analyses even if they are turned off in a one-Wasm-module context.</p>\n<p>(I suspect that having different meta-settings or \"opt levels\" would make sense here, with a preset for \"one Wasm module, likely produced by an optimizing compiler\" and a preset for \"Wasm components with multiple modules linking together\" and a preset for \"unoptimizing frontend driving Cranelift\".)</p>\n<p>On the particular opts (aside from <code>inlining</code> covered above):</p>\n<ul>\n<li><code>precompute-propagate</code> and <code>code-pushing</code> should both be subsumed by the mid-end optimizer with its already-existing work in the prototype: in particular, the constant folding is more complete (follows an arbitrarily-long chain) and the code-motion allowed by scoped elaboration naturally pushes defs down when they are \"partially redundant\" (not used on some paths following computation).</li>\n<li>we don't have anything like <code>code-folding</code>; I'd be curious to see how often it occurs in already-optimized modules. It should primarily benefit code size, and only indirectly speed (by reducing icache footprint); so useful if it applies but IMHO not likely to produce huge gains. I suspect this is an artifact of <code>wasm-opt</code>'s focus on module size as well as speed.</li>\n<li><code>duplicate-function-elimination</code> should indeed fall out of the incremental compilation cache; in a mode where we care about maximizing this we could do a MapReduce-style thing where we (i) generate IR and compute cache keys for <em>every</em> function, then (ii) sort and deduplicate, then (iii) compile only once for each cache key. Thoughts @bnjbvr?</li>\n<li>\n<p><code>directize</code>: yes absolutely, once we inline across multiple modules, I suspect this will be a very important optimization. One difficulty is that it is generally best as an interprocedural analysis, as one can reach stronger conclusions about constant function pointers this way (e.g., not only \"I directly stored this function pointer\" case, but \"I read this field and across the program, the only function pointer stored into this field is this value\" cases). That might require a bit of scaffolding to do properly.</p>\n</li>\n<li>\n<p>Not covered above because out-of-scope for Wasm-to-Wasm (<code>wasm-opt</code>), but IMHO still important for us to build: bounds-check elimination and the value-range analysis that can feed into it.</p>\n</li>\n</ul>\n<p>That's all I can think of for now, but we should dump more thoughts about building any of the above here and/or split out specific issues as needed. (Also at least inlining has #4127 but I don't know if the others do already.) Thanks again for the survey!</p>\n</blockquote>",
        "id": 293613861,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660594416
    },
    {
        "content": "<p>bnjbvr <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712#issuecomment-1216477403\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712\">issue #4712</a>:</p>\n<blockquote>\n<blockquote>\n<p>duplicate-function-elimination should indeed fall out of the incremental compilation cache; in a mode where we care about maximizing this we could do a MapReduce-style thing where we (i) generate IR and compute cache keys for every function, then (ii) sort and deduplicate, then (iii) compile only once for each cache key. Thoughts @bnjbvr?</p>\n</blockquote>\n<p>Indeed we should get this for free as long as the <code>FunctionStencil</code>s are exactly the same for two different functions (think newtypes, (proc-)macro-generated code like cranelift-entity's <code>entity_impl!</code> or <code>derive(serde)</code>, etc. tend to create lots of duplicated code).</p>\n<p>Now this is the first time I hear of this MapReduce-style idea, and IIUC this is semantically equivalent to what we have right now, just changes the order of operations: in the existing implementation, a <code>CacheStore</code> would do the deduplication by reusing hashed entries (and likely run into race conditions which handling is deferred to the cache store impl), while in the proposal the deduplication would happen earlier, and then only the <code>CacheStore</code> would be probed for existing entries.</p>\n<p>If I'm not mistaken, the MapReduce approach makes parallel compilation of functions a bit harder, as it requires precomputing all the cache keys for all functions in a module, thus reading every function's body before compiling them. I don't imagine that we expect performing streaming compilation using Cranelift any time soon.</p>\n<p>Right now we can compile all functions in a module, in parallel, and let the <code>CacheStore</code> implementation handle races via locking. To explicit what I mean here: two duplicated functions may start getting compiled at the same time, the <code>CacheStore</code> wouldn't see existing cache entries for both, so it could compile both and store the same compiled artifact twice. The MapReduce approach would prevent all such race conditions in the <code>CacheStore</code>, by providing a set of unique cache keys in the first place.</p>\n</blockquote>",
        "id": 293693172,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660647358
    },
    {
        "content": "<p>akirilov-arm labeled <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712\">issue #4712</a>:</p>\n<blockquote>\n<p>What is Cranelift's job (in the context of Wasmtime)? To take Wasm that is produced by LLVM and already optimized 99% of the time and do the architecture-specific code generation that LLVM cannot do when targeting Wasm (e.g. instruction selection, regalloc). We don't want to duplicate <em>all</em> of LLVM's mid-end optimizations, only the ones that are beneficial for cleaning up and improving code after we've lowered Wasm memory operations into raw base + offset memory operations, etc. This is an interesting place for a compiler to be, and it means the set of passes and trade offs we have are different from what one might assume by default.</p>\n<p>There is another compiler that is in a similar space, at least as far as consuming already-optimized-by-LLVM Wasm binaries and attempting to further optimize them: binaryen and <code>wasm-opt</code>. The big difference is that <code>wasm-opt</code> is emitting another Wasm binary while we are emitting machine code. But maybe they have passes that are not specific to targeting Wasm and which are beneficial to run on already-optimized-by-LLVM Wasm binaries? AIUI, the LLVM IR to Wasm lowering introduces some suboptimal code patterns.</p>\n<p>So I did an informal census of what passes are run by <code>wasm-opt</code>, filtering out anything that looked overly specific to targeting Wasm. Results are summarized in the table below, and might give us some food for thought as we start looking into Cranelift's code quality some more. (FWIW, I wasn't 100% sure about some things below, so if you see something that you know is incorrect, feel free to edit this issue and correct it!)</p>\n<table>\n<thead>\n<tr>\n<th>Pass</th>\n<th>Description</th>\n<th>Cranelift has equivalent?</th>\n<th>Discussion</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>local-cse</td>\n<td>Perform common-subexpression elimination within a block</td>\n<td><strong>Yes</strong></td>\n<td>Our GVN should cover all of this.</td>\n</tr>\n<tr>\n<td>dce</td>\n<td>Perform dead code elimination to remove unreachable blocks and unused expressions</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>optimize-instructions</td>\n<td>Peephole optimizations</td>\n<td><strong>Partial</strong></td>\n<td>We have some peepholes, but not as many as <code>wasm-opt</code>, and could definitely add more. Although, they care a lot about Wasm encoding tricks for peepholes where we do not. Probably better to look at LLVM itself here for inspiration. Finally, they also have some Souper-synthesized peepholes, and we should really add some of our own once the e-graphs work lands.</td>\n</tr>\n<tr>\n<td>pick-load-signs</td>\n<td>Look at uses of a load to determine whether to use sign extension or zero extension for the load (e.g. if the load instruction is <code>i32.load8_u</code> but a majority of uses are prefixed with <code>i32.extend8_s</code>, then change the load to <code>i32.load8_s</code>, remove the now-redundant sign extends from the majority of uses, and insert zero extends for the other uses.)</td>\n<td><strong>No</strong></td>\n<td>Unclear how much this is worth it in practice, especially if our primary goal is code speed rather than code size.</td>\n</tr>\n<tr>\n<td>precompute[-propagate]</td>\n<td>Constant propagation and folding</td>\n<td><strong>Partial</strong></td>\n<td>We have a couple peepholes in <code>simple_preopt</code> that do some of this, but only 2 levels deep. Should investigate doing this more completely once the WIP e-graphs work merges.</td>\n</tr>\n<tr>\n<td>code-pushing</td>\n<td>Push defs down towards uses. Might move the def into a block on the other side of a conditional, making it so that it is not executed unless needed.</td>\n<td><strong>No</strong></td>\n<td>Unsure whether their pass is aware of loop boundaries, and whether this might \"undo\" some manual LICM the programmer/LLVM did (this comes before their LICM in their phase ordering; our LICM won't create partially dead code, fwiw.) Although maybe we start (or can start) doing this with the new e-graphs work?</td>\n</tr>\n<tr>\n<td>code-folding</td>\n<td>Merge common tails of all of a block's predecessors into the block itself.</td>\n<td><strong>No</strong></td>\n<td>I don't believe we do any block-level optimizations that look at multiple predecessors or multiple successors at the same time (i.e. we can merge one successor block into its sole predecessor). FWIW, I don't see a dual pass for merging common heads of successor blocks into their predecessor block in <code>wasm-opt</code>, but that seems like an obvious thing to implement if you've implemented merging common tails of successor blocks. Totally possible it exists and I missed it.</td>\n</tr>\n<tr>\n<td>merge-blocks</td>\n<td>Sort of Wasm-specific, but essentially merge a block into its sole predecessor.</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>duplicate-function-elimination</td>\n<td>Interprocedural optimization to deduplicate identical functions.</td>\n<td><strong>No</strong></td>\n<td>When the new incremental caching infra is enabled, I <em>guess</em> we <em>could</em> get this for free? But also depends on implementation (which I am not personally familiar with) and order of function compilation scheduling in the face of our parallelism.</td>\n</tr>\n<tr>\n<td>inlining</td>\n<td>Inline a callee function into its caller, removing function call overhead and, more importantly, providing opportunity for more optimization based on the actual arguments to the call.</td>\n<td><strong>No</strong></td>\n<td>We probably don't want this for regular core Wasm modules right now, since LLVM did all the profitable inlining already and has way better heuristics than anything we are going to come up with on the first try. If something is both small and not inlined into callers by the time we see it, then it was probably either marked no-inline or cold or something like that and we just don't have those annotations anymore. However, with the component model this is going to change: callees will remain a black box until after component linking time (so LLVM won't ever have had a chance to inline these calls) and we will have lots of oppotunities to do some nice cross-module inlining ourselves.</td>\n</tr>\n<tr>\n<td>directize</td>\n<td>Turn <code>call_indirect</code>s into <code>call</code>s. Devirtualization.</td>\n<td><strong>No</strong></td>\n<td>Probably not profitable for us to do this, since LLVM already does it, but could be very profitable when done optimistically in concert with PGO data and then inline the callee into the caller.</td>\n</tr>\n</tbody>\n</table>\n</blockquote>",
        "id": 296848126,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1662133949
    },
    {
        "content": "<p>akirilov-arm labeled <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712\">issue #4712</a>:</p>\n<blockquote>\n<p>What is Cranelift's job (in the context of Wasmtime)? To take Wasm that is produced by LLVM and already optimized 99% of the time and do the architecture-specific code generation that LLVM cannot do when targeting Wasm (e.g. instruction selection, regalloc). We don't want to duplicate <em>all</em> of LLVM's mid-end optimizations, only the ones that are beneficial for cleaning up and improving code after we've lowered Wasm memory operations into raw base + offset memory operations, etc. This is an interesting place for a compiler to be, and it means the set of passes and trade offs we have are different from what one might assume by default.</p>\n<p>There is another compiler that is in a similar space, at least as far as consuming already-optimized-by-LLVM Wasm binaries and attempting to further optimize them: binaryen and <code>wasm-opt</code>. The big difference is that <code>wasm-opt</code> is emitting another Wasm binary while we are emitting machine code. But maybe they have passes that are not specific to targeting Wasm and which are beneficial to run on already-optimized-by-LLVM Wasm binaries? AIUI, the LLVM IR to Wasm lowering introduces some suboptimal code patterns.</p>\n<p>So I did an informal census of what passes are run by <code>wasm-opt</code>, filtering out anything that looked overly specific to targeting Wasm. Results are summarized in the table below, and might give us some food for thought as we start looking into Cranelift's code quality some more. (FWIW, I wasn't 100% sure about some things below, so if you see something that you know is incorrect, feel free to edit this issue and correct it!)</p>\n<table>\n<thead>\n<tr>\n<th>Pass</th>\n<th>Description</th>\n<th>Cranelift has equivalent?</th>\n<th>Discussion</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>local-cse</td>\n<td>Perform common-subexpression elimination within a block</td>\n<td><strong>Yes</strong></td>\n<td>Our GVN should cover all of this.</td>\n</tr>\n<tr>\n<td>dce</td>\n<td>Perform dead code elimination to remove unreachable blocks and unused expressions</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>optimize-instructions</td>\n<td>Peephole optimizations</td>\n<td><strong>Partial</strong></td>\n<td>We have some peepholes, but not as many as <code>wasm-opt</code>, and could definitely add more. Although, they care a lot about Wasm encoding tricks for peepholes where we do not. Probably better to look at LLVM itself here for inspiration. Finally, they also have some Souper-synthesized peepholes, and we should really add some of our own once the e-graphs work lands.</td>\n</tr>\n<tr>\n<td>pick-load-signs</td>\n<td>Look at uses of a load to determine whether to use sign extension or zero extension for the load (e.g. if the load instruction is <code>i32.load8_u</code> but a majority of uses are prefixed with <code>i32.extend8_s</code>, then change the load to <code>i32.load8_s</code>, remove the now-redundant sign extends from the majority of uses, and insert zero extends for the other uses.)</td>\n<td><strong>No</strong></td>\n<td>Unclear how much this is worth it in practice, especially if our primary goal is code speed rather than code size.</td>\n</tr>\n<tr>\n<td>precompute[-propagate]</td>\n<td>Constant propagation and folding</td>\n<td><strong>Partial</strong></td>\n<td>We have a couple peepholes in <code>simple_preopt</code> that do some of this, but only 2 levels deep. Should investigate doing this more completely once the WIP e-graphs work merges.</td>\n</tr>\n<tr>\n<td>code-pushing</td>\n<td>Push defs down towards uses. Might move the def into a block on the other side of a conditional, making it so that it is not executed unless needed.</td>\n<td><strong>No</strong></td>\n<td>Unsure whether their pass is aware of loop boundaries, and whether this might \"undo\" some manual LICM the programmer/LLVM did (this comes before their LICM in their phase ordering; our LICM won't create partially dead code, fwiw.) Although maybe we start (or can start) doing this with the new e-graphs work?</td>\n</tr>\n<tr>\n<td>code-folding</td>\n<td>Merge common tails of all of a block's predecessors into the block itself.</td>\n<td><strong>No</strong></td>\n<td>I don't believe we do any block-level optimizations that look at multiple predecessors or multiple successors at the same time (i.e. we can merge one successor block into its sole predecessor). FWIW, I don't see a dual pass for merging common heads of successor blocks into their predecessor block in <code>wasm-opt</code>, but that seems like an obvious thing to implement if you've implemented merging common tails of successor blocks. Totally possible it exists and I missed it.</td>\n</tr>\n<tr>\n<td>merge-blocks</td>\n<td>Sort of Wasm-specific, but essentially merge a block into its sole predecessor.</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>duplicate-function-elimination</td>\n<td>Interprocedural optimization to deduplicate identical functions.</td>\n<td><strong>No</strong></td>\n<td>When the new incremental caching infra is enabled, I <em>guess</em> we <em>could</em> get this for free? But also depends on implementation (which I am not personally familiar with) and order of function compilation scheduling in the face of our parallelism.</td>\n</tr>\n<tr>\n<td>inlining</td>\n<td>Inline a callee function into its caller, removing function call overhead and, more importantly, providing opportunity for more optimization based on the actual arguments to the call.</td>\n<td><strong>No</strong></td>\n<td>We probably don't want this for regular core Wasm modules right now, since LLVM did all the profitable inlining already and has way better heuristics than anything we are going to come up with on the first try. If something is both small and not inlined into callers by the time we see it, then it was probably either marked no-inline or cold or something like that and we just don't have those annotations anymore. However, with the component model this is going to change: callees will remain a black box until after component linking time (so LLVM won't ever have had a chance to inline these calls) and we will have lots of oppotunities to do some nice cross-module inlining ourselves.</td>\n</tr>\n<tr>\n<td>directize</td>\n<td>Turn <code>call_indirect</code>s into <code>call</code>s. Devirtualization.</td>\n<td><strong>No</strong></td>\n<td>Probably not profitable for us to do this, since LLVM already does it, but could be very profitable when done optimistically in concert with PGO data and then inline the callee into the caller.</td>\n</tr>\n</tbody>\n</table>\n</blockquote>",
        "id": 296848127,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1662133949
    },
    {
        "content": "<p>akirilov-arm labeled <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4712\">issue #4712</a>:</p>\n<blockquote>\n<p>What is Cranelift's job (in the context of Wasmtime)? To take Wasm that is produced by LLVM and already optimized 99% of the time and do the architecture-specific code generation that LLVM cannot do when targeting Wasm (e.g. instruction selection, regalloc). We don't want to duplicate <em>all</em> of LLVM's mid-end optimizations, only the ones that are beneficial for cleaning up and improving code after we've lowered Wasm memory operations into raw base + offset memory operations, etc. This is an interesting place for a compiler to be, and it means the set of passes and trade offs we have are different from what one might assume by default.</p>\n<p>There is another compiler that is in a similar space, at least as far as consuming already-optimized-by-LLVM Wasm binaries and attempting to further optimize them: binaryen and <code>wasm-opt</code>. The big difference is that <code>wasm-opt</code> is emitting another Wasm binary while we are emitting machine code. But maybe they have passes that are not specific to targeting Wasm and which are beneficial to run on already-optimized-by-LLVM Wasm binaries? AIUI, the LLVM IR to Wasm lowering introduces some suboptimal code patterns.</p>\n<p>So I did an informal census of what passes are run by <code>wasm-opt</code>, filtering out anything that looked overly specific to targeting Wasm. Results are summarized in the table below, and might give us some food for thought as we start looking into Cranelift's code quality some more. (FWIW, I wasn't 100% sure about some things below, so if you see something that you know is incorrect, feel free to edit this issue and correct it!)</p>\n<table>\n<thead>\n<tr>\n<th>Pass</th>\n<th>Description</th>\n<th>Cranelift has equivalent?</th>\n<th>Discussion</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>local-cse</td>\n<td>Perform common-subexpression elimination within a block</td>\n<td><strong>Yes</strong></td>\n<td>Our GVN should cover all of this.</td>\n</tr>\n<tr>\n<td>dce</td>\n<td>Perform dead code elimination to remove unreachable blocks and unused expressions</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>optimize-instructions</td>\n<td>Peephole optimizations</td>\n<td><strong>Partial</strong></td>\n<td>We have some peepholes, but not as many as <code>wasm-opt</code>, and could definitely add more. Although, they care a lot about Wasm encoding tricks for peepholes where we do not. Probably better to look at LLVM itself here for inspiration. Finally, they also have some Souper-synthesized peepholes, and we should really add some of our own once the e-graphs work lands.</td>\n</tr>\n<tr>\n<td>pick-load-signs</td>\n<td>Look at uses of a load to determine whether to use sign extension or zero extension for the load (e.g. if the load instruction is <code>i32.load8_u</code> but a majority of uses are prefixed with <code>i32.extend8_s</code>, then change the load to <code>i32.load8_s</code>, remove the now-redundant sign extends from the majority of uses, and insert zero extends for the other uses.)</td>\n<td><strong>No</strong></td>\n<td>Unclear how much this is worth it in practice, especially if our primary goal is code speed rather than code size.</td>\n</tr>\n<tr>\n<td>precompute[-propagate]</td>\n<td>Constant propagation and folding</td>\n<td><strong>Partial</strong></td>\n<td>We have a couple peepholes in <code>simple_preopt</code> that do some of this, but only 2 levels deep. Should investigate doing this more completely once the WIP e-graphs work merges.</td>\n</tr>\n<tr>\n<td>code-pushing</td>\n<td>Push defs down towards uses. Might move the def into a block on the other side of a conditional, making it so that it is not executed unless needed.</td>\n<td><strong>No</strong></td>\n<td>Unsure whether their pass is aware of loop boundaries, and whether this might \"undo\" some manual LICM the programmer/LLVM did (this comes before their LICM in their phase ordering; our LICM won't create partially dead code, fwiw.) Although maybe we start (or can start) doing this with the new e-graphs work?</td>\n</tr>\n<tr>\n<td>code-folding</td>\n<td>Merge common tails of all of a block's predecessors into the block itself.</td>\n<td><strong>No</strong></td>\n<td>I don't believe we do any block-level optimizations that look at multiple predecessors or multiple successors at the same time (i.e. we can merge one successor block into its sole predecessor). FWIW, I don't see a dual pass for merging common heads of successor blocks into their predecessor block in <code>wasm-opt</code>, but that seems like an obvious thing to implement if you've implemented merging common tails of successor blocks. Totally possible it exists and I missed it.</td>\n</tr>\n<tr>\n<td>merge-blocks</td>\n<td>Sort of Wasm-specific, but essentially merge a block into its sole predecessor.</td>\n<td><strong>Yes</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>duplicate-function-elimination</td>\n<td>Interprocedural optimization to deduplicate identical functions.</td>\n<td><strong>No</strong></td>\n<td>When the new incremental caching infra is enabled, I <em>guess</em> we <em>could</em> get this for free? But also depends on implementation (which I am not personally familiar with) and order of function compilation scheduling in the face of our parallelism.</td>\n</tr>\n<tr>\n<td>inlining</td>\n<td>Inline a callee function into its caller, removing function call overhead and, more importantly, providing opportunity for more optimization based on the actual arguments to the call.</td>\n<td><strong>No</strong></td>\n<td>We probably don't want this for regular core Wasm modules right now, since LLVM did all the profitable inlining already and has way better heuristics than anything we are going to come up with on the first try. If something is both small and not inlined into callers by the time we see it, then it was probably either marked no-inline or cold or something like that and we just don't have those annotations anymore. However, with the component model this is going to change: callees will remain a black box until after component linking time (so LLVM won't ever have had a chance to inline these calls) and we will have lots of oppotunities to do some nice cross-module inlining ourselves.</td>\n</tr>\n<tr>\n<td>directize</td>\n<td>Turn <code>call_indirect</code>s into <code>call</code>s. Devirtualization.</td>\n<td><strong>No</strong></td>\n<td>Probably not profitable for us to do this, since LLVM already does it, but could be very profitable when done optimistically in concert with PGO data and then inline the callee into the caller.</td>\n</tr>\n</tbody>\n</table>\n</blockquote>",
        "id": 296848128,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1662133949
    }
]