[
    {
        "content": "<p>asurdo opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243\">issue #5243</a>:</p>\n<blockquote>\n<h4>Question:</h4>\n<p>I use wasmtime in c++ multithread program. I expected performance improvement when I use more threads, but it does not. Is there any bottleneck in wasmtime? And how can I avoid it?</p>\n<h4>Hear is my performance test log.</h4>\n<p>My machine environment is 24 core and 64GB memory.</p>\n<p>1 thread: speed 8217/s, average time cost 120us<br>\n2 thread: speed: 10137/s, average time cost: 196us<br>\n10 thread: speed 11190/s, average time cost: 892us</p>\n<h4>My code logic</h4>\n<p>singleton for the engine and linker, creating a store on each call, use wasi and some host function.</p>\n</blockquote>",
        "id": 308930269,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1668063837
    },
    {
        "content": "<p>asurdo <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243#issuecomment-1309865770\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243\">issue #5243</a>:</p>\n<blockquote>\n<p>PS: I compile module only onceï¼Œand instantiate every time.<br>\nI seprate one call to three step, init store, instantiate and call instance func. The step where the time-consuming increases the most is in call instance func.</p>\n</blockquote>",
        "id": 308931342,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1668064456
    },
    {
        "content": "<p>bjorn3 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243#issuecomment-1310019615\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243\">issue #5243</a>:</p>\n<blockquote>\n<p>The bottleneck is probably in the linux kernel. As I understand it changing memory mappings will acquire a process wide lock on the memory mappings and once it is done changing them every currently running thread of the process is interrupted to make sure that the cpu core knows about the changed memory mapping. Every time you instantiate a module, memory mappings are changed. This effectively means that the instantiation count per second across the whole process is limited by how fast a single core can handle memory mapping changes and in addition every instantiation pauses execution of other threads for a moment.</p>\n</blockquote>",
        "id": 308951151,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1668073402
    },
    {
        "content": "<p>bjorn3 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243#issuecomment-1310022747\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243\">issue #5243</a>:</p>\n<blockquote>\n<p>I believe using the pooling allocation strategy (set using <code>config.allocation_strategy()</code>) is more performant than the default on demand strategy.</p>\n</blockquote>",
        "id": 308951574,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1668073569
    },
    {
        "content": "<p>asurdo <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243#issuecomment-1310215861\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243\">issue #5243</a>:</p>\n<blockquote>\n<blockquote>\n<p>I believe using the pooling allocation strategy (set using <code>config.allocation_strategy()</code>) is more performant than the default on demand strategy.</p>\n</blockquote>\n<p>Is config.allocation_strategy() only for Rust? I can't find it in c-API. And in fact, I use jemalloc for my c++ program, malloc is in user mode.</p>\n<p>I avoid this problem using multi-process(more memory cost for hundreds of modules), and it worked as I expected.</p>\n</blockquote>",
        "id": 308978311,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1668083600
    },
    {
        "content": "<p>bjorn3 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243#issuecomment-1310229725\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243\">issue #5243</a>:</p>\n<blockquote>\n<blockquote>\n<p>Is config.allocation_strategy() only for Rust?</p>\n</blockquote>\n<p>Looks like it. Seems to be an oversight.</p>\n<blockquote>\n<p>And in fact, I use jemalloc for my c++ program, malloc is in user mode.</p>\n</blockquote>\n<p>When instantiating wasmtime mmap's a (memfd) file containing the initial data of the linear memory of the wasm module. This allows the kernel to lazily load parts and copy only when the data is modified as opposed to having wasmtime write it all at once even if the vast majority isn't used at all or isn't modified. This is a necessity for fast instantiating.</p>\n</blockquote>",
        "id": 308980723,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1668084418
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243#issuecomment-1310396452\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243\">issue #5243</a>:</p>\n<blockquote>\n<p>I believe that this is largely the same issue as <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">https://github.com/bytecodealliance/wasmtime/issues/4637</a>, so I'm going to close in favor of that.</p>\n<p>The pooling allocator is known to help here. New <code>*_keep_resident</code> configuration options are also known to help. Very-recent Linux kernels are also known to help. This is an ongoing area of investigation for WebAssembly runtimes and is something we're always interested in improving on.</p>\n</blockquote>",
        "id": 309003116,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1668091369
    },
    {
        "content": "<p>alexcrichton closed <a href=\"https://github.com/bytecodealliance/wasmtime/issues/5243\">issue #5243</a>:</p>\n<blockquote>\n<h4>Question:</h4>\n<p>I use wasmtime in c++ multithread program. I expected performance improvement when I use more threads, but it does not. Is there any bottleneck in wasmtime? And how can I avoid it?</p>\n<h4>Hear is my performance test log.</h4>\n<p>My machine environment is 24 core and 64GB memory.</p>\n<p>1 thread: speed 8217/s, average time cost 120us<br>\n2 thread: speed: 10137/s, average time cost: 196us<br>\n10 thread: speed 11190/s, average time cost: 892us</p>\n<h4>My code logic</h4>\n<p>singleton for the engine and linker, creating a store on each call, use wasi and some host function.</p>\n</blockquote>",
        "id": 309003120,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1668091369
    }
]