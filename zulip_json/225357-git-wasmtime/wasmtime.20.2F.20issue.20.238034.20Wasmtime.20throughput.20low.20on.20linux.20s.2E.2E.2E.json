[
    {
        "content": "<p><a href=\"https://github.com/thomastaylor312\">thomastaylor312</a> added the bug label to <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">Issue #8034</a>.</p>",
        "id": 424120700,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709245824
    },
    {
        "content": "<p>thomastaylor312 opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<h3>Test Case</h3>\n<p>This is the wasm file, zipped up in order to upload to GH. It is from <a href=\"https://github.com/sunfishcode/hello-wasi-http.git\">https://github.com/sunfishcode/hello-wasi-http.git</a><br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/files/14454123/hello_wasi_http.wasm.zip\">hello_wasi_http.wasm.zip</a></p>\n<h3>Steps to Reproduce</h3>\n<p>Try these steps on a linux machine and on a macos machine (preferably close to the same size):</p>\n<ol>\n<li>Run the component with <code>wasmtime serve</code> (no additional flags)</li>\n<li>Run <code>hey -z 10s -c 100 http://localhost:8080/</code></li>\n</ol>\n<h3>Expected Results</h3>\n<p>I expect the number of requests/second to be the same or greater on linux than they are on Mac</p>\n<h3>Actual Results</h3>\n<p>On my Mac (details on OS below) that was running a bunch of other applications, I get around 20k req/s<br>\nOn linux (details on OS below), I get around 4.3k req/s</p>\n<h3>Versions and Environment</h3>\n<p>Wasmtime version or commit: 18.0.2</p>\n<p><strong>Mac</strong><br>\nOperating system: Sonoma 14.3.1</p>\n<p>Architecture: M1 Max (2 performance cores, 8 normal cores) and 64 GB of memory</p>\n<p><strong>Linux</strong><br>\nOperating system: Debian Bookworm (6.1.0-18-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01) x86_64 GNU/Linux)</p>\n<p>Architecture: AMD64 (16 cores) and 64 GB of memory</p>\n<p>This was run on a cloud VM but I also tested this on a ubuntu 20.04 amd64 server running at my house with similar performance</p>\n<h3>Extra Info</h3>\n<p>On the linux server, I did double check my file descriptor limit had been raised and also observed that the wasmtime processes were all getting to an uninterruptible sleep state almost constantly through the whole test (which could mean nothing). Also, I did a similar test with wasmCloud and Spin, which both use wasmtime and was getting a similar drop in numbers between mac and linux. For reference, I also did some smoke tests with normal server traffic (I did a test with Caddy and with NATS) and all of them were getting easily into the 100k+ range. So this definitely seems like something on the wasmtime side.</p>\n<p>I did see #4637 and that does explain some of the horizontal scaling issues, but I didn't expect such a drastic difference between Mac and Linux<br>\n</p>\n</blockquote>",
        "id": 424120703,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709245825
    },
    {
        "content": "<p>fitzgen <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1972093383\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>First off: are you enabling the pooling allocator? Eg <code>-O pooling-allocator</code> in the CLI. Enabling or disabling the pooling allocator is going to greatly affect requests/second.</p>\n<p>So will disabling virtual memory-based bounds checks and replacing them with explicit bounds checks (<code>-O static-memory-maximum-size=0</code>) which should increase requests/second for short-lived Wasm but will slow down Wasm execution by ~1.5x.</p>\n<blockquote>\n<p>I expect the number of requests/second to be the same or greater on linux than they are on Mac</p>\n</blockquote>\n<p>I don't think we can make hard guarantees about this unless you disable virtual memory-based bounds checks completely, because the performance bottleneck for concurrent Wasm guests is the kernel's virtual memory subsystem. Even with the pooling allocator, we are bottlenecked on <code>madvise</code> kinds of things and their associated IPIs. Without the pooling allocator, you're essentially benchmarking concurrent <code>mmap</code>.</p>\n</blockquote>",
        "id": 424122050,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709246409
    },
    {
        "content": "<p>fitzgen <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1972104092\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>For example, here are the results I get on my ~6 year old think pad (4-core / 8 hyperthreads) running linux:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">No</span><span class=\"w\"> </span><span class=\"n\">pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span>:                        <span class=\"mf\">5983.7146</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n<span class=\"n\">Pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span>:                          <span class=\"mf\">34980.6398</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n<span class=\"n\">Pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">explicit</span><span class=\"w\"> </span><span class=\"n\">bounds</span><span class=\"w\"> </span><span class=\"n\">checks</span>: <span class=\"mf\">35368.5013</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n</code></pre></div>\n<p>(I'd expect the delta between the second and third configuration to be even greater on machines with more cores)</p>\n</blockquote>",
        "id": 424123090,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709246948
    },
    {
        "content": "<p>fitzgen <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1972106731\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>Ah, you also have to pass <code>-O memory-init-cow=n</code> to get rid of all the virtual memory interaction here. Once I do that I get the following results:</p>\n<p><div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">No</span><span class=\"w\"> </span><span class=\"n\">pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span>:                                        <span class=\"mf\">5983.7146</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n<span class=\"n\">Pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span>:                                          <span class=\"mf\">34980.6398</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n<span class=\"n\">Pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">explicit</span><span class=\"w\"> </span><span class=\"n\">bounds</span><span class=\"w\"> </span><span class=\"n\">checks</span>:                 <span class=\"mf\">35368.5013</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n<span class=\"n\">Pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">explicit</span><span class=\"w\"> </span><span class=\"n\">bounds</span><span class=\"w\"> </span><span class=\"n\">checks</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">no</span><span class=\"w\"> </span><span class=\"n\">memory</span><span class=\"w\"> </span><span class=\"n\">CoW</span>: <span class=\"mf\">45451.2630</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n</code></pre></div><br>\n</p>\n</blockquote>",
        "id": 424123414,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709247091
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1972107701\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>@thomastaylor312 could you tell us more about your hardware on the Linux side? The reason I ask is that \"cloud VM\" is pretty vague -- it could be an older microarchitecture, or with oversubscribed cores, or something else. Without more specific details, I'm not sure why it's a given that RPS should be higher on Linux on hardware A vs. macOS on hardware B.</p>\n<p>Also a possible experiment: are you able to run a Linux VM on your M1 Max hardware (even better, native Linux such as Asahi, but a VM in e.g. UTM is fine too), and test Wasmtime there? That would tell us a lot about how the raw CPU power actually compares.</p>\n</blockquote>",
        "id": 424123532,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709247139
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1972114531\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>I apologize if this is a bit piling on at this point, but I wanted to comment the same as @fitzgen, this is probably the <code>-O pooling-allocator</code> vs not. Locally the difference I see is:</p>\n<ul>\n<li><code>wasmtime serve</code> - 5.5k rps</li>\n<li><code>wasmtime serve -O pooling-allocator</code> - 236k rps</li>\n</ul>\n<p>This is perhaps an argument that we should turn on the pooling allocator by default for the <code>wasmtime serve</code> command!</p>\n<p>Also, as mentioned in <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">https://github.com/bytecodealliance/wasmtime/issues/4637</a>, there's various knobs to help with the overhead of virtual memory here. They're not always applicable in all cases, for example <code>wasmtime serve -O pooling-allocator,memory-init-cow=n,static-memory-maximum-size=0,pooling-memory-keep-resident=$((10&lt;&lt;20)),pooling-table-keep-resident=$((10&lt;&lt;20))</code> yields 193k rps for me locally. There are zero interactions with virtual memory in the steady state, but wasmtime spends 70% of its time in memset resetting memory between http requests.</p>\n</blockquote>",
        "id": 424124251,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709247497
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1972118327\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>(also thank you for such a detailed report!)</p>\n</blockquote>",
        "id": 424124679,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709247691
    },
    {
        "content": "<p>thomastaylor312 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973566563\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>@cfallin For sure! This is a GCP machine running on an AMD Rome series processor. The exact instance size is a n2d-standard-16. Also, to call out again, I did try this on a local linux server running a slightly older intel processor with similar effects. Working on trying out some of the options suggested and will report back soon</p>\n</blockquote>",
        "id": 424291132,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709313003
    },
    {
        "content": "<p>thomastaylor312 edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973566563\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>@cfallin For sure! This is a GCP machine running on an AMD Rome series processor. The exact instance size is a n2d-standard-16. Also, to call out again, I did try this on a local linux server running a slightly older intel processor with similar results. Working on trying out some of the options suggested and will report back soon</p>\n</blockquote>",
        "id": 424291184,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709313018
    },
    {
        "content": "<p>thomastaylor312 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973590535\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>Here are my numbers:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">No</span><span class=\"w\"> </span><span class=\"n\">pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span>:                                        <span class=\"mf\">4714.3026</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n<span class=\"n\">Pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span>:                                          <span class=\"mf\">42696.3823</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n<span class=\"n\">Pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">explicit</span><span class=\"w\"> </span><span class=\"n\">bounds</span><span class=\"w\"> </span><span class=\"n\">checks</span>:                 <span class=\"mf\">42603.7692</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n<span class=\"n\">Pooling</span><span class=\"w\"> </span><span class=\"n\">allocator</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">explicit</span><span class=\"w\"> </span><span class=\"n\">bounds</span><span class=\"w\"> </span><span class=\"n\">checks</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">no</span><span class=\"w\"> </span><span class=\"n\">memory</span><span class=\"w\"> </span><span class=\"n\">CoW</span>: <span class=\"mf\">55162.2862</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"o\">/</span><span class=\"n\">second</span>\n</code></pre></div>\n<p>So that seems to line up with what you were seeing @fitzgen. What I am a little unsure about are the tradeoffs involved here. I think I wouldn't want <code>static-memory-maximum-size=0</code> since I don't want to slow down execution of longer lived components. I did already try out the pooling allocator in wasmCloud and saw the benefits, but all of the options were a little confusing as to what the memory footprint will be. I wasn't sure how to set all of those values to use the right amount of memory on any given machine. I was starting to make some guesses but wasn't entirely certain.</p>\n<p>Also, are there any tradeoffs around using <code>memory-init-cow=n</code> with the pooling allocator?<br>\n</p>\n</blockquote>",
        "id": 424294030,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709313935
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973612338\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>There are two performance \"figures of merit\": the instantiation speed and the runtime speed (how fast the Wasm executes once instantiation completes). The first two lines (no pooling and pooling) keep exactly the same generated code, so there's no runtime slowdown; the pooling allocator speedup is pure win on instantiation speed (due to the virtual-memory tricks). <code>memory-init-cow</code> again has to do with instantiation speed and doesn't alter runtime. One other configuration you haven't run yet, that might be interesting to try, is pooling + no memory CoW (but without explicit bounds checks): that should have fully optimal generated code and no runtime slowdown.</p>\n<p>(Reality-is-complicated footnote: there may be some effects in the margins with page fault latency that do actually affect runtime depending on the virtual memory strategy; but those effects should be much smaller than the explicit vs. static bounds checks.)</p>\n</blockquote>",
        "id": 424296335,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709314770
    },
    {
        "content": "<p>thomastaylor312 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973617485\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>Yep I actually just tried the pooling + no memory CoW and that looks good. My remaining concern is around how all the different pooling allocator knobs can affect runtime</p>\n</blockquote>",
        "id": 424296890,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709314971
    },
    {
        "content": "<p>thomastaylor312 edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973617485\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>Yep I actually just tried the pooling + no memory CoW and that looks good. My remaining concern is around how all the different pooling allocator knobs can affect runtime/memory usage.</p>\n</blockquote>",
        "id": 424297168,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709315067
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973654989\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<blockquote>\n<p>runtime/memory usage</p>\n</blockquote>\n<p>@thomastaylor312 the best answer is usually \"try it and see\", since tradeoffs can cross different inflection points depending on your particular workload.</p>\n<p>As mentioned above, runtime (that is, execution speed of the Wasm) is unaffected by the pooling allocator. Explicit bounds checks are the only option that modify the generated code.</p>\n<p>Memory usage (resident set size) might increase slightly if the \"no CoW\" option is set, because the pooling allocator keeps private pages around for each slot. (I don't remember if it retains a \"warmed up\" slot for a given image to reuse it though, @alexcrichton do you remember?) CoW is more memory-efficient because it can leverage shared read-only mappings of the heap image that aren't modified by the guest.</p>\n</blockquote>",
        "id": 424300268,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709316273
    },
    {
        "content": "<p>thomastaylor312 closed <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<h3>Test Case</h3>\n<p>This is the wasm file, zipped up in order to upload to GH. It is from <a href=\"https://github.com/sunfishcode/hello-wasi-http.git\">https://github.com/sunfishcode/hello-wasi-http.git</a><br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/files/14454123/hello_wasi_http.wasm.zip\">hello_wasi_http.wasm.zip</a></p>\n<h3>Steps to Reproduce</h3>\n<p>Try these steps on a linux machine and on a macos machine (preferably close to the same size):</p>\n<ol>\n<li>Run the component with <code>wasmtime serve</code> (no additional flags)</li>\n<li>Run <code>hey -z 10s -c 100 http://localhost:8080/</code></li>\n</ol>\n<h3>Expected Results</h3>\n<p>I expect the number of requests/second to be the same or greater on linux than they are on Mac</p>\n<h3>Actual Results</h3>\n<p>On my Mac (details on OS below) that was running a bunch of other applications, I get around 20k req/s<br>\nOn linux (details on OS below), I get around 4.3k req/s</p>\n<h3>Versions and Environment</h3>\n<p>Wasmtime version or commit: 18.0.2</p>\n<p><strong>Mac</strong><br>\nOperating system: Sonoma 14.3.1</p>\n<p>Architecture: M1 Max (2 performance cores, 8 normal cores) and 64 GB of memory</p>\n<p><strong>Linux</strong><br>\nOperating system: Debian Bookworm (6.1.0-18-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01) x86_64 GNU/Linux)</p>\n<p>Architecture: AMD64 (16 cores) and 64 GB of memory</p>\n<p>This was run on a cloud VM but I also tested this on a ubuntu 20.04 amd64 server running at my house with similar performance</p>\n<h3>Extra Info</h3>\n<p>On the linux server, I did double check my file descriptor limit had been raised and also observed that the wasmtime processes were all getting to an uninterruptible sleep state almost constantly through the whole test (which could mean nothing). Also, I did a similar test with wasmCloud and Spin, which both use wasmtime and was getting a similar drop in numbers between mac and linux. For reference, I also did some smoke tests with normal server traffic (I did a test with Caddy and with NATS) and all of them were getting easily into the 100k+ range. So this definitely seems like something on the wasmtime side.</p>\n<p>I did see #4637 and that does explain some of the horizontal scaling issues, but I didn't expect such a drastic difference between Mac and Linux<br>\n</p>\n</blockquote>",
        "id": 424303346,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709317573
    },
    {
        "content": "<p>thomastaylor312 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973713492\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>Ok, I think between that and the docs that are already on the pooling allocator, I think I should be good enough. I'll go ahead and close this, but hopefully this issue can be helpful to others who might be optimizing things. Thanks for the help all!</p>\n</blockquote>",
        "id": 424303348,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709317574
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1973749738\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034\">issue #8034</a>:</p>\n<blockquote>\n<p>I was inspired to summarize some of the bits and bobs here in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8038\">https://github.com/bytecodealliance/wasmtime/pull/8038</a> to hopefully help future readers as well.</p>\n<blockquote>\n<p>that might be interesting to try, is pooling + no memory CoW</p>\n</blockquote>\n<p>...</p>\n<blockquote>\n<p>Yep I actually just tried the pooling + no memory CoW and that looks good</p>\n</blockquote>\n<p>This surprises me! I would not expect disabling CoW to provide much benefit when explicit bounds checks are still enabled. If anything I'd expect it to get a bit slower (like what I <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8034#issuecomment-1972114531\">measured above</a>). </p>\n<p>I say this because even if you disable copy-on-write we still use <code>madvise</code> to clear memory (regardless of whether bounds checks are enabled or not) which involves IPIs which don't scale well. This might be a case of you running into something Chris has pointed out in the past where when using CoW if you first read a page that will fault in a read-only mapping, but then if you write to the same page the copy happens <em>in addition to</em> an IPI to clear the old mapping. This means that CoW, while beneficial for large heap images due to removing startup cost, may be less beneficial over time if pages are read-then-written to cause even more IPIs (which aren't scalable) to happen over time.</p>\n<blockquote>\n<p>Memory usage (resident set size) might increase slightly if the \"no CoW\" option is set, because the pooling allocator keeps private pages around for each slot.</p>\n</blockquote>\n<p>I don't think this will actually be the case because when a slot is deallocated we <code>madvise</code>-reset the entire linear memory which should release all memory back to the kernel, so with-and-without CoW should have the same rss for deallocated slots.</p>\n<p>Now for allocated slots, as you've pointed out, having 1000 instances with CoW should have less rss than 1000 instances without CoW because readonly pages will be shared in the CoW case.</p>\n</blockquote>",
        "id": 424307771,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1709319262
    }
]