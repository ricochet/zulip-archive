[
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996251975\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>I updated a few simple tests here as an example, but I figured it'd be good to get some review/feedback on this before getting to eager about anything else. AFAIK there's been some desire for this but not discussion around the design, so I don't want to presume too much here.</p>\n</blockquote>",
        "id": 265230806,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639694783
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996415784\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>Huh, wow, this is certainly a wild idea! Unit tests that update themselves to pass (with permission)!</p>\n<p>I do actually think this is a really cool solution to the \"golden outputs are super-tedious to update\" problem. The golden tests were never meant (or at least, I never meant them) to get to the volume and specificity that they have, where they trip over all sorts of incidental cross-cutting changes, like regalloc perturbations. They were mostly meant to be a stopgap; we really should pay down the techdebt soon. That said, a few reservations:</p>\n<ul>\n<li>The golden-output tests were kind of a stopgap, and I actually think not relying more on runtests was a mistake. IMHO (now, with hindsight), a golden-output test is useful to ensure some properties, such as that a lowering is or isn't using a certain instruction, a certain optimization is or isn't happening, or e.g. the ABI code is using a certain stackframe layout. But the vast majority of the tests are actually asserting a particular output as an indirect way of asserting correct execution, because that particular output in theory executes correctly. (Even that isn't always verified though.)</li>\n</ul>\n<p>So my \"fix the fundamental problem\" solution would be to delete most golden-output tests once we ensure we have coverage with runtests.</p>\n<ul>\n<li>While it's a huge pain, the process of updating golden outputs is at least in principle supposed to be a human check -- yes, the output changed, does the change make sense? While one can look at deltas before doing an automated thing, in practice having a \"just mash the big button to make it pass\" option is the kind of thing that will encourage us to get sloppy about the golden test updates at some point.</li>\n</ul>\n<p>As a human check goes, though, tediously reformatting the output line-by-line into filecheck format and removing block markers and such is probably more than is actually needed; the ideal \"manual update\" step is (IMHO) just copy/pasting or piping a blob from one place to another. Maybe we can do something with the filecheck infra to improve this; not sure.</p>\n<ul>\n<li>The parsing here is a bit brittle, which worries me; one more thing to maintain if our output format changes. (the \"load-bearing <code>Debug</code> format\" problem, basically.)</li>\n</ul>\n<p>I'm open to more discussion here of course; curious what others think as well. I do agree that our time is not well-spent now so we have to do <em>something</em>; maybe it's just time to move to runtests :-)</p>\n</blockquote>",
        "id": 265251161,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639713217
    },
    {
        "content": "<p>cfallin edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996415784\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>Huh, wow, this is certainly a wild idea! Unit tests that update themselves to pass (with permission)!</p>\n<p>I do actually think this is a really cool solution to the \"golden outputs are super-tedious to update\" problem. The golden tests were never meant (or at least, I never meant them) to get to the volume and specificity that they have, where they trip over all sorts of incidental cross-cutting changes, like regalloc perturbations. They were mostly meant to be a stopgap; we really should pay down the techdebt soon. That said, a few reservations:</p>\n<ul>\n<li>The golden-output tests were kind of a stopgap, and I actually think not relying more on runtests was a mistake. IMHO (now, with hindsight), a golden-output test is useful to ensure some properties, such as that a lowering is or isn't using a certain instruction, a certain optimization is or isn't happening, or e.g. the ABI code is using a certain stackframe layout. But the vast majority of the tests are actually asserting a particular output as an indirect way of asserting correct execution, because that particular output in theory executes correctly. (Even that isn't always verified though.)</li>\n</ul>\n<p>So my \"fix the fundamental problem\" solution would be to delete most golden-output tests once we ensure we have coverage with runtests.</p>\n<ul>\n<li>\n<p>While it's a huge pain, the process of updating golden outputs is at least in principle supposed to be a human check -- yes, the output changed, does the change make sense? While one can look at deltas before doing an automated thing, in practice having a \"just mash the big button to make it pass\" option is the kind of thing that will encourage us to get sloppy about the golden test updates at some point.</p>\n<p>As a human check goes, though, tediously reformatting the output line-by-line into filecheck format and removing block markers and such is probably more than is actually needed; the ideal \"manual update\" step is (IMHO) just copy/pasting or piping a blob from one place to another. Maybe we can do something with the filecheck infra to improve this; not sure.</p>\n</li>\n<li>\n<p>The parsing here is a bit brittle, which worries me; one more thing to maintain if our output format changes. (the \"load-bearing <code>Debug</code> format\" problem, basically.)</p>\n<p>I'm open to more discussion here of course; curious what others think as well. I do agree that our time is not well-spent now so we have to do <em>something</em>; maybe it's just time to move to runtests :-)</p>\n</li>\n</ul>\n</blockquote>",
        "id": 265251211,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639713261
    },
    {
        "content": "<p>cfallin edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996415784\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>Huh, wow, this is certainly a wild idea! Unit tests that update themselves to pass (with permission)!</p>\n<p>I do actually think this is a really cool solution to the \"golden outputs are super-tedious to update\" problem. The golden tests were never meant (or at least, I never meant them) to get to the volume and specificity that they have, where they trip over all sorts of incidental cross-cutting changes, like regalloc perturbations. They were mostly meant to be a stopgap; we really should pay down the techdebt soon. That said, a few reservations:</p>\n<ul>\n<li>\n<p>The golden-output tests were kind of a stopgap, and I actually think not relying more on runtests was a mistake. IMHO (now, with hindsight), a golden-output test is useful to ensure some properties, such as that a lowering is or isn't using a certain instruction, a certain optimization is or isn't happening, or e.g. the ABI code is using a certain stackframe layout. But the vast majority of the tests are actually asserting a particular output as an indirect way of asserting correct execution, because that particular output in theory executes correctly. (Even that isn't always verified though.)</p>\n<p>So my \"fix the fundamental problem\" solution would be to delete most golden-output tests once we ensure we have coverage with runtests.</p>\n</li>\n<li>\n<p>While it's a huge pain, the process of updating golden outputs is at least in principle supposed to be a human check -- yes, the output changed, does the change make sense? While one can look at deltas before doing an automated thing, in practice having a \"just mash the big button to make it pass\" option is the kind of thing that will encourage us to get sloppy about the golden test updates at some point.</p>\n<p>As a human check goes, though, tediously reformatting the output line-by-line into filecheck format and removing block markers and such is probably more than is actually needed; the ideal \"manual update\" step is (IMHO) just copy/pasting or piping a blob from one place to another. Maybe we can do something with the filecheck infra to improve this; not sure.</p>\n</li>\n<li>\n<p>The parsing here is a bit brittle, which worries me; one more thing to maintain if our output format changes. (the \"load-bearing <code>Debug</code> format\" problem, basically.)</p>\n<p>I'm open to more discussion here of course; curious what others think as well. I do agree that our time is not well-spent now so we have to do <em>something</em>; maybe it's just time to move to runtests :-)</p>\n</li>\n</ul>\n</blockquote>",
        "id": 265251217,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639713271
    },
    {
        "content": "<p>cfallin edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996415784\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>Huh, wow, this is certainly a wild idea! Unit tests that update themselves to pass (with permission)!</p>\n<p>I do actually think this is a really cool solution to the \"golden outputs are super-tedious to update\" problem. The golden tests were never meant (or at least, I never meant them) to get to the volume and specificity that they have, where they trip over all sorts of incidental cross-cutting changes, like regalloc perturbations. They were mostly meant to be a stopgap; we really should pay down the techdebt soon. That said, a few reservations:</p>\n<ul>\n<li>\n<p>The golden-output tests were kind of a stopgap, and I actually think not relying more on runtests was a mistake. IMHO (now, with hindsight), a golden-output test is useful to ensure some properties, such as that a lowering is or isn't using a certain instruction, a certain optimization is or isn't happening, or e.g. the ABI code is using a certain stackframe layout. But the vast majority of the tests are actually asserting a particular output as an indirect way of asserting correct execution, because that particular output in theory executes correctly. (Even that isn't always verified though.)</p>\n<p>So my \"fix the fundamental problem\" solution would be to delete most golden-output tests once we ensure we have coverage with runtests.</p>\n</li>\n<li>\n<p>While it's a huge pain, the process of updating golden outputs is at least in principle supposed to be a human check -- yes, the output changed, does the change make sense? While one can look at deltas before doing an automated thing, in practice having a \"just mash the big button to make it pass\" option is the kind of thing that will encourage us to get sloppy about the golden test updates at some point.</p>\n<p>As a human check goes, though, tediously reformatting the output line-by-line into filecheck format and removing block markers and such is probably more than is actually needed; the ideal \"manual update\" step is (IMHO) just copy/pasting or piping a blob from one place to another. Maybe we can do something with the filecheck infra to improve this; not sure.</p>\n</li>\n<li>\n<p>The parsing here is a bit brittle, which worries me; one more thing to maintain if our output format changes. (the \"load-bearing <code>Debug</code> format\" problem, basically. EDIT: of course it's load-bearing today because it's captured as golden; but it's even more load-bearing if we write parsing code to it!)</p>\n<p>I'm open to more discussion here of course; curious what others think as well. I do agree that our time is not well-spent now so we have to do <em>something</em>; maybe it's just time to move to runtests :-)</p>\n</li>\n</ul>\n</blockquote>",
        "id": 265251344,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639713418
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996833461\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>FWIW I've used this technique to what feels like good success in many other projects, and I know rust-lang/rust has been using this format of tests quite successfully for quite some time for compiler error messages.</p>\n<p>I do agree that <code>run</code> tests are better than <code>compile</code> tests, but I also don't want to ignore what's already there today. I think both styles of tests have advantages and disadvantages. For example if we're testing that a particular special case in ISLE is used then <code>run</code> won't be too useful and <code>compile</code> is better, although all <code>compile</code> tests should arguably be coupled with <code>run</code> tests as well. In the meantime though we've got tons of <code>compile</code> tests that are already tedious to update and it will presumably be just as tedious to migrate them all to <code>run</code> tests as well.</p>\n<p>Otherwise the point about getting sloppy about updates is probably subjective. Subjectively I have never seen this be a problem in other projects I've worked on. While theoretically possible this is sort of what code review is for and if a code reviewer is ignoring code then I'm not sure that there's much we can do about that.</p>\n<p>I think I'd also disagree that this is brittle. This is testing a 100% match which means there's little room for deviation. There's some arguably weird heuristics applied to the <code>Debug</code> output to make the assertions prettier in the test files, but if the <code>Debug</code> output ever changes then all tests will instantly fail. It's pretty easy to either update the output-massaging code or to update all tests en-masse when the output changes in this regard. If we expect rapid changes to <code>Debug</code> to happen over time which would require tedious maintenance of \"map the program output to the test expectation\" then we should invest in a more proper \"map the program output to a string\" implementation that doesn't use <code>Debug</code>. Basically I've found that it's little consequence to be \"sloppy\" with creating the textual what's-expected-from-the-test because if you mess up tests fail and if you succeed then you can see the diff of what the output actually is.</p>\n</blockquote>",
        "id": 265316369,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639756744
    },
    {
        "content": "<p>froydnj <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996919340\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>Lurking in: I don't know the history here, but has something like LLVM's <code>FileCheck</code> tool been considered?  (<a href=\"https://llvm.org/docs/CommandGuide/FileCheck.html\">https://llvm.org/docs/CommandGuide/FileCheck.html</a>)  It's probably not reasonable to use <code>FileCheck</code> directly, but it's not that complicated of a tool to write yourself if you need to (and <code>FileCheck</code> itself doesn't need any clif-specific knowledge).</p>\n<p>Assuming you write patterns well, checks should be resistant to things like register allocation changes.  It also makes explicit what the test is actually testing (e.g. these particular instructions are generated in this order): it's not always easy to remember/tell exactly what a test is actually testing when reviewing auto updates.  Any changes to the checks themselves do have to be manual, but they're also much more easily reviewable by virtue of being small and targeted.</p>\n</blockquote>",
        "id": 265336705,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639764505
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996929235\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>@alexcrichton , it's interesting (and good to know) that this sort of thing is used successfully elsewhere, thanks!</p>\n<p>Specific thoughts below; my last reservation (specifics of how the parsing works) was the most concrete and I think there's a reasonable way to address it.</p>\n<blockquote>\n<p>Otherwise the point about getting sloppy about updates is probably subjective. Subjectively I have never seen this be a problem in other projects I've worked on. While theoretically possible this is sort of what code review is for and if a code reviewer is ignoring code then I'm not sure that there's much we can do about that.</p>\n</blockquote>\n<p>Yeah, that's fair!</p>\n<blockquote>\n<p>I think I'd also disagree that this is brittle. This is testing a 100% match which means there's little room for deviation. There's some arguably weird heuristics applied to the <code>Debug</code> output to make the assertions prettier in the test files, but if the <code>Debug</code> output ever changes then all tests will instantly fail. It's pretty easy to either update the output-massaging code or to update all tests en-masse when the output changes in this regard. If we expect rapid changes to <code>Debug</code> to happen over time which would require tedious maintenance of \"map the program output to the test expectation\" then we should invest in a more proper \"map the program output to a string\" implementation that doesn't use <code>Debug</code>. Basically I've found that it's little consequence to be \"sloppy\" with creating the textual what's-expected-from-the-test because if you mess up tests fail and if you succeed then you can see the diff of what the output actually is.</p>\n</blockquote>\n<p>I think that much of my reservation actually came just from the detailed parsing that's happening e.g. <a href=\"https://github.com/alexcrichton/wasmtime/blob/913935adf5c45facbeb113a4ae384910e82c0c97/cranelift/filetests/src/test_compile.rs#L123-L128\">here</a>. That feels like unnecessary coupling. I actually agree that a stable canonical \"string representation of VCode\" with less output meant for human readability is the better answer here: just output like</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">block0</span>:\n  <span class=\"nc\">add</span><span class=\"w\"> </span><span class=\"n\">ra</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">rb</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">rc</span><span class=\"w\"></span>\n<span class=\"w\">  </span><span class=\"n\">cmp</span><span class=\"w\"> </span><span class=\"n\">rd</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">re</span><span class=\"w\"></span>\n<span class=\"w\">  </span><span class=\"n\">jne</span><span class=\"w\"> </span><span class=\"n\">block1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">block2</span><span class=\"w\"></span>\n<span class=\"n\">block1</span>:\n  <span class=\"o\">..</span><span class=\"p\">.</span><span class=\"w\"></span>\n</code></pre></div>\n<p>would lead to clearer diffs I think. If we implement this as a method on <code>VCode</code> and then use this unconditionally for filetests rather than only in a special \"precise mode\", then I think it's a great idea!</p>\n<blockquote>\n<p>Lurking in: I don't know the history here, but has something like LLVM's FileCheck tool been considered? [ ... ] It also makes explicit what the test is actually testing (e.g. these particular instructions are generated in this order)</p>\n</blockquote>\n<p>The <code>filetest</code> infra was written by @sunfishcode with direct inspiration from LLVM's filecheck, IIRC (<a href=\"https://crates.io/crates/filecheck\">crate</a>). It does actually have more features than we give it credit for / use regularly currently -- full regexes, and ability to flexibly match lines in certain sequences while skipping others. (The last bit is used to skip over extraneous VCode debug output but the rest, mostly not, at least in tests I've written.)</p>\n<p>It's a good point that a lot of this pain goes away if we write more targeted tests: if we want to test that instruction X is emitted after Y, or that lowering an operator results in specifically instruction Z, we can test that without asserting on the whole function body. That's a different kind of test than we have today but it's what I'd prefer we migrate to, for the compile tests that can't just become runtests.</p>\n<hr>\n<p>So I think this is what I'm converging on, at least:</p>\n<ul>\n<li>We can land something like this, maybe just without the post-fixup-on-VCode-text stuff but rather with a clean alternate output mode always used for compile tests</li>\n<li>We can work over time to move to more specific and fewer compile tests, just for those tests that are actually trying to test a property of the output beyond \"it works\"; this can be a gradual effort and doesn't need to block things here</li>\n</ul>\n<p>Thoughts?</p>\n</blockquote>",
        "id": 265339037,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639765509
    },
    {
        "content": "<p>cfallin edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-996929235\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>@alexcrichton , it's interesting (and good to know) that this sort of thing is used successfully elsewhere, thanks!</p>\n<p>Specific thoughts below; my last reservation (specifics of how the parsing works) was the most concrete and I think there's a reasonable way to address it.</p>\n<blockquote>\n<p>Otherwise the point about getting sloppy about updates is probably subjective. Subjectively I have never seen this be a problem in other projects I've worked on. While theoretically possible this is sort of what code review is for and if a code reviewer is ignoring code then I'm not sure that there's much we can do about that.</p>\n</blockquote>\n<p>Yeah, that's fair!</p>\n<blockquote>\n<p>I think I'd also disagree that this is brittle. This is testing a 100% match which means there's little room for deviation. There's some arguably weird heuristics applied to the <code>Debug</code> output to make the assertions prettier in the test files, but if the <code>Debug</code> output ever changes then all tests will instantly fail. It's pretty easy to either update the output-massaging code or to update all tests en-masse when the output changes in this regard. If we expect rapid changes to <code>Debug</code> to happen over time which would require tedious maintenance of \"map the program output to the test expectation\" then we should invest in a more proper \"map the program output to a string\" implementation that doesn't use <code>Debug</code>. Basically I've found that it's little consequence to be \"sloppy\" with creating the textual what's-expected-from-the-test because if you mess up tests fail and if you succeed then you can see the diff of what the output actually is.</p>\n</blockquote>\n<p>I think that much of my reservation actually came just from the detailed parsing that's happening e.g. <a href=\"https://github.com/alexcrichton/wasmtime/blob/913935adf5c45facbeb113a4ae384910e82c0c97/cranelift/filetests/src/test_compile.rs#L123-L128\">here</a>. That feels like unnecessary coupling. I actually agree that a stable canonical \"string representation of VCode\" with less output meant for human readability is the better answer here: just output like</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">block0</span>:\n  <span class=\"nc\">add</span><span class=\"w\"> </span><span class=\"n\">ra</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">rb</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">rc</span><span class=\"w\"></span>\n<span class=\"w\">  </span><span class=\"n\">cmp</span><span class=\"w\"> </span><span class=\"n\">rd</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">re</span><span class=\"w\"></span>\n<span class=\"w\">  </span><span class=\"n\">jne</span><span class=\"w\"> </span><span class=\"n\">block1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">block2</span><span class=\"w\"></span>\n<span class=\"n\">block1</span>:\n  <span class=\"o\">..</span><span class=\"p\">.</span><span class=\"w\"></span>\n</code></pre></div>\n<p>would lead to clearer diffs I think. If we implement this as a method on <code>VCode</code> and then use this unconditionally for filetests rather than only in a special \"precise mode\", then I think it's a great idea!</p>\n<blockquote>\n<p>Lurking in: I don't know the history here, but has something like LLVM's FileCheck tool been considered? [ ... ] It also makes explicit what the test is actually testing (e.g. these particular instructions are generated in this order)</p>\n</blockquote>\n<p>The <code>filetest</code> infra was written by @sunfishcode (EDIT: I'm told actually largely Jakob Stoklund, the original Cranelift author, so it goes way back) with direct inspiration from LLVM's filecheck, IIRC (<a href=\"https://crates.io/crates/filecheck\">crate</a>). It does actually have more features than we give it credit for / use regularly currently -- full regexes, and ability to flexibly match lines in certain sequences while skipping others. (The last bit is used to skip over extraneous VCode debug output but the rest, mostly not, at least in tests I've written.)</p>\n<p>It's a good point that a lot of this pain goes away if we write more targeted tests: if we want to test that instruction X is emitted after Y, or that lowering an operator results in specifically instruction Z, we can test that without asserting on the whole function body. That's a different kind of test than we have today but it's what I'd prefer we migrate to, for the compile tests that can't just become runtests.</p>\n<hr>\n<p>So I think this is what I'm converging on, at least:</p>\n<ul>\n<li>We can land something like this, maybe just without the post-fixup-on-VCode-text stuff but rather with a clean alternate output mode always used for compile tests</li>\n<li>We can work over time to move to more specific and fewer compile tests, just for those tests that are actually trying to test a property of the output beyond \"it works\"; this can be a gradual effort and doesn't need to block things here</li>\n</ul>\n<p>Thoughts?</p>\n</blockquote>",
        "id": 265350473,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639770206
    },
    {
        "content": "<p>abrown <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-997101506\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>No real opinion here except that in most of those compile tests I would like to retain the \"meaning\" of the current tests--whether that means that they need to use <code>filecheck</code> better to remove, e.g., <code>%xmm4</code> or whether the tests change by blessing them is not too important to me.</p>\n</blockquote>",
        "id": 265378863,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639786180
    },
    {
        "content": "<p>abrown edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-997101506\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>No real opinion here except that in most of those compile tests I would like to retain the \"meaning\" of the current tests--whether that means that they need to use <code>filecheck</code> better to remove, e.g., <code>%xmm4</code> or whether the tests change by blessing them is not too important to me. I am thinking specifically of test cases where we check that a certain instruction sequence is emitted.</p>\n</blockquote>",
        "id": 265378981,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1639786258
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-998080717\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>I'm happy to assert on any output myself, I only did the current custom filtering to make it be a smaller diff. @cfallin would you prefer that the output isn't massaged at all and instead it's asserted raw? I think it's basically <a href=\"https://github.com/bytecodealliance/wasmtime/blob/ad6f76a789cf93ca01970fe5802a5f67864cc440/cranelift/codegen/src/machinst/vcode.rs#L755-L804\">this output</a> which is generated as part of the <code>set_disasm(true)</code> on the compilation context. There's not a ton of infrastructure for configuring that output but I can hack stuff in if you'd prefer to assert on a cleaner output too.</p>\n</blockquote>",
        "id": 265585182,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1640017600
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-998314438\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>@alexcrichton maybe as a transitional step we can just land something that records the whole VCode pretty-print output and asserts on it; that's the effect we were going for anyway, and \"precise mode\" might as well assert complete equality. When we eventually have more tests that look for just certain features then we can go back to regexes, partial line matches, etc., I think.</p>\n<p>I don't mind a big diff that updates all the tests as long as it's just a mechanical update (maybe in a separate commit just so we can see clean tests before/after)!</p>\n<p>We can think about ways to clean up the pretty-print in a separate issue; the <code>PrettyPrint</code> trait is part of <a href=\"http://regalloc.rs\">regalloc.rs</a> which makes it a bit intertwined so maybe I can just do this as part of the eventual regalloc2 overhaul. Anyway, on further thought, it's definitely out-of-scope here. Thanks!</p>\n</blockquote>",
        "id": 265622833,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1640038885
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-998890674\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>Sure thing, updated with that! I also added some more logic necessary that I forgot from earlier where if multiple edits to one file are needed they'll need to be coordinated.</p>\n</blockquote>",
        "id": 265700154,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1640101794
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612#issuecomment-999685875\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3612\">issue #3612</a>:</p>\n<blockquote>\n<p>I realize now though it's probably best to make sure others are on-board with this, so I'll bring this up in the next cranelift meeting before merging.</p>\n</blockquote>",
        "id": 265820447,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1640188754
    }
]