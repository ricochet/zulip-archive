[
    {
        "content": "<p>Duslia opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>When I try to use tokio to scale wasmtime horizontally, I found that wasmtime performance drops significantly. This is my performance data:<br>\n| thread and task | data |<br>\n| --- | ----------- |<br>\n| thread 1, task 1      | 11w/task       |<br>\n| thread 3, task 3      | 4w/task       |<br>\n| thread 8, task 8      | 1w/task       |</p>\n<p>This is my code: <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787\">https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787</a> .</p>\n<p>And the wasm code is just a function return 1. </p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">package</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"></span>\n\n<span class=\"c1\">//export echo</span>\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">echo</span><span class=\"p\">(</span><span class=\"n\">ctxLen</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">reqLen</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"c1\">//nolint</span>\n<span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n</blockquote>",
        "id": 292385325,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1659955291
    },
    {
        "content": "<p>Duslia edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>When I try to use tokio to scale wasmtime horizontally, I found that wasmtime performance drops significantly. This is my performance data:<br>\n| thread and task | data |<br>\n| --- | ----------- |<br>\n| thread 1, task 1      | 11w/task       |<br>\n| thread 3, task 3      | 4w/task       |<br>\n| thread 8, task 8      | 1w/task       |</p>\n<p>This is my test code: <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787\">https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787</a> .</p>\n<p>And the wasm code is just a function return 1. </p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">package</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"></span>\n\n<span class=\"c1\">//export echo</span>\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">echo</span><span class=\"p\">(</span><span class=\"n\">ctxLen</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">reqLen</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"c1\">//nolint</span>\n<span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n</blockquote>",
        "id": 292385429,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1659955353
    },
    {
        "content": "<p>Duslia edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>When I try to use tokio to scale wasmtime horizontally, I found that wasmtime performance drops significantly. It looks like there are some shared resources inside. And no matter how many threads I open, The total number of execute num is close. This is my performance data:<br>\n| thread and task | data |<br>\n| --- | ----------- |<br>\n| thread 1, task 1      | 11w/task       |<br>\n| thread 3, task 3      | 4w/task       |<br>\n| thread 8, task 8      | 1w/task       |</p>\n<p>This is my test code: <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787\">https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787</a> .</p>\n<p>And the wasm code is just a function return 1. </p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">package</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"></span>\n\n<span class=\"c1\">//export echo</span>\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">echo</span><span class=\"p\">(</span><span class=\"n\">ctxLen</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">reqLen</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"c1\">//nolint</span>\n<span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n</blockquote>",
        "id": 292394732,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1659961690
    },
    {
        "content": "<p>Duslia edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>When I try to use tokio to scale wasmtime horizontally, I found that wasmtime performance drops significantly. It looks like there are some shared resources inside. And no matter how many threads I open, The total number of execute num is almost the same. This is my performance data:<br>\n| thread and task | data |<br>\n| --- | ----------- |<br>\n| thread 1, task 1      | 11w/task       |<br>\n| thread 3, task 3      | 4w/task       |<br>\n| thread 8, task 8      | 1w/task       |</p>\n<p>This is my test code: <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787\">https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787</a> .</p>\n<p>And the wasm code is just a function return 1. </p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">package</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"></span>\n\n<span class=\"c1\">//export echo</span>\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">echo</span><span class=\"p\">(</span><span class=\"n\">ctxLen</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">reqLen</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"c1\">//nolint</span>\n<span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n</blockquote>",
        "id": 292395653,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1659962266
    },
    {
        "content": "<p>Duslia edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>When I try to use tokio to scale wasmtime horizontally, I found that wasmtime performance drops significantly. It looks like there are some shared resources inside. And no matter how many threads I open, The total number of execute num is almost the same. This is my performance data:<br>\n| thread and task | data | total execute nums |<br>\n| --- | ----------- | -------- |<br>\n| thread 1, task 1      | 11w/task       | 11w<br>\n| thread 3, task 3      | 4w/task       | 12w<br>\n| thread 8, task 8      | 1w/task       | 8w</p>\n<p>This is my test code: <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787\">https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787</a> .</p>\n<p>And the wasm code is just a function return 1. </p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">package</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"></span>\n\n<span class=\"c1\">//export echo</span>\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">echo</span><span class=\"p\">(</span><span class=\"n\">ctxLen</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">reqLen</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"c1\">//nolint</span>\n<span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n</blockquote>",
        "id": 292396759,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1659962916
    },
    {
        "content": "<p>Duslia edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>When I try to use tokio to scale wasmtime horizontally, I found that wasmtime performance drops significantly. It looks like there are some shared resources inside. And no matter how many threads I open, the total number of execute num is almost the same. This is my performance data:<br>\n| thread and task | data | total execute nums |<br>\n| --- | ----------- | -------- |<br>\n| thread 1, task 1      | 11w/task       | 11w<br>\n| thread 3, task 3      | 4w/task       | 12w<br>\n| thread 8, task 8      | 1w/task       | 8w</p>\n<p>This is my test code: <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787\">https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787</a> .</p>\n<p>And the wasm code is just a function return 1. </p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">package</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"></span>\n\n<span class=\"c1\">//export echo</span>\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">echo</span><span class=\"p\">(</span><span class=\"n\">ctxLen</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">reqLen</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"c1\">//nolint</span>\n<span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n</blockquote>",
        "id": 292396894,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1659963009
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1208288255\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>Thanks for the report! Could you share the Wasmtime version you're using as well as the specific <code>*.wasm</code> file that you're benchmarking?</p>\n</blockquote>",
        "id": 292418545,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1659973081
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1208303055\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>That being said what you're almost certainly running into is limits on concurrently running <code>mmap</code> in multiple threads within one process. Allocation of fiber stacks for asynchronously executing WebAsembly primarily goes through <code>mmap</code> right now and unmapping a stack requires broadcasting to other threads with an IPI which slows down concurrent execution.</p>\n<p>One improvement that you can do to your benchmark is to use the pooling allocator instead of the on-demand allocator. Otherwise though improvements need to come from the kernel itself (assuming you're running Linux, which would also be great to specify in the issue description). Historically the kernel has prototyped a <code>madvisev</code> syscall which allows batch unmappings that Wasmtime could use with the pooling allocator. We've tested this historically and it's shown to greatly increase the throughput in situations like this. This isn't landed in Linux, however.</p>\n<p>Finally it depends on your use case of whether this comes up much in practice. Typically the actual work performed by a wasm module dwarfs the cost of the memory mappings since the wasm modules live for a longer time than \"return 1\". If that's the case then this microbenchmark isn't necessarily indicative of performance and it's recommended to use a more realistic workload for your use case. If this sort of short-running computation is common for your embedding, however, then multithreaded execution will currently have the impact that you're measuring here.</p>\n</blockquote>",
        "id": 292420430,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1659973925
    },
    {
        "content": "<p>Duslia <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1209019368\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>Thanks!! I got it. </p>\n</blockquote>",
        "id": 292535609,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660030459
    },
    {
        "content": "<p>Duslia <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1209346760\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>I'm trying to comment out these two lines and it can scale horizontally with a high performance. <br>\n&lt;img width=\"855\" alt=\"image\" src=\"<a href=\"https://user-images.githubusercontent.com/37136584/183650680-efd55dcf-b8ad-4748-ae9d-3382b96caa74.png\">https://user-images.githubusercontent.com/37136584/183650680-efd55dcf-b8ad-4748-ae9d-3382b96caa74.png</a>\"&gt;<br>\nIs the function of these lines of code to prevent the previous data from being obtained in the next execution?</p>\n</blockquote>",
        "id": 292572703,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660049743
    },
    {
        "content": "<p>Duslia edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1209346760\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>I'm trying to comment out these two lines and it can scale horizontally with a high performance. <br>\n&lt;img width=\"855\" alt=\"image\" src=\"<a href=\"https://user-images.githubusercontent.com/37136584/183650680-efd55dcf-b8ad-4748-ae9d-3382b96caa74.png\">https://user-images.githubusercontent.com/37136584/183650680-efd55dcf-b8ad-4748-ae9d-3382b96caa74.png</a>\"&gt;<br>\nI 'd like to ask that is the function of these lines of code to prevent the previous data from being obtained in the next execution?</p>\n</blockquote>",
        "id": 292572809,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660049770
    },
    {
        "content": "<p>bjorn3 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1209360589\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>Yes, without that call the old data would be preserved when reusing a memory region as linear memory for the wasm module.</p>\n</blockquote>",
        "id": 292574910,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660050522
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1209461708\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>Ah ok that's a good confirmation that <code>madvise</code> and IPIs are indeed likely your scaling bottleneck. In that case there's unfortunately not much that can be done right now that Wasmtime can do. <a href=\"https://lore.kernel.org/all/20151208173825.GA1351950@devbig084.prn1.facebook.com/T/\">This is some (old) discussion</a> about a possible <code>madvisev</code> syscall which would largely fix this scaling bottleneck within Wasmtime. This was confirmed with a modified kernel with that patch and developing a version of Wasmtime that uses <code>madvisev</code>.</p>\n<p>In the meantime though this is something that basically just needs to be taken into account when capacity planning with Wasmtime. If you're willing assistance in getting a patch such as that into the Linux kernel (or investigating other routes of handling this bottleneck) would be greatly appreciated!</p>\n</blockquote>",
        "id": 292589791,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660055577
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1209462274\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>Oh sorry, and to confirm the <code>madvise</code> call is absolutely critical for safety, it cannot be removed and still have a secure sandbox for wasm.</p>\n</blockquote>",
        "id": 292589902,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660055605
    },
    {
        "content": "<p>Duslia <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1209526153\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>OK. I will think about how to solve it. Thanks a lot!!</p>\n</blockquote>",
        "id": 292599309,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660058586
    },
    {
        "content": "<p>ihciah <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1210084056\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>I'm wondering if the following 2 solutions may be accepted?</p>\n<ol>\n<li>Call madvise with io_uring: It saves syscall but I don't know if it will work to avoid scaling bottleneck.</li>\n<li>Maintain 2 index groups, one is dirty, the other one is clean. On drop, the index is put into dirty group and try to merge with others, and won't be madvised instantly. On index allocation, try to get from the clean one, and if there's no available in it, try to obtain a continuous range from dirty group and call madvise, then return one of it back and put all the index left inside the range into the clean group. In this solution, we can call less madvise by clean stacks in batch.</li>\n</ol>\n</blockquote>",
        "id": 292686029,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660099202
    },
    {
        "content": "<p>bjorn3 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1210238785\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<p>Call madvise with io_uring: It saves syscall but I don't know if it will work to avoid scaling bottleneck.</p>\n</blockquote>\n<p>As I understand it the IPI is the bottlwneck, not the syscal overhead. Every time madvise is run all cores running the wasmtime process have to be temporarily interupted and suspended while the page table is being modified. This scales really badly.</p>\n</blockquote>",
        "id": 292702038,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660114274
    },
    {
        "content": "<p>bjorn3 edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1210238785\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<p>Call madvise with io_uring: It saves syscall but I don't know if it will work to avoid scaling bottleneck.</p>\n</blockquote>\n<p>As I understand it the IPI is the bottlwneck, not the syscall overhead. Every time madvise is run all cores running the wasmtime process have to be temporarily interupted and suspended while the page table is being modified. This scales really badly. io_uring can't do anything against this.</p>\n</blockquote>",
        "id": 292702067,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660114310
    },
    {
        "content": "<p>ihciah <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1210287374\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>Call madvise with io_uring: It saves syscall but I don't know if it will work to avoid scaling bottleneck.</p>\n</blockquote>\n<p>As I understand it the IPI is the bottlwneck, not the syscall overhead. Every time madvise is run all cores running the wasmtime process have to be temporarily interupted and suspended while the page table is being modified. This scales really badly. io_uring can't do anything against this.</p>\n</blockquote>\n<p>Thanks for the explain! I also checked the kernel code and found in func <code>io_madvise</code> of io_uring.c, it just call <code>do_madvise</code>, the lock will be acquired inside <code>do_madvise</code>, which means there's no aggregation with it.<br>\nTo do madvise aggregation without modifying kernel, what about the solution 2? I implemented a demo here(not finished yet): <a href=\"https://github.com/ihciah/wasmtime/commit/071f5d9978d94ee669d72c90bf2d6f2728324303\">https://github.com/ihciah/wasmtime/commit/071f5d9978d94ee669d72c90bf2d6f2728324303</a></p>\n</blockquote>",
        "id": 292707204,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660117382
    },
    {
        "content": "<p>bjorn3 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1210300041\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>I might be misunderstanding, but isn't that what the pooling allocator does?</p>\n</blockquote>",
        "id": 292708633,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660118101
    },
    {
        "content": "<p>ihciah <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1210315831\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<p>I might be misunderstanding, but isn't that what the pooling allocator does?</p>\n</blockquote>\n<p>The pool inside the wasmtime allocates a big memory one time in a single call, and manages it by maintaining <code>index</code> of memory pages manually. Each time the memory page(memory_pages, table_pages, stack_pages) dropped, it will clear the memory content by calling decommit-&gt;madvise, and mark the <code>index</code> available for the later allocation. And here is the problem.<br>\nWhat I want to impl is to make the indices to recycle continues and clear the memory in batch. I think it is a kind aggregation in userspace.<br>\n(I started reading wasmtime code just 2 from days ago. Maybe there's something I misunderstand..)</p>\n</blockquote>",
        "id": 292710671,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660118977
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1210796431\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<p>Call madvise with io_uring</p>\n</blockquote>\n<p>While we haven't directly experimented with this I think as you've discovered we took a look and concluded it probably wouldn't help.</p>\n<blockquote>\n<p>Maintain 2 index groups, one is dirty, the other one is clean.</p>\n</blockquote>\n<p>This we have actually experimented with historically. Our conclusion was that it did not actually help all that much. We found that the cost of an <code>madvise</code> scaled with the size of the region being advised, which meant that when we batched everything up it took effectively just as long as doing everything individually. We had various hypotheses for why this was the case but we didn't bottom it out 100% at the time.</p>\n<p>Our testing with <code>madvisev</code> is mostly what led us to stop investigating this issue because <code>madvisev</code> so clearly fixed the issue relative to all other workarounds.</p>\n</blockquote>",
        "id": 292771160,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660143492
    },
    {
        "content": "<p>ihciah <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1211022435\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<p>We found that the cost of an madvise scaled with the size of the region being advised</p>\n</blockquote>\n<p>Thank you for sharing past experiments.</p>\n<p>But it still confuse me that advising multiple regions with <code>madvisev</code> is better than <code>madvise</code> with a merged region. If this is true, does this means we can impl a better <code>madvise</code> in kernel by simply split the region and do it one by one?</p>\n</blockquote>",
        "id": 292798527,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660152216
    },
    {
        "content": "<p>pchickey <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1211036418\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>You can't merge the regions to madvise - they're not contiguous.</p>\n</blockquote>",
        "id": 292800501,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660152961
    },
    {
        "content": "<p>ihciah <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1211063859\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<p>You can't merge the regions to madvise - they're not contiguous.</p>\n</blockquote>\n<p>I mean maybe we can make the indices as contiguous as possible and then we can do madvise on the biggest contiguous region when alloc. I don't know if this will work.</p>\n</blockquote>",
        "id": 292803757,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660154190
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1211098736\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>IIRC we were just as confused as you are. I remember though reorganizing the pooling allocator to put the table/memory/instance allocations all next to each other so each instance's index identified one large contiguous region of memory. In the benchmarks for this repository parallel instantiation actually got worse than the current performance on <code>main</code>. My guess at the time was that it had to do with the very large size of the <code>madvise</code> since <code>madvise</code> got more expensive as the region got larger.</p>\n<p>As for kernel improvements that's never something we've looked into beyond <code>madvisev</code></p>\n</blockquote>",
        "id": 292809237,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660156189
    },
    {
        "content": "<p>ihciah <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1212700151\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>I finished my experiment and result shows the same conclusion.<br>\nWhat I did: clear stack pool in batch(<a href=\"https://github.com/ihciah/wasmtime/tree/main\">https://github.com/ihciah/wasmtime/tree/main</a>)<br>\nResult: running return 1 demo(on a KVM vm)</p>\n<ul>\n<li>Original 1 core: 117 kQPS</li>\n<li>Original 3 cores: 66 kQPS * 3</li>\n<li>Exp 1 core: 133 kQPS</li>\n<li>Exp 3 cores: 52kQPS * 3</li>\n</ul>\n<p>Under 1 core, the modified version has slightly better performance(the madvise batch is 1000 times original); but under 3 cores it becomes worse.</p>\n<blockquote>\n<p>My guess at the time was that it had to do with the very large size of the madvise since madvise got more expensive as the region got larger.</p>\n</blockquote>\n<p>Also, I changed stack pool size 1000 to bigger values, <code>size=2048</code> =&gt; 20ms and <code>size=4096</code> =&gt; 45ms, which matches your guess.<br>\nI haven't looked closely at the <code>madvisev</code> impl yet. Comparing to calling <code>madvise</code>, what does it saves? It still need the same IPI in theory.</p>\n</blockquote>",
        "id": 293041135,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660275737
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1215075763\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>Always good to have independent confirmation, thanks for doing that!</p>\n<p>As for why <code>madvisev</code> improves the situation, I believe this happens because each call to <code>madvise</code> issues an IPI. With <code>madvisev</code> only one IPI is necessary still. This means that instead of N calls to <code>madvisev</code> and N IPIs there's only one call to <code>madvisev</code> with only one IPI. From the kernel's perspective all the calling process needs is to know that by the time the syscall returns that the address space is as-expected in all threads, but during execution of the syscall there's a lot more flexibility about precisely what all threads see.</p>\n</blockquote>",
        "id": 293529663,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660573604
    },
    {
        "content": "<p>ihciah <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1220354306\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>Thanks for the explanation. But I guess IPI cost(or other bottleneck) is not linear with syscall times but the memory ranges since doing madvise with twice bigger memory a little bit cost more than twice longer time.</p>\n<p>It seems there's no easy way to save the madvise cost and avoid its bottleneck for general usage...<br>\nSince users may control the instance isolation by their own, for example they can make sure only the same tenant and the same wasm binary share instances. Under these circumstances doing madvise maybe unneccessary.<br>\nIs it acceptable to add an option to disable madvise(madvise can be still enabled by default, and the option can only be touched when some features like \"dangerous\" enabled)?</p>\n</blockquote>",
        "id": 294216031,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660894812
    },
    {
        "content": "<p>bjorn3 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1220364445\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<p>Since users may control the instance isolation by their own, for example they can make sure only the same tenant and the same wasm binary share instances. Under these circumstances doing madvise maybe unneccessary.</p>\n</blockquote>\n<p>In that case you could reuse the <code>Instance</code> instead of recreate it every time, right? Not calling <code>madvise</code> is effectively equivalent to reusing the <code>Instance</code> while resetting all tables and wasm globals (actual global variables are in the linear memory and as such not reset), but not the linear memory, in which case you might as well reset nothing. That prevents everything getting out of sync and most observable state isn't reset anyway if you don't do <code>madvise</code>.</p>\n</blockquote>",
        "id": 294217244,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660895559
    },
    {
        "content": "<p>ihciah <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1220382599\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<blockquote>\n<p>Not calling madvise is effectively equivalent to reusing the Instance while resetting all tables and wasm globals (actual global variables are in the linear memory and as such not reset), but not the linear memory, in which case you might as well reset nothing.</p>\n</blockquote>\n<p>I see. There are 3 callers for <code>decommit</code>, the <code>decommit_memory_pages</code> and <code>decommit_table_pages</code> are with the <code>instance</code> lifecycle. So we may not affect them.<br>\nThe hot path is the third caller <code>decommit_stack_pages</code>. Every call for <code>func.call_async</code> will do it. So we may only disable <code>decommit_stack_pages</code> optionally.</p>\n</blockquote>",
        "id": 294219583,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660896730
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1221042189\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>Using <code>madvise</code> to clear linear memory and tables is required for correctness and we can't add a knob to turn that off, but I think it would be reasonable to add a knob for the stacks to avoid <code>madvise</code> since that's purely a defense-in-depth thing and is not required for correctness.</p>\n</blockquote>",
        "id": 294352189,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1660938591
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637#issuecomment-1334598887\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>The <code>*_keep_resident</code> options added recently should additionally help by reducing page faults and therefore contention on kernel-level locks. There's also work towards optimizing bounds-checking-mode in Cranelift to avoid pagefaults/mprotect even more to reduce contention even further.</p>\n<p>Otherwise though this is a known issue and isn't something where there's any easy wins on the table we know of that haven't been applied yet. PRs and more discussion around this are of course always welcome but I'm going to go ahead and close this since there's not a whole lot more that can be done at this time.</p>\n</blockquote>",
        "id": 313402883,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1669939491
    },
    {
        "content": "<p>alexcrichton closed <a href=\"https://github.com/bytecodealliance/wasmtime/issues/4637\">issue #4637</a>:</p>\n<blockquote>\n<p>When I try to use tokio to scale wasmtime horizontally, I found that wasmtime performance drops significantly. It looks like there are some shared resources inside. And no matter how many threads I open, the total number of execute num is almost the same. This is my performance data:<br>\n| thread and task | data | total execute nums |<br>\n| --- | ----------- | -------- |<br>\n| thread 1, task 1      | 11w/task       | 11w<br>\n| thread 3, task 3      | 4w/task       | 12w<br>\n| thread 8, task 8      | 1w/task       | 8w</p>\n<p>This is my test code: <a href=\"https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787\">https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2021&amp;gist=e9fc13a7d68b80afdc07076482a5b787</a> .</p>\n<p>And the wasm code is just a function return 1. </p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">package</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"></span>\n\n<span class=\"c1\">//export echo</span>\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">echo</span><span class=\"p\">(</span><span class=\"n\">ctxLen</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">reqLen</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">int32</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"c1\">//nolint</span>\n<span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"n\">func</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n</blockquote>",
        "id": 313402884,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1669939492
    }
]