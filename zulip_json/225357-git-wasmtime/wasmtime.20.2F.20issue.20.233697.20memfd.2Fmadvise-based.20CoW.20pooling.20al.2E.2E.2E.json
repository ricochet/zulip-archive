[
    {
        "content": "<p>github-actions[bot] <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1016013230\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<h4>Subscribe to Label Action</h4>\n<p>cc @peterhuene</p>\n<p>&lt;details&gt;<br>\nThis issue or pull request has been labeled: \"wasmtime:api\"</p>\n<p>Thus the following users have been cc'd because of the following labels:</p>\n<ul>\n<li>peterhuene: wasmtime:api</li>\n</ul>\n<p>To subscribe or unsubscribe from this label, edit the &lt;code&gt;.github/subscribe-to-label.json&lt;/code&gt; configuration file.</p>\n<p><a href=\"https://github.com/bytecodealliance/subscribe-to-label-action\">Learn more.</a><br>\n&lt;/details&gt;</p>\n</blockquote>",
        "id": 268483937,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1642558527
    },
    {
        "content": "<p>koute <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1016181422\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>I've hooked up your implementation to our benchmarks; here are the results:</p>\n<p>![call_empty_function](<a href=\"https://user-images.githubusercontent.com/246574/150087000-f1ecfa32-25de-490d-810e-c3e80c85fa94.png\">https://user-images.githubusercontent.com/246574/150087000-f1ecfa32-25de-490d-810e-c3e80c85fa94.png</a>)</p>\n<p>![dirty_1mb_of_memory](<a href=\"https://user-images.githubusercontent.com/246574/150087009-c371470c-391f-449e-9f17-7c6d90d620ba.png\">https://user-images.githubusercontent.com/246574/150087009-c371470c-391f-449e-9f17-7c6d90d620ba.png</a>)</p>\n<p>Legend:</p>\n<ul>\n<li><code>instance_pooling_memfd</code> - this PR</li>\n<li><code>native_instance_reuse</code> - <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3691\">https://github.com/bytecodealliance/wasmtime/pull/3691</a></li>\n<li><code>recreate_instance</code> - create a fresh instance with <code>InstanceAllocationStrategy::OnDemand</code> strategy</li>\n<li><code>instance_pooling_with_uffd</code>: create a fresh instance with <code>InstanceAllocationStrategy::Pooling</code> strategy with uffd turned on and without this PR</li>\n<li><code>instance_pooling_without_uffd</code>: create a fresh instance with <code>InstanceAllocationStrategy::Pooling</code> strategy without uffd turned on and without this PR</li>\n</ul>\n<p>And here's the comparison in raw numeric values (times are in microseconds):</p>\n<h2>call_empty_function</h2>\n<table>\n<thead>\n<tr>\n<th>threads</th>\n<th>instance_pooling_with_uffd</th>\n<th>instance_pooling_without_uffd</th>\n<th>recreate_instance</th>\n<th>native_instance_reuse</th>\n<th>instance_pooling_memfd</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>41</td>\n<td>78</td>\n<td>80</td>\n<td>4</td>\n<td>27</td>\n</tr>\n<tr>\n<td>2</td>\n<td>63</td>\n<td>110</td>\n<td>114</td>\n<td>11</td>\n<td>43</td>\n</tr>\n<tr>\n<td>4</td>\n<td>76</td>\n<td>172</td>\n<td>188</td>\n<td>17</td>\n<td>66</td>\n</tr>\n<tr>\n<td>8</td>\n<td>123</td>\n<td>360</td>\n<td>439</td>\n<td>24</td>\n<td>89</td>\n</tr>\n<tr>\n<td>16</td>\n<td>286</td>\n<td>695</td>\n<td>1117</td>\n<td>36</td>\n<td>127</td>\n</tr>\n</tbody>\n</table>\n<h2>dirty_1mb_of_memory</h2>\n<table>\n<thead>\n<tr>\n<th>threads</th>\n<th>instance_pooling_with_uffd</th>\n<th>instance_pooling_without_uffd</th>\n<th>recreate_instance</th>\n<th>native_instance_reuse</th>\n<th>instance_pooling_memfd</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>312</td>\n<td>216</td>\n<td>221</td>\n<td>144</td>\n<td>238</td>\n</tr>\n<tr>\n<td>2</td>\n<td>844</td>\n<td>325</td>\n<td>334</td>\n<td>209</td>\n<td>385</td>\n</tr>\n<tr>\n<td>4</td>\n<td>1400</td>\n<td>526</td>\n<td>549</td>\n<td>306</td>\n<td>453</td>\n</tr>\n<tr>\n<td>8</td>\n<td>1979</td>\n<td>1047</td>\n<td>1168</td>\n<td>581</td>\n<td>618</td>\n</tr>\n<tr>\n<td>16</td>\n<td>3084</td>\n<td>1814</td>\n<td>1954</td>\n<td>765</td>\n<td>840</td>\n</tr>\n</tbody>\n</table>\n<p>So there's still some way to go performance-wise. (:</p>\n<blockquote>\n<p>Thanks to Jan on Zulip (are you also @koute from #3691?) for the initial<br>\nidea/inspiration!</p>\n</blockquote>\n<p>Yes that's me. (:</p>\n</blockquote>",
        "id": 268503417,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1642579688
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1016656825\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Can you share the branch and/or code to reproduce the results with this PR? I was able to reproduce somewhat locally where I got:</p>\n<ul>\n<li><code>call_empty_function_from_kusama_runtime_with_legacy_instance_reuse_on_1_threads</code> - 97us</li>\n<li><code>call_empty_function_from_kusama_runtime_with_native_instance_reuse_on_1_threads</code> - 7.1us (your PR)</li>\n<li><code>call_empty_function_from_kusama_runtime_with_native_instance_reuse_on_1_threads</code> - 32us (this PR)</li>\n</ul>\n<p>I think that roughly aligns with what you measured, but I wanted to confirm by peeking at the code if possible.</p>\n<p>Some quick benchmarking shows that <a href=\"https://github.com/bytecodealliance/wasmtime/blob/2649d2352c415979ea0fe7f43eacb2f3f65d771c/crates/runtime/src/instance/allocator.rs#L463\">this function</a> is quite hot in this PR. That makes sense to me because @koute your PR skips that function entirely with the reuse mechanism you've implemented.</p>\n<p>I couldn't for sure drill down into what's causing the issue but my best guess is that <a href=\"https://github.com/bytecodealliance/wasmtime/blob/2649d2352c415979ea0fe7f43eacb2f3f65d771c/crates/runtime/src/instance/allocator.rs#L514-L539\">this loop</a> is the slow part. That's a one-time-initialization which is pretty branch-y which happens once-per-function in a module, and the modules you're loading are quite large.</p>\n<p>I personally think that thtis PR's allocation strategy is more viable in terms of long term maintenance so I'd like to continue to measure this and push on this if we can, but \"the numbers don't lie\" and your PR has some very impressive numbers! I think we should be working towards that as a goal because that would be awesome to reinstantiate instances that fast. P</p>\n<p>Personally I think it would be fruitful to diff the performance between these two PRs. Without memfd enabled I was measuring like 132us for the benchmark above, so I think that this PR shows that 100us of that can be shaved off with memfd/cow, but it looks like there's a remaining ~25us or so to get shaved off. I believe the function-initialization is probably one of those issues, but there may be others lurking. Quantifying the difference and where the remaining 25us or so can go I think would be helpful to figure out the best viable implementation strategy going forward.</p>\n</blockquote>",
        "id": 268567838,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1642610694
    },
    {
        "content": "<p>alexcrichton edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1016656825\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Can you share the branch and/or code to reproduce the results with this PR? I was able to reproduce somewhat locally where I got:</p>\n<ul>\n<li><code>call_empty_function_from_kusama_runtime_with_legacy_instance_reuse_on_1_threads</code> - 97us</li>\n<li><code>call_empty_function_from_kusama_runtime_with_native_instance_reuse_on_1_threads</code> - 7.1us (your PR)</li>\n<li><code>call_empty_function_from_kusama_runtime_with_native_instance_reuse_on_1_threads</code> - 32us (this PR)</li>\n</ul>\n<p>I think that roughly aligns with what you measured, but I wanted to confirm by peeking at the code if possible.</p>\n<p>Some quick benchmarking shows that <a href=\"https://github.com/bytecodealliance/wasmtime/blob/2649d2352c415979ea0fe7f43eacb2f3f65d771c/crates/runtime/src/instance/allocator.rs#L463\">this function</a> is quite hot in this PR. That makes sense to me because @koute your PR skips that function entirely with the reuse mechanism you've implemented.</p>\n<p>I couldn't for sure drill down into what's causing the issue but my best guess is that <a href=\"https://github.com/bytecodealliance/wasmtime/blob/2649d2352c415979ea0fe7f43eacb2f3f65d771c/crates/runtime/src/instance/allocator.rs#L514-L539\">this loop</a> is the slow part. That's a one-time-initialization which is pretty branch-y which happens once-per-function in a module, and the modules you're loading are quite large.</p>\n<p>I personally think that this PR's allocation strategy is more viable in terms of long term maintenance so I'd like to continue to measure this and push on this if we can, but \"the numbers don't lie\" and your PR has some very impressive numbers! I think we should be working towards that as a goal because that would be awesome to reinstantiate instances that fast. </p>\n<p>Personally I think it would be fruitful to diff the performance between these two PRs. Without memfd enabled I was measuring like 132us for the benchmark above, so I think that this PR shows that 100us of that can be shaved off with memfd/cow, but it looks like there's a remaining ~25us or so to get shaved off. I believe the function-initialization is probably one of those issues, but there may be others lurking. Quantifying the difference and where the remaining 25us or so can go I think would be helpful to figure out the best viable implementation strategy going forward.</p>\n</blockquote>",
        "id": 268571722,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1642612115
    },
    {
        "content": "<p>koute <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1017164295\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>Can you share the branch and/or code to reproduce the results with this PR?</p>\n</blockquote>\n<p>Sure. Here's the branch:</p>\n<p><a href=\"https://github.com/koute/substrate/tree/master_wasmtime_benchmarks_with_cfallin_memfd_cow\">https://github.com/koute/substrate/tree/master_wasmtime_benchmarks_with_cfallin_memfd_cow</a></p>\n<p>And here's how to run them to get numbers for this PR (essentially the same as described in my PR, just with an extra cargo feature):</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">FORCE_WASMTIME_INSTANCE_POOLING</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"n\">rustup</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">nightly</span><span class=\"o\">-</span><span class=\"mi\">2021</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">01</span><span class=\"o\">-</span><span class=\"n\">x86_64</span><span class=\"o\">-</span><span class=\"n\">unknown</span><span class=\"o\">-</span><span class=\"n\">linux</span><span class=\"o\">-</span><span class=\"n\">gnu</span><span class=\"w\"> </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">bench</span><span class=\"w\"> </span><span class=\"o\">--</span><span class=\"n\">features</span><span class=\"w\"> </span><span class=\"n\">wasmtime</span><span class=\"p\">,</span><span class=\"n\">sc</span><span class=\"o\">-</span><span class=\"n\">executor</span><span class=\"o\">-</span><span class=\"n\">wasmtime</span><span class=\"o\">/</span><span class=\"n\">memfd</span><span class=\"o\">-</span><span class=\"n\">allocator</span><span class=\"w\"> </span><span class=\"n\">call_empty_function_from_kusama_runtime_with_recreate_instance_on_1_threads</span><span class=\"w\"></span>\n</code></pre></div>\n<p>If you have any further questions feel free to ask! (I can also make myself available for discussion on Zulip.)</p>\n<blockquote>\n<p>Some quick benchmarking shows that <a href=\"https://github.com/bytecodealliance/wasmtime/blob/2649d2352c415979ea0fe7f43eacb2f3f65d771c/crates/runtime/src/instance/allocator.rs#L463\">this function</a> is quite hot in this PR. That makes sense to me because @koute your PR skips that function entirely with the reuse mechanism you've implemented.</p>\n</blockquote>\n<p>Indeed! One of the reasons why I implemented my PR the way I did is because you have to punch through less layers of abstraction so it's easier to make it faster. (:</p>\n</blockquote>",
        "id": 268648348,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1642660700
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1018989359\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>OK, so I've spent some time digging into this more as well, and thinking about potential designs. My benchmarking (mainly with the <code>server</code> benchmark included here, tweaked to load a <code>spidermonkey.wasm</code> to stress instantiation more) also shows that the <code>VMCallerCheckedAnyfunc</code> initialization is a large part of the <code>VMContext</code> initialization time.</p>\n<p>A few thoughts:</p>\n<ul>\n<li>\n<p>At the highest level, there's the question of what re-instantiation strategy different use-cases require. At least in cases I'm familiar with, and as I understand it, the \"freshly instantiated for every request\" property is a hard security requirement. The instance-reuse PR #3691 relaxes the boundary somewhat by effectively emulating a fresh instantiation, but without going through all initialization; I remain pretty concerned with the risk here, especially as we add more Wasm features in the future, that we'll forget something and have a small but critical information leak. (In other words, it inverts the initialization: we have to positively erase state or else stale state might come through, rather than today where we'd have to explicitly copy data for it to leak.)</p>\n<p>That question feels biggest to me. If all use-cases are OK with reuse and accepting risk of leakage, then let's pursue something like your PR. But if there's still a case for the stronger isolation, then I think it's interesting to see if we can approach it from the \"other side\" -- making initialization from scratch as fast as possible.</p>\n</li>\n<li>\n<p>There is also the question of whether we want <em>both</em>, for different circumstances. Currently we seem to be taking for granted that \"the fastest approach wins\", or in other words, that if we can't get this approach to be at ~parity with #3691, then we pick that approach instead. But the requirements addressed by both are different; this one does more work, for lower risk-profile. Maybe we can optimize it to be close, I'm not sure; but if there is still a gap, I think we should consider them as solving slightly different problems and evaluate appropriately.</p>\n</li>\n<li>\n<p>All that said, I think it may be possible to optimize and come close. As @alexcrichton pointed out above, the major speed delta between the two is I think almost completely in the function-table initialization in the <code>VMContext</code>. (There won't be a significant difference in globals or tables unless the initializers are complex: the reuse-based PR splats a saved array of values over both, while this PR uses the ordinary from-scratch initialization.)</p>\n<p>We can't fully skip the function-table initialization; the <code>VMCallerCheckedAnyfunc</code>s are referred to by pointers in others' tables when one instance imports another's function, as far as I can tell. We also cannot share them between all instances of a module (which would have let us either add indirection to share one block of them, or else do a fast <code>memcpy</code>):</p>\n<ul>\n<li>This module's functions (possibly exported) embed the <code>vmctx</code> in the 3-tuple, so one of every 3 words needs to be specific to the <code>VMContext</code> we're building;</li>\n<li>Imported functions also may have different <code>vmctx</code>s for each instantiation (ie not just the default-callee vmctx for host functions).</li>\n</ul>\n<p>However, I think we can get away with initializing <em>lazily</em> in <code>wasmtime_runtime::Instance::get_caller_checked_anyfunc</code>; just check if null, and if so, lazily init then return. (Re <code>&amp;self</code> and concurrency, use atomic stores and store the checked-for-null ptr last; updates are idempotent so benign race is safe.) This is sort of loosely inspired by late-binding with dynamic linkers and such, except here the binding happens when someone else asks for the reference (ie during module linking or when filling in table entries), not when a call first occurs. I'm curious what @alexcrichton thinks of all this.</p>\n</li>\n</ul>\n<p>Anyway, thoughts on the above? Sorry for the lengthy braindump; I wanted to try to represent how I'm thinking about this and framing the design space in my head, is all. And @koute I want to repeat again a \"thank you\" for spawning all of this thinking and exploration!</p>\n</blockquote>",
        "id": 268922480,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1642811063
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1019077428\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>I just implemented the lazy-anyfunc-initialization scheme I mentioned above: b8bb1d5d85faeebb3fe275c9a6403fd42d76b8af</p>\n<p>It seems to help in my quick-and-dirty local testing; I'm curious what it will do to the benchmarks above but will save that benchmarking (and benchmarking in general with enough precision to have quotable numbers) for Monday :-)</p>\n</blockquote>",
        "id": 268941585,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1642833985
    },
    {
        "content": "<p>koute <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1019781998\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>At the highest level, there's the question of what re-instantiation strategy different use-cases require. At least in cases I'm familiar with, and as I understand it, the \"freshly instantiated for every request\" property is a hard security requirement. The instance-reuse PR <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3691\">Add copy-on-write based instance reuse mechanism #3691</a> relaxes the boundary somewhat by effectively emulating a fresh instantiation, but without going through all initialization; I remain pretty concerned with the risk here, especially as we add more Wasm features in the future, that we'll forget something and have a small but critical information leak. (In other words, it inverts the initialization: we have to positively erase state or else stale state might come through, rather than today where we'd have to explicitly copy data for it to leak.)</p>\n<p>That question feels biggest to me. If all use-cases are OK with reuse and accepting risk of leakage, then let's pursue something like your PR. But if there's still a case for the stronger isolation, then I think it's interesting to see if we can approach it from the \"other side\" -- making initialization from scratch as fast as possible.</p>\n</blockquote>\n<p>This is a good point; I imagine that there should be use cases on both sides of the spectrum. I guess maybe it'd be nice to survey other <code>wasmtime</code> users' and ask their opinion about this?</p>\n<p>I can only speak for ourselves, but in our main use case it's less of a security issue, and more of a \"we just don't want consecutive instantiations to interfere with each other by accident\". Our WASM module is - in general - trusted, and (simplifying a lot) we run a distributed system where the nodes essentially \"check\" each other's results, so leaking information locally within the same node is not a critical issue.</p>\n<p>Originally we didn't even clear the linear memory <em>at all</em> - we just manually reinitialized the globals and statics on every \"fake\" instantiation. But that ended up being problematic, since depending on the flags with which the WASM module was compiled the information about which statics need to be cleared on reinstantiation might not be emitted, so we decided to start just clearing the memory for simplicity as a quick fix, and started working on speeding it up, which resulted in my PR.</p>\n<p>As for the new features - in general we're pretty conservative here, and we disable every <code>wasm_*</code> feature flag by default until we can validate that there's a benefit to enabling them, and that everything still works as it should.</p>\n<blockquote>\n<p>And @koute I want to repeat again a \"thank you\" for spawning all of this thinking and exploration!</p>\n</blockquote>\n<p>Thank you for tackling this problem!</p>\n<blockquote>\n<p>It seems to help in my quick-and-dirty local testing; I'm curious what it will do to the benchmarks above but will save that benchmarking (and benchmarking in general with enough precision to have quotable numbers) for Monday :-)</p>\n</blockquote>\n<p>I updated my <code>wasmtime</code> benchmarking branch (I took your branch and merged the branch from my PR) and reran all the benchmarks and (unless I brainfarted somewhere and did something wrong) unfortunately it looks like the results are unchanged from the previous run for our benchmarks (within the margin of error):</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">call_empty_function</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">threads</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v2</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v1</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|---------|---------------------------|---------------------------|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">27</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">27</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">47</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">43</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">67</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">66</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">8</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">90</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">89</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"w\">      </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">125</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">127</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n\n<span class=\"n\">dirty_1mb_of_memory</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">threads</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v2</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v1</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|---------|---------------------------|---------------------------|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">239</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">238</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">380</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">385</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">456</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">453</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">8</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">613</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">618</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"w\">      </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">838</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">840</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n</code></pre></div>\n<p>(<code>instance_pooling_memfd_v2</code> is the new run, <code>instance_pooling_memfd_v1</code> are the numbers from the original run)</p>\n</blockquote>",
        "id": 269066466,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643008058
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020240918\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Thanks for writing all that up @cfallin, and at least my own personal opinion is to primarily err on the side of this PR in the sense of information leakage and spec-compliance. I very much want to get to the performance numbers in #3691 but I am not 100% convinced yet it's necessarily worth the loss in internal structuring (e.g. implementing an \"undo\" in addition to implementing a fast \"redo\".). Before making that conclusion though I think it's best to get this approach as close to #3691 as we possibly can.</p>\n<blockquote>\n<p>However, I think we can get away with initializing lazily</p>\n</blockquote>\n<p>Originally I didn't think this would work but digging more into your patch it looks like it's doing everything that would be necessary (although it is pretty gnarly). That being said I believe that the construction of an owned <code>InstanceAllocationInfo</code> is probably a large hit to instantiation-time costs, there's some large-ish arrays internally which we don't want to reallocate on all instantiations so to get a speedup from this that would probably need to be cached across instantiations somehow (and may explain why the measured numbers aren't all that different.</p>\n<hr>\n<p>I personally still think that the biggest win is going to be not creating a <code>VMCallerCheckedAnyfunc</code> for all functions in the module, only for the \"possibly exported\" ones. That set is at least an order of magnitude smaller than the set of all functions and will make whatever we do that much faster since there's less work to be done. I don't think that it will close the gap fully, though, and further investigation/tweaking may still be warranted.</p>\n<p>Another possible idea I might have is that the <code>VMCallerCheckedAnyfunc</code> representation today is:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"cp\">#[repr(C)]</span><span class=\"w\"></span>\n<span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"k\">struct</span> <span class=\"nc\">VMCallerCheckedAnyfunc</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"n\">func_ptr</span>: <span class=\"nc\">NonNull</span><span class=\"o\">&lt;</span><span class=\"n\">VMFunctionBody</span><span class=\"o\">&gt;</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"n\">type_index</span>: <span class=\"nc\">VMSharedSignatureIndex</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"n\">vmctx</span>: <span class=\"o\">*</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"n\">VMContext</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n<p>but we could probably change this to:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"cp\">#[repr(C)]</span><span class=\"w\"></span>\n<span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"k\">struct</span> <span class=\"nc\">VMCallerCheckedAnyfunc</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"n\">info</span>: <span class=\"o\">*</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"n\">VMCallerCheckedAnyfuncStaticInfo</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"n\">vmctx</span>: <span class=\"o\">*</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"n\">VMContext</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n\n<span class=\"cp\">#[repr(C)]</span><span class=\"w\"></span>\n<span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"k\">struct</span> <span class=\"nc\">VMCallerCheckedAnyfuncStaticInfo</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"n\">func_ptr</span>: <span class=\"nc\">NonNull</span><span class=\"o\">&lt;</span><span class=\"n\">VMFunctionBody</span><span class=\"o\">&gt;</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"w\">    </span><span class=\"k\">pub</span><span class=\"w\"> </span><span class=\"n\">type_index</span>: <span class=\"nc\">VMSharedSignatureIndex</span><span class=\"p\">,</span><span class=\"w\"></span>\n<span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n<p>Here <code>VMCallerCheckedAnyfunc</code> is per-instance but <code>VMCallerCheckedAnyfuncStaticInfo</code> can be calculated per-module. This means that at <code>Module</code> construction time we could build up an array of the \"static info\" and then instantiation would have less cross-correlation to do across a number of lists (I think we could entirely skip the <code>SharedSignatures</code> thing being passed down. I think that this will probably speed up that loop but I don't know by how much. This'll also add one more load or two to <code>call_indirect</code> and I don't know the performance implications of that. Theoretically though instantiating a module many times consumes less memory since <code>VMCallerCheckedAnyfunc</code> is a pointer smaller!</p>\n</blockquote>",
        "id": 269124701,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643039265
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020425496\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>I personally still think that the biggest win is going to be not creating a <code>VMCallerCheckedAnyfunc</code> for all functions in the module, only for the \"possibly exported\" ones. That set is at least an order of magnitude smaller than the set of all functions and will make whatever we do that much faster since there's less work to be done. I don't think that it will close the gap fully, though, and further investigation/tweaking may still be warranted.</p>\n</blockquote>\n<p>@alexcrichton I may be misunderstanding something about the internal workings but my intent with the lazy initialization was to actually subsume this; i.e. with lazy init we should <em>at least</em> only construct the anyfuncs that are possibly exported, and ideally even fewer than that. (So in other words I think we should already be getting that benefit with the patch.) Or are you imagining something different here?</p>\n<blockquote>\n<p>[factoring the anyfunc into two parts]</p>\n</blockquote>\n<p>I do like this, but it involves quite a lot more changes to generated code so I'd like to see if we can avoid doing it if we can. I think the most promising approach (what I'm poking at right now) is to share more of the init work wrt the <code>SharedSignatures</code>. I'm not sure but I suspect this may be why @koute 's benchmark still isn't showing improvement from lazy-init.</p>\n<p>(I've been testing by instantiating <code>spidermonkey.wasm</code>, so the benefit from skipping initialization of an anyfunc per function is probably significantly magnified compared to smaller modules; I'm planning to get the substrate benchmark working locally for me too, so I can steer based on the same numbers...)</p>\n</blockquote>",
        "id": 269151073,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643049959
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020447395\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@cfallin oh that's a good point, I was mostly thinking of other ways to get the speedup without laziness now that I'm thinking about it due to the complexity here. Otherwise though I think your approach is currently slower for unrelated reasons to laziness, the creation of <code>InstanceAllocationInfo</code> on each instantiation?</p>\n</blockquote>",
        "id": 269154930,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643051406
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020624190\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@koute I spent some time trying to reproduce numbers on your branch, and ran into a series of issues:</p>\n<ul>\n<li>A clone of the branch linked above refers to a no-longer-existing commit in your wasmtime fork (I guess because of a rebase?); I edited <code>client/executor/wasmtime/Cargo.toml</code> and <code>primitives/wasm-interface/Cargo.toml</code> to refer to my local clone of your wasmtime;</li>\n<li>When trying to build with your <code>cargo bench</code> commandline above, I hit a number of build errors in <code>client/executor/wasmtime/src/tests.rs</code> related to the <code>Semantics</code> struct (no <code>fast_instance_reuse</code> bool, instead an <code>instantiation_strategy</code> field);</li>\n<li>When I tried to hack that to work (using a \"reuse\" or \"recreate\" strategy), I ran into: <code>thread 'main' panicked at 'creating a full node doesn't fail: Client(VersionInvalid(\"RuntimeConstruction(Other(\\\"failed to instantiate a new WASM module instance: Incompatible allocation strategy\\\"))\"))', bin/node/cli/benches/block_production.rs:114:57</code></li>\n</ul>\n<p>I'm benchmarking locally with <code>benches/server.rs</code> in this PR, modified to load some other Wasm modules; so far a big one (spidermonkey.wasm) but I'll test with some others as well. I'm planning to improve the lazy-anyfunc-init and will hopefully have another round of changes today at least verified locally :-)</p>\n</blockquote>",
        "id": 269183299,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643064197
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020674800\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>I've modified the scheme in this PR to not recompute signature info on every instantiation; now it's showing a decent speedup on SpiderMonkey instantiation, from ~1.69us to ~1.48us on my machine (raw instance creation only, no start function invocation). WIthout the signature fix I was seeing ~1.6us IIRC. I'm redoing some profiling now to see what else might be done...</p>\n</blockquote>",
        "id": 269190710,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643069200
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020711683\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@koute I've now improved perf a bit more -- in my local tests (raw instantiate, no start function, <code>spidermonkey.wasm</code>) I went from 1.68us earlier today to 773ns (!):</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">strategy</span><span class=\"w\"> </span><span class=\"n\">pooling</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">occupancy</span><span class=\"w\"> </span><span class=\"mi\">1000</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">benches</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s\">\"spidermonkey.wasm\"</span><span class=\"p\">]</span><span class=\"w\"></span>\n<span class=\"w\">                        </span><span class=\"n\">time</span>:   <span class=\"p\">[</span><span class=\"mf\">679.58</span><span class=\"w\"> </span><span class=\"n\">ns</span><span class=\"w\"> </span><span class=\"mf\">773.45</span><span class=\"w\"> </span><span class=\"n\">ns</span><span class=\"w\"> </span><span class=\"mf\">876.66</span><span class=\"w\"> </span><span class=\"n\">ns</span><span class=\"p\">]</span><span class=\"w\"></span>\n<span class=\"w\">                        </span><span class=\"n\">change</span>: <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">58.530</span><span class=\"o\">%</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"mf\">52.164</span><span class=\"o\">%</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"mf\">44.172</span><span class=\"o\">%</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mf\">0.00</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"mf\">0.05</span><span class=\"p\">)</span><span class=\"w\"></span>\n</code></pre></div>\n<p>I'm curious if the factor of ~2 will translate into your benchmark as well, and/or what constant factors are left. (I'd still like to somehow be able to get your benchmark to run locally too...)</p>\n<p>The trick in my latest changes, beyond lazy init of anyfuncs, was to represent non-init with a zeroed bitmap in the vmctx, rather than zeroes in the sparse array. Zeroing the latter was slow; per-field writes, even just a memset, array is too sparse. After the change it doesn't seem to show up in my profiles any more; the actual table-element init (building anyfuncs that are actually referenced) is the bulk of the instance init time.</p>\n</blockquote>",
        "id": 269195799,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643073460
    },
    {
        "content": "<p>koute <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1021119493\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@cfallin Sorry about the non-working branch! I've updated the code, but I haven't updated the <code>Cargo.lock</code> so it was referring to the old commit. (Updating the <code>Cargo.lock</code> would have fixed it.)</p>\n<p>I've pulled in your most recent changes and updated the branches; here are the numbers:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">call_empty_function</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">threads</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">native_instance_reuse</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v3</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v1</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|---------|-----------------------|---------------------------|---------------------------|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\">                     </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">28</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">27</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">11</span><span class=\"w\">                    </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">49</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">43</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">17</span><span class=\"w\">                    </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">71</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">66</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">8</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">24</span><span class=\"w\">                    </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">98</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">89</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"w\">      </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">35</span><span class=\"w\">                    </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">127</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">127</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n\n<span class=\"n\">dirty_1mb_of_memory</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">threads</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">native_instance_reuse</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v3</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v1</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|---------|-----------------------|---------------------------|---------------------------|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">147</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">242</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">238</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">214</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">381</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">385</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">309</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">453</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">453</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">8</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">580</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">629</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">618</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"w\">      </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">732</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">831</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">840</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n</code></pre></div>\n<p>Looks like they're mostly... unchanged again?</p>\n</blockquote>",
        "id": 269246777,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643112404
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1021515810\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@koute -- thanks! @alexcrichton and I tracked down what we think is the delta in runtime perf; it led back to a comment you left at your use of <code>Mmap::populated_range</code> regarding performance of touching anonymous-zero memory vs CoW-mapped memory. Obvious in hindsight (zeroed, or even better pre-zeroed, is faster than copied) but I had missed it. I <em>think</em> this PR should be on par with yours wrt runtime perf now; instantiation still TBD, likely some gap remaining, but the lazy anyfunc init or some other scheme (knowing which functions are reference-taken and having a separate index space for those, for example) is the key for large modules I think.</p>\n</blockquote>",
        "id": 269307259,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643137604
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1021728997\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@koute: I'm unfortunately still not able to run your benchmarks: I'm reaching the same \"Incompatible allocation strategy\" error I quoted above. I'm on the latest commit (<code>25917541d78</code>), with Cargo.tomls modified to refer to a local checkout of your Wasmtime branch, commit <code>ec95b9365e3</code>. I had to comment out tests in client/executor/wasmtime/src/tests.rs to get the build to complete.</p>\n<p>In any case, I suspect that this PR is much closer now -- I'll wait for your confirmation, or not, on this to be sure; if we know we can get close with the \"from-scratch\" approach, or otherwise if we can quantify what the remaining delta is, then this should help us decide what to do next.</p>\n</blockquote>",
        "id": 269345358,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643156015
    },
    {
        "content": "<p>koute edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1017164295\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>Can you share the branch and/or code to reproduce the results with this PR?</p>\n</blockquote>\n<p>Sure. Here's the branch:</p>\n<p><a href=\"https://github.com/koute/substrate/tree/master_wasmtime_benchmarks_with_cfallin_memfd_cow\">https://github.com/koute/substrate/tree/master_wasmtime_benchmarks_with_cfallin_memfd_cow</a></p>\n<p>And here's how to run them to get numbers for this PR (essentially the same as described in my PR, just with an extra cargo feature):</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">cd</span><span class=\"w\"> </span><span class=\"n\">substrate</span><span class=\"o\">/</span><span class=\"n\">client</span><span class=\"o\">/</span><span class=\"n\">executor</span><span class=\"o\">/</span><span class=\"n\">benches</span><span class=\"w\"></span>\n<span class=\"n\">FORCE_WASMTIME_INSTANCE_POOLING</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"n\">rustup</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">nightly</span><span class=\"o\">-</span><span class=\"mi\">2021</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">01</span><span class=\"o\">-</span><span class=\"n\">x86_64</span><span class=\"o\">-</span><span class=\"n\">unknown</span><span class=\"o\">-</span><span class=\"n\">linux</span><span class=\"o\">-</span><span class=\"n\">gnu</span><span class=\"w\"> </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">bench</span><span class=\"w\"> </span><span class=\"o\">--</span><span class=\"n\">features</span><span class=\"w\"> </span><span class=\"n\">wasmtime</span><span class=\"p\">,</span><span class=\"n\">sc</span><span class=\"o\">-</span><span class=\"n\">executor</span><span class=\"o\">-</span><span class=\"n\">wasmtime</span><span class=\"o\">/</span><span class=\"n\">memfd</span><span class=\"o\">-</span><span class=\"n\">allocator</span><span class=\"w\"> </span><span class=\"n\">call_empty_function_from_kusama_runtime_with_recreate_instance_on_1_threads</span><span class=\"w\"></span>\n</code></pre></div>\n<p>If you have any further questions feel free to ask! (I can also make myself available for discussion on Zulip.)</p>\n<blockquote>\n<p>Some quick benchmarking shows that <a href=\"https://github.com/bytecodealliance/wasmtime/blob/2649d2352c415979ea0fe7f43eacb2f3f65d771c/crates/runtime/src/instance/allocator.rs#L463\">this function</a> is quite hot in this PR. That makes sense to me because @koute your PR skips that function entirely with the reuse mechanism you've implemented.</p>\n</blockquote>\n<p>Indeed! One of the reasons why I implemented my PR the way I did is because you have to punch through less layers of abstraction so it's easier to make it faster. (:</p>\n</blockquote>",
        "id": 269362231,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643171392
    },
    {
        "content": "<p>koute <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1021874083\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>I'm unfortunately still not able to run your benchmarks: I'm reaching the same \"Incompatible allocation strategy\" error I quoted above.</p>\n</blockquote>\n<p>@cfallin: This might be a silly question, but... are you using the right branch and calling the benchmarks exactly as described? (:</p>\n<p>There's the branch from my original PR (which is unmodified, and won't work), and there's the branch for this PR (link is here: <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1017164295\">https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1017164295</a>). You also have to <code>cd</code> into the right directory first. (I don't remember whether the whole codebase compiled on that branch, but it might not, since I was only interested in our WASM executor and the benchmarks.)</p>\n<p>Anyway, I reran the benchmarks again, and here are the results:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">call_empty_function</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">threads</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">native_instance_reuse</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v4</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v1</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|---------|-----------------------|---------------------------|---------------------------|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"w\">                     </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">24</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">27</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">11</span><span class=\"w\">                    </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">46</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">43</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">17</span><span class=\"w\">                    </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">69</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">66</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">8</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">24</span><span class=\"w\">                    </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">97</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">89</span><span class=\"w\">                        </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"w\">      </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">35</span><span class=\"w\">                    </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">128</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">127</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n\n<span class=\"n\">dirty_1mb_of_memory</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">threads</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">native_instance_reuse</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v4</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">instance_pooling_memfd_v1</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|---------|-----------------------|---------------------------|---------------------------|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">144</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">170</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">238</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">217</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">241</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">385</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">307</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">338</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">453</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">8</span><span class=\"w\">       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">589</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">605</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">618</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n<span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">16</span><span class=\"w\">      </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">764</span><span class=\"w\">                   </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">815</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"mi\">840</span><span class=\"w\">                       </span><span class=\"o\">|</span><span class=\"w\"></span>\n</code></pre></div>\n<p>There's an improvement!</p>\n</blockquote>",
        "id": 269364666,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643173885
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1021926688\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>You also have to <code>cd</code> into the right directory first.</p>\n</blockquote>\n<p>Oh, I definitely missed that bit; I was running <code>cargo bench</code> from the root of your tree, and indeed hitting build failures. I confirm I can run some of your benchmarks (the \"legacy reuse\" ones specifically) locally now.</p>\n<p>However, on a clean checkout of <code>3d80c9593624dc9f28ab877bae6e7d86459523db</code> (your latest commit), with no local changes, I get the following error for a \"native reuse\" benchmark:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"cp\">$</span><span class=\"w\"> </span><span class=\"n\">cd</span><span class=\"w\"> </span><span class=\"n\">client</span><span class=\"o\">/</span><span class=\"n\">executor</span><span class=\"o\">/</span><span class=\"n\">benches</span><span class=\"o\">/</span><span class=\"w\"></span>\n<span class=\"cp\">$</span><span class=\"w\"> </span><span class=\"n\">FORCE_WASMTIME_INSTANCE_POOLING</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"n\">rustup</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">nightly</span><span class=\"o\">-</span><span class=\"mi\">2021</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">01</span><span class=\"o\">-</span><span class=\"n\">x86_64</span><span class=\"o\">-</span><span class=\"n\">unknown</span><span class=\"o\">-</span><span class=\"n\">linux</span><span class=\"o\">-</span><span class=\"n\">gnu</span><span class=\"w\"> </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">bench</span><span class=\"w\"> </span><span class=\"o\">--</span><span class=\"n\">features</span><span class=\"w\"> </span><span class=\"n\">wasmtime</span><span class=\"p\">,</span><span class=\"n\">sc</span><span class=\"o\">-</span><span class=\"n\">executor</span><span class=\"o\">-</span><span class=\"n\">wasmtime</span><span class=\"o\">/</span><span class=\"n\">memfd</span><span class=\"o\">-</span><span class=\"n\">allocator</span><span class=\"w\"> </span><span class=\"n\">call_empty_function_from_test_runtime_with_native_instance_reuse_on_1_threads</span><span class=\"w\"></span>\n\n<span class=\"p\">[</span><span class=\"w\"> </span><span class=\"o\">..</span><span class=\"p\">.</span><span class=\"w\"> </span><span class=\"p\">]</span><span class=\"w\"></span>\n<span class=\"n\">Benchmarking</span><span class=\"w\"> </span><span class=\"n\">call_empty_function_from_test_runtime_with_native_instance_reuse_on_1_threads</span>: <span class=\"nc\">Warming</span><span class=\"w\"> </span><span class=\"n\">up</span><span class=\"w\"> </span><span class=\"k\">for</span><span class=\"w\"> </span><span class=\"mf\">3.0000</span><span class=\"w\"> </span><span class=\"n\">sthread</span><span class=\"w\"> </span><span class=\"o\">'</span><span class=\"na\">main</span><span class=\"o\">'</span><span class=\"w\"> </span><span class=\"n\">panicked</span><span class=\"w\"> </span><span class=\"n\">at</span><span class=\"w\"> </span><span class=\"o\">'</span><span class=\"na\">called</span><span class=\"w\"> </span><span class=\"err\">`</span><span class=\"nb\">Result</span>::<span class=\"n\">unwrap</span><span class=\"p\">()</span><span class=\"err\">`</span><span class=\"w\"> </span><span class=\"n\">on</span><span class=\"w\"> </span><span class=\"n\">an</span><span class=\"w\"> </span><span class=\"err\">`</span><span class=\"nb\">Err</span><span class=\"err\">`</span><span class=\"w\"> </span><span class=\"n\">value</span>: <span class=\"nc\">RuntimeConstruction</span><span class=\"p\">(</span><span class=\"n\">Other</span><span class=\"p\">(</span><span class=\"s\">\"failed to instantiate a new WASM module instance: Incompatible allocation strategy\"</span><span class=\"p\">))</span><span class=\"o\">'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">client</span><span class=\"o\">/</span><span class=\"n\">executor</span><span class=\"o\">/</span><span class=\"n\">benches</span><span class=\"o\">/</span><span class=\"n\">bench</span><span class=\"p\">.</span><span class=\"n\">rs</span>:<span class=\"mi\">171</span>:<span class=\"mi\">67</span><span class=\"w\"></span>\n<span class=\"n\">note</span>: <span class=\"nc\">run</span><span class=\"w\"> </span><span class=\"n\">with</span><span class=\"w\"> </span><span class=\"err\">`</span><span class=\"n\">RUST_BACKTRACE</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"err\">`</span><span class=\"w\"> </span><span class=\"n\">environment</span><span class=\"w\"> </span><span class=\"n\">variable</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">display</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"n\">backtrace</span><span class=\"w\"></span>\n</code></pre></div>\n<p>If I should be running a different command, or from a different commit base, please do let me know!</p>\n<blockquote>\n<p>[new numbers]</p>\n</blockquote>\n<p>It's good to see confirmation here, thanks!</p>\n<p>So I think at this point we can conclude something like: when the instance runs for long enough, the performance of this PR converges to that of your PR, because the underlying memory mapping is the same (anonymous mmap for zeroes, CoW of memfd otherwise). There is still an instantiation-time penalty, and this still mostly has to do with anyfunc initialization and initialization of the table elements that refer to those anyfunc structs.</p>\n<p>One thing that @alexcrichton and I noticed when looking at your benchmark in particular today is that it does not appear to contain any <code>memory.grow</code> instruction at all; so the memory growth path is not stressed here. That is a hot path for other use-cases, and the instance-reuse PR needs to call <code>mprotect</code>, which <a href=\"https://github.com/torvalds/linux/blob/0280e3c58f92b2fe0e8fbbdf8d386449168de4a8/mm/mprotect.c#L551-L552\">takes the process-wide <code>mmap_lock</code></a>; this is something we want to avoid in highly-concurrent-server contexts. This PR uses a per-slot memfd with <code>ftruncate</code> to avoid that.</p>\n<p>Anyway, at this point I think it might be fair to say that we've closed part of the gap (between stock wasmtime and your instance-reuse PR), know what the remaining gap can be attributed to, and this PR's approach might be slightly easier to maintain (as per @alexcrichton above); so it comes down to a choice of squeezing every last microsecond with a \"trusted module\" (your case) vs taking a still-significant improvement over today's mainline with existing security boundaries (this PR). That's a choice that depends on relative priorities; do others want to weigh in more here?</p>\n<p>One last thing I might say is that it does seem possible to build a snapshot/rewind-like approach on top of a memfd-as-part-of-normal-instantiation foundation (this PR); basically we would provide an extra hook that does the <code>madvise()</code>, and a <code>Snapshot</code> object attached to an instance that saves and restores globals/table entries, and splats them back en-masse. This layering -- putting the memfd and mmap magic in the traditional instantiation machinery -- lets us use it for a \"flash-reset\" too, while the other way around, building all of this as a side-mechanism, precludes us from getting any benefit in a \"transparent behind existing API\" scenario. So this might be a way to satisfy both use-cases with one (or 1.2-ish) implementation(s). Thoughts?</p>\n</blockquote>",
        "id": 269371057,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643181118
    },
    {
        "content": "<p>koute <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1021984991\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>However, on a clean checkout of <code>3d80c9593624dc9f28ab877bae6e7d86459523db</code> (your latest commit), with no local changes, I get the following error for a \"native reuse\" benchmark:</p>\n<p>```<br>\n$ cd client/executor/benches/<br>\n$ FORCE_WASMTIME_INSTANCE_POOLING=1 rustup run nightly-2021-11-01-x86_64-unknown-linux-gnu cargo bench --features wasmtime,sc-executor-wasmtime/memfd-allocator call_empty_function_from_test_runtime_with_native_instance_reuse_on_1_threads</p>\n<p>[ ... ]<br>\nBenchmarking call_empty_function_from_test_runtime_with_native_instance_reuse_on_1_threads: Warming up for 3.0000 sthread 'main' panicked at 'called <code>Result::unwrap()</code> on an <code>Err</code> value: RuntimeConstruction(Other(\"failed to instantiate a new WASM module instance: Incompatible allocation strategy\"))', client/executor/benches/bench.rs:171:67<br>\nnote: run with <code>RUST_BACKTRACE=1</code> environment variable to display a backtrace<br>\n```</p>\n<p>If I should be running a different command, or from a different commit base, please do let me know!</p>\n</blockquote>\n<p>Sorry, the way I quickly hacked different instantiation strategies into the benchmarks is little janky, so you can't run all of them with a single command. So for each instantiation strategy you basically have to run them separately, e.g. to get the numbers for your PR:</p>\n<div class=\"codehilite\"><pre><span></span><code>$ FORCE_WASMTIME_INSTANCE_POOLING=1 rustup run nightly-2021-11-01-x86_64-unknown-linux-gnu cargo bench --features wasmtime,sc-executor-wasmtime/memfd-allocator with_recreate_instance\n</code></pre></div>\n\n<p>To get the numbers for just recreating each instance from scratch with the ondemand strategy without any pooling:</p>\n<div class=\"codehilite\"><pre><span></span><code>$ rustup run nightly-2021-11-01-x86_64-unknown-linux-gnu cargo bench --features wasmtime with_recreate_instance\n</code></pre></div>\n\n<p>And to get the numbers for my PR:</p>\n<div class=\"codehilite\"><pre><span></span><code>$ rustup run nightly-2021-11-01-x86_64-unknown-linux-gnu cargo bench --features wasmtime native_instance_reuse\n</code></pre></div>\n\n<blockquote>\n<p>So I think at this point we can conclude something like: when the instance runs for long enough, the performance of this PR converges to that of your PR, because the underlying memory mapping is the same (anonymous mmap for zeroes, CoW of memfd otherwise).</p>\n</blockquote>\n<p>Well, yes, but the whole point here is to reduce the overhead of starting fresh, relatively short-lived instances, right? (: Even just using the ondemand strategy converges to the same performance if it runs long enough.</p>\n<blockquote>\n<p>One thing that @alexcrichton and I noticed when looking at your benchmark in particular today is that it does not appear to contain any <code>memory.grow</code> instruction at all; so the memory growth path is not stressed here. That is a hot path for other use-cases, and the instance-reuse PR needs to call <code>mprotect</code>, which <a href=\"https://github.com/torvalds/linux/blob/0280e3c58f92b2fe0e8fbbdf8d386449168de4a8/mm/mprotect.c#L551-L552\">takes the process-wide <code>mmap_lock</code></a>; this is something we want to avoid in highly-concurrent-server contexts. This PR uses a per-slot memfd with <code>ftruncate</code> to avoid that.</p>\n</blockquote>\n<p>Indeed. Do you want me to add some extra benchmarks for that?</p>\n<p>Hmm... also, couldn't the <code>ftruncate</code> trick be also used with my approach? (I can try adding it in, but I'm not sure if there's a point <em>if</em> the chances of my PR being accepted are low anyway?)</p>\n<blockquote>\n<p>lets us use it for a \"flash-reset\" too, while the other way around, building all of this as a side-mechanism, precludes us from getting any benefit in a \"transparent behind existing API\" scenario</p>\n</blockquote>\n<p>Since we're talking about the API, I just want to chime in here that from our perspective as <code>wasmtime</code> users (this might or might not be true for other people with different use cases) the \"side-mechanism\" is actually more of a \"transparent behind existing API\" approach. (:</p>\n<p>Consider the scenario where someone is currently using the ondemand instance allocator and wants to switch to a faster approach. With the approach from my PR it's basically this (I'm obviously simplifying as this is not the actual patch from our code, but that's essentially what it took):</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"o\">@@</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">13</span><span class=\"w\"> </span><span class=\"o\">@@</span><span class=\"w\"></span>\n<span class=\"o\">+</span><span class=\"k\">if</span><span class=\"w\"> </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"nb\">Some</span><span class=\"p\">((</span><span class=\"n\">instance</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">store</span><span class=\"p\">))</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">instance_cache</span><span class=\"p\">.</span><span class=\"n\">pop</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n\n<span class=\"o\">+</span><span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"nb\">Ok</span><span class=\"p\">((</span><span class=\"n\">instance</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">store</span><span class=\"p\">));</span><span class=\"w\"></span>\n<span class=\"o\">+</span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"k\">else</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n<span class=\"w\">     </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">store</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">initialize_store</span><span class=\"p\">()</span><span class=\"o\">?</span><span class=\"p\">;</span><span class=\"w\"></span>\n\n<span class=\"o\">-</span><span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">instance</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">instance_pre</span><span class=\"p\">.</span><span class=\"n\">instantiate</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"n\">store</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"p\">;</span><span class=\"w\"></span>\n<span class=\"o\">+</span><span class=\"w\">    </span><span class=\"kd\">let</span><span class=\"w\"> </span><span class=\"n\">instance</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">instance_pre</span><span class=\"p\">.</span><span class=\"n\">instantiate_reusable</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"k\">mut</span><span class=\"w\"> </span><span class=\"n\">store</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"w\"></span>\n<span class=\"w\">     </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"nb\">Ok</span><span class=\"p\">((</span><span class=\"n\">instance</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">store</span><span class=\"p\">));</span><span class=\"w\"></span>\n<span class=\"o\">+</span><span class=\"p\">}</span><span class=\"w\"></span>\n<span class=\"o\">+</span><span class=\"w\"></span>\n<span class=\"o\">+</span><span class=\"c1\">// ...and when we're done with the instance...</span>\n<span class=\"o\">+</span><span class=\"k\">if</span><span class=\"w\"> </span><span class=\"n\">instance_cache</span><span class=\"p\">.</span><span class=\"n\">len</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">MAXIMUM_CACHED_INSTANCES</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span>\n\n<span class=\"o\">+</span><span class=\"w\">    </span><span class=\"n\">instance</span><span class=\"p\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span><span class=\"o\">?</span><span class=\"p\">;</span><span class=\"w\"></span>\n<span class=\"o\">+</span><span class=\"w\">    </span><span class=\"n\">instance_cache</span><span class=\"p\">.</span><span class=\"n\">push</span><span class=\"p\">((</span><span class=\"n\">instance</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">store</span><span class=\"p\">));</span><span class=\"w\"></span>\n<span class=\"o\">+</span><span class=\"p\">}</span><span class=\"w\"></span>\n</code></pre></div>\n<p>It's fast, gives the user control and is super simple. There's no need to define any limits on the WASM module itself (so the WASM executor won't suddenly get bricked if the module has more functions/globals/larger memory/etc. than expected), it will still work if we instantiate too many modules at the same time (the extra modules just won't be reused), and it allows the user to tweak how many instances are being cached without reinitializing everything.</p>\n<p>(And if we wanted to make it even more transparent the reuse could maybe be integrated into <code>InstancePre</code>, which would implicitly create reusable instances with its normal <code>instantiate</code> and cache them once they're dropped, without the user having to do anything besides setting a single flag. That would obviously require some more API changes in general since e.g. <code>Instance</code> is currently <code>Copy</code>, among other things. I didn't do this because it would have been even more controversial.)</p>\n<p>Now consider switching to the pooling allocator. Yes, turning it on is essentially just a single line of code calling <code>Config::allocation_strategy</code>, but that's not all it takes! Now suddenly you have limitations which you didn't have before. Your WASM module can't exceed certain limit, and you can only have a certain number of instances active at the same time. It's not transparent. Now you need to write some code to have a fallback, and since the point where the strategy is configured is when creating the <code>Engine</code> and <em>not</em> when instantiating the module it's not very convenient.</p>\n<p>So I totally understand that my approach might be harder to maintain, <em>however</em> purely as a user I think the pooling approach (completely ignoring how it performs) is not very convenient to use. (Unless it would be truly a \"set and forget\" without having to explicitly set any hard limits.)</p>\n</blockquote>",
        "id": 269378035,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643186822
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1022380343\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>My personal take an opinion is that we should follow a sequence of steps that looks like:</p>\n<ul>\n<li>Focus on the strategy outlined in this PR, accelerating instantiation rather than implementing snapshots.</li>\n<li>Update this PR to be simply memfd-backed-CoW for the pooling allocator</li>\n<li>Enumerate remaining optimizations around instantiation which need to be improved. I think an important point to note here is that we literally haven't been able to measure these other bits of instantiation because the memory initialization was so expensive. That basically means that I think there's a lot of low-hanging fruit to optimize here:<ul>\n<li>Optimize the <code>VMCallerCheckedAnyfunc</code> array by making it smaller, lazily initialized, faster to initialize, or some combination thereof.</li>\n<li>Remove the need to grab an rwlock for inserting a host function into a store.</li>\n<li>Remove the need to clone function types when inserting a host function into a store.</li>\n<li>Optimize tracking of trampolines in the store to probably not use <code>HashMap</code> vanilla (either a different data structure or a more optimized hash algorithm)</li>\n</ul>\n</li>\n<li>Add a new feature to Wasmtime which is enabling the memory pooling allocator with the on-demand instance allocator. Perhaps with some default settings about how many memories to pool or similar. (or something like this).</li>\n<li>Work towards enabling memfd by default on Linux where we can with the on-demand instance allocator.</li>\n</ul>\n<p>I believe we can solve all the problems here using a hybrid of this PR and @koute's, I don't think that this is an either/or choice. In isolation I don't think that anyone would disagree that start-from-scratch instantiation is more robust and easier to reason about. The main reason to implement some sort of snapshot-like approach would be performance, but I'm pretty sure we have a good handle on performance from poking around at the examples from @koute your substrate fork. This so far has revealed:</p>\n<ul>\n<li>As mentioned above the initialize-an-instance path hasn't really been optimized to this degree because we couldn't measure things. There's lots of low-hanging fruit to optimize.</li>\n<li>@cfallin's original PR and implementation showed that CoW-ing a page of zeros from <code>memfd</code> is much slower than from an anonymous mapping. This led to tweaking the design to, as your PR does @koute, minimizing the size of the <code>memfd</code> image as well as using an anonymous <code>mmap</code> for the initial heap contents that are all zero.</li>\n<li>Locally I'm seeing that for the threaded benchmarks you have @koute the main performance bottleneck seems to be the usage of <code>std::sync::RwLock&lt;T&gt;</code> around the signatures in an engine and hitting that rwlock ~100 times during instantiation. (this is also one of those easy \"low hanging fruit\" to optimize)</li>\n</ul>\n<p>I'm pretty confident that we have basically quantified the difference in numbers that you're seeing @koute between <code>native_instance_reuse</code> and <code>instance_pooling_memfd_v*</code>. I don't mean to pretend that they're the same and we don't have to worry about the difference, my point is that I feel like we have a good handle on why there's a difference and a number of avenues ahead of us to make the difference even smaller. It may be the case that this PR's strategy of always-initialize-the-instace may not microsecond-for-microsecond come out on par with \"reuse a snapshot\", but I believe that there's still legwork to be done to definitively say \"this is the difference\". Until such a conclusion is reached I don't personally think that it's worthwhile to implement a snapshot-like-scheme in Wasmtime.</p>\n<hr>\n<p>I'd like to also ideally address some of our API-wise concerns @koute. You've mentioned that for your use case it's important that instantiation always succeeds and you don't want to configure the pooling allocator so far. I think much of this can be solved by having a default pooling allocator for memories within engines like I mentioned above. We can pretty easily remove the need to have limits on the memory pool as well. I think it's worthwhile to acknowledge, though, that the desire to run an unlimited number of instances concurrently isn't a constraint we've seen yet and is so far unique to you.</p>\n<p>With a memory pool that enables the CoW/memfd strategy implemented in this PR to be usable with the on-demand allocation strategy. I think that this would solve the issues around trying to configure the pooling allocator since you wouldn't be using it and you could allocate as many stores/instances as the system can support. The limits related to memory could also be relaxed in this situation so they don't need to be specified up-front.</p>\n<p>The final thing you've mentioned is that it's easy to build pooling allocators externally, so why not do that? The problem with this is that it places a lot of constraints on the API design of the <code>wasmtime</code> crate itself. We originally wanted to do precisely that with the current pooling allocator, allowing an external crate to implement it rather than baking it into <code>wasmtime</code> itself. The problem is that the API hooks necessary to implement this were far too complicated and invasive. It effectively would have been a massive amount of effort (too much) to design and support such APIs. Similarly with your approach you're enabling a pooling-style of allocation but it comes at the cost of <code>wasmtime</code> having to maintain a snapshot-like API. This snapshot-like API probably won't always be exclusively used for pooling which means that it has to be maintained and grow over time for a variety of use-cases. Overall at least my personal conclusion is that it's better for now for us to implement common practices in Wasmtime to give us the maximum flexibility in the API and support over time to ensure we don't lock ourselves into anything.</p>\n<hr>\n<p>Well that's a bit of a long-winded post, sorry for the wall of text. I think that captures well my thoughts on all this, though, and I'd like to hear what others hink about this all too!</p>\n</blockquote>",
        "id": 269441662,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643215220
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1023663014\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@alexcrichton I've rebased out the lazy-anyfunc stuff into #3733, and done some preliminary cleanup here (e.g. the <code>Mmap</code> changes weren't needed). I think this is ready for review now if you'd like to take a look -- thanks!</p>\n<p>I'm working on a slot-affinity <code>InstanceAllocationStrategy</code> now, and can either add that to this PR when it's ready, or do it as a separate PR if that's preferable.</p>\n</blockquote>",
        "id": 269642321,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643319584
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1023823741\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@alexcrichton thanks a bunch for all of the review comments -- this definitely polished off a few rough edges!</p>\n<p>The main outstanding question right now seems to be how to handle config and conditional compilation. I think it might make sense to do a followup PR that cleans up the pooling allocator in general, including for uffd code, to have a dynamic \"mode\" setting, and operate based on that. But since it probably makes sense to do that for uffd along with memfd, I think that it's a little out of scope for this PR. Would you mind if I saved this for a followup?</p>\n</blockquote>",
        "id": 269677863,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643336528
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1024298592\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>The main outstanding question right now seems to be how to handle config and conditional compilation</p>\n</blockquote>\n<p>I agree punting this to a future PR makes the mose sense and following uffd's pattern here is the way to go. I'll take a look at #3737 after this, thanks for that!</p>\n</blockquote>",
        "id": 269747630,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643381661
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1024749608\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Thanks @alexcrichton ! I've addressed all of your comments I think. Lots of refactoring but things feel much better now. Hopefully that's mostly it :-)</p>\n<p>The remaining questions are mostly around how this looks next to uffd, I think. While I definitely understand the desire to make things more unified, I worry a little that the scope of this PR keeps expanding to \"clean up uffd while we're at it\", and I feel like that could be an endless treadmill, especially if we're hoping to eventually remove uffd if memfd supersedes it. If you feel very strongly about it of course, I'm happy to keep going with whatever suggestions you have.</p>\n</blockquote>",
        "id": 269819721,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643413771
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1026137717\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>@alexcrichton Thanks for the continued review feedback -- I think I've addressed everything! (Modulo one of your uffd comments -- a \"simple\" cleanup turned into a huge rabbithole and so I gingerly climbed out. If we really, really need uffd and really, really need the cleanup then I can invest the significant effort to refactor how uffd initialization works.)</p>\n<p>I actually agree that it may make sense to start without the ftruncate trick. Synthetic benchmarks have shown the mmap lock to be a bottleneck, e.g. some of my testing comparing worst-case (never reuse the same image, always re-mmap) to best-case (just madvise) shows a significant delta, and the perf profile shows a lot of time spent inside mmap, as one would expect. The <code>mprotect</code> necessary to grow the heap without the ftruncate trip hits the same lock. But what I <em>haven't</em> been able to reproduce is a real-world use-case where, including all the overhead of running code and everything else, the heap growth / mmap lock becomes a significant bottleneck.</p>\n<p>So I've removed that, in favor of just anon-mmap zeroed memory. I did so as a separate commit so if we want to bring this back some day, it's in the history; and we can certainly look at benchmark results if/when the memfd technique sees more real-world use.</p>\n</blockquote>",
        "id": 270091803,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643657839
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1026227616\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Just as an addendum here: @alexcrichton and I discussed the possibility of using the implementation bits here (<code>MemFdSlot</code>) in the on-demand allocator as well. This would address any use-case that requires that allocator (@koute , we're still very interested in getting something into the codebase that serves your needs!).</p>\n<p>I took a bit of a deeper look now and I think it's probably best as a followup PR -- it involves some more refactoring in the alloactor machinery (e.g. the <code>RuntimeMemoryCreator</code> and <code>RuntimeLinearMemory</code> traits). But I'm happy to do this work!</p>\n</blockquote>",
        "id": 270107693,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643664404
    },
    {
        "content": "<p>cfallin edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1026227616\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Just as an addendum here: @alexcrichton and I discussed the possibility of using the implementation bits here (<code>MemFdSlot</code>) in the on-demand allocator as well. This would address any use-case that requires that allocator (@koute , we're still very interested in getting something into the codebase that serves your needs!).</p>\n<p>I took a bit of a deeper look now and I think it's probably best as a followup PR -- it involves some more refactoring in the allocator machinery (e.g. the <code>RuntimeMemoryCreator</code> and <code>RuntimeLinearMemory</code> traits). But I'm happy to do this work!</p>\n</blockquote>",
        "id": 270107746,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643664429
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1026254538\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Actually, it turned out to be pretty simple to extend this to the on-demand allocator too; pushed another commit for this, but happy to split it to a second PR if that makes review easier @alexcrichton .</p>\n</blockquote>",
        "id": 270112236,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643666446
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1026308696\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Sorry I'm heading out for the day and only recently noticed the addition to the on-demand allocator, I skimmed it and it looks good to me, but I'll need to dig in a bit more depth tomorrow, otherwise I've got one instance of what I think is a bug but otherwise just a bunch of nits which can all be deferred to later.</p>\n</blockquote>",
        "id": 270123523,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643671177
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1027404829\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Thanks @alexcrichton !</p>\n<p>I did a few last refactors based on your comments; given the criticality of this code I want to make sure you're happy with the last commit before merging!</p>\n<p>I played around a bit with simplifying <code>MemFdSlot</code> to carry less state, but in the end I think as it manages reuse, and has to know what the current mapping state is to reuse mappings, there is not much complexity reduction in externalizing that state and requiring it to be passed back in. It felt a little more brittle to me. Perhaps we could go the other way and have an abstraction for \"something that manages an address range\", build another one that doesn't use memfds, and make this \"slot manager\" the one source of truth. We can probably play with this in followup PRs -- what do you think?</p>\n</blockquote>",
        "id": 270309725,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643759847
    },
    {
        "content": "<p>koute <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1027582514\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<blockquote>\n<p>@koute we haven't heard from you in a bit, but to reiterate this PR brings all the CoW benefits to the on-demand allocator as well although the reuse case isn't simply an <code>madvise</code> to reset. That being said for your \"empty\" function benchmark from before I'm clocking this PR (re-instantiation in a loop) at around 40us and \"'just <code>madvise</code>\", your PR, at around 5ns. A huge portion of the remaining time is table initialization, scheduled to be addressed in #3733 after some further work (which should make table initialization effectively zero-cost). Basically I want to reiterate we continue to be very interested in solving your use case and accomodating your performance constraints. If you'd like we'd be happy to ping you when other optimization work has settled down and we're ready to re-benchmark.</p>\n</blockquote>\n<p>Yes, I saw you're pretty busy working on this so that's why I was keeping quiet until you finish. (:</p>\n<p>Please do ping me once this is ready to rebenchmark!</p>\n</blockquote>",
        "id": 270334950,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643778314
    },
    {
        "content": "<p>acfoltzer <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1028187887\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>(tagging myself as a reviewer on this just so I don't lose track of it, but please don't hold up the merge on me)</p>\n</blockquote>",
        "id": 270426633,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643823607
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1028321353\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697\">issue #3697</a>:</p>\n<blockquote>\n<p>Hmm, running into the <code>madvise</code> semantics gaps in <code>qemu</code> on s390x it seems -- I recall a similar issue when we were adding tests for aarch64 in the uffd work. I'll find a way to skip the tests when on qemu.</p>\n</blockquote>",
        "id": 270450797,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643833055
    }
]