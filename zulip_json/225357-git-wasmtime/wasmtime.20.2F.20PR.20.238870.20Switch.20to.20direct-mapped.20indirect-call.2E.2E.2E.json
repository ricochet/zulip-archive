[
    {
        "content": "<p>cfallin opened <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a> from <code>cfallin:direct-mapped-indirect-cache</code> to <code>bytecodealliance:main</code>:</p>\n<blockquote>\n<p>Currently, the indirect-call cache operates on a basis of <em>one slot per callsite</em>: each <code>call_indirect</code> instruction in a module, up to a limit, has its own slot of storage in the VM context struct that caches a called-table-index to called-raw-function-pointer mapping.</p>\n<p>This is fine but means the storage requirement scales with the module size; hence \"up to a limit\" above. It also means that each callsite needs to \"warm up\" separately, whereas we could in theory reuse the resolved index-&gt;code pointer mapping for the same index across callsites.</p>\n<p>This PR switches instead to a \"direct-mapped cache\": that is, we have a fixed number of cache slots per table per instance, of user-configurable count, and we look in a slot selected by the called table index (modulo the cache size). As before, if the \"tag\" (cache key, called table index) matches, we use the \"value\" (raw code pointer).</p>\n<p>The main advantage of this scheme, and my motivation for making the switch, is that the storage size is fixed and quite small, even for arbitrarily-large modules: for example, on a 64-bit platform with 12-byte slots(*) (4-byte key, 8-byte resolved pointer), for a module with one funcptr table, a 1K-entry cache uses 12KiB per instance. That's much smaller than the total VMFuncRef array size in large modules and should be no problem. My goal in getting to this constant size offset is that turning this on by default eventually will be easier to justify, and that we won't have unexpected perf cliffs for callsites beyond a certain index.</p>\n<p>This also means that if one callsite resolves index 23 to some raw code pointer, other callsites that call index 23 also receive a \"hit\" from that warmup. This could be beneficial when there are many callsites but a relatively smaller pool of called functions (e.g., ICs).</p>\n<p>The downside to caching indexed on callee rather than callsite is that if there are a large number of callees, we can expect more cache conflicts and hence misses. (If funcref table indices 1 and 1025 are both frequently called, a 1024-entry direct-mapped cache will thrash.) But I expect with ICs in particular to have a lot of callsites and relatively few (shared) callees.</p>\n<p>On Octane-compiled-to-Wasm with my JS AOT compilation tooling using <code>call_indirect</code> for all ICs, I see: baseline score (higher is better, proportional to runtime speed) of 2406, score with old one-entry-per-callsite scheme of 2479, score with this scheme of<br>\n2509. So it's slightly faster as well, probably due to a combination of the warmup benefit and a smaller cache footprint, even with the more involved logic to compute the slot address. (This also tells me the benefit of this cache is smaller than I had originally measured on a microbenchmark (20%) -- at 5% on all of Octane -- but that's still worth it, IMHO.)</p>\n<p>(*) Note that slots are not actually contiguous: I did a<br>\n    struct-of-arrays trick, separating cache tags from cache values, so<br>\n    that the assembly lowering can use scaling amodes (<code>vmctx + offset +\n    4*idx</code> for u32 accesses, and <code>8*idx</code> for u64 accesses) for more<br>\n    efficient code.</p>\n<p>&lt;!--<br>\nPlease make sure you include the following information:</p>\n<ul>\n<li>\n<p>If this work has been discussed elsewhere, please include a link to that<br>\n  conversation. If it was discussed in an issue, just mention \"issue #...\".</p>\n</li>\n<li>\n<p>Explain why this change is needed. If the details are in an issue already,<br>\n  this can be brief.</p>\n</li>\n</ul>\n<p>Our development process is documented in the Wasmtime book:<br>\n<a href=\"https://docs.wasmtime.dev/contributing-development-process.html\">https://docs.wasmtime.dev/contributing-development-process.html</a></p>\n<p>Please ensure all communication follows the code of conduct:<br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/CODE_OF_CONDUCT.md\">https://github.com/bytecodealliance/wasmtime/blob/main/CODE_OF_CONDUCT.md</a><br>\n--&gt;</p>\n</blockquote>",
        "id": 446756443,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719280532
    },
    {
        "content": "<p><strong>cfallin</strong> requested <a href=\"https://github.com/orgs/bytecodealliance/teams/wasmtime-fuzz-reviewers\">wasmtime-fuzz-reviewers</a> for a review on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 446756444,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719280533
    },
    {
        "content": "<p><strong>cfallin</strong> requested <a href=\"https://github.com/alexcrichton\">alexcrichton</a> for a review on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 446756445,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719280533
    },
    {
        "content": "<p><strong>cfallin</strong> requested <a href=\"https://github.com/orgs/bytecodealliance/teams/wasmtime-core-reviewers\">wasmtime-core-reviewers</a> for a review on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 446756446,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719280533
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<p>Currently, the indirect-call cache operates on a basis of <em>one slot per callsite</em>: each <code>call_indirect</code> instruction in a module, up to a limit, has its own slot of storage in the VM context struct that caches a called-table-index to called-raw-function-pointer mapping.</p>\n<p>This is fine but means the storage requirement scales with the module size; hence \"up to a limit\" above. It also means that each callsite needs to \"warm up\" separately, whereas we could in theory reuse the resolved index-&gt;code pointer mapping for the same index across callsites.</p>\n<p>This PR switches instead to a \"direct-mapped cache\": that is, we have a fixed number of cache slots per table per instance, of user-configurable count, and we look in a slot selected by the called table index (modulo the cache size). As before, if the \"tag\" (cache key, called table index) matches, we use the \"value\" (raw code pointer).</p>\n<p>The main advantage of this scheme, and my motivation for making the switch, is that the storage size is fixed and quite small, even for arbitrarily-large modules: for example, on a 64-bit platform with 12-byte slots(*) (4-byte key, 8-byte resolved pointer), for a module with one funcptr table, a 1K-entry cache uses 12KiB per instance. That's much smaller than the total VMFuncRef array size in large modules and should be no problem. My goal in getting to this constant size offset is that turning this on by default eventually will be easier to justify, and that we won't have unexpected perf cliffs for callsites beyond a certain index.</p>\n<p>This also means that if one callsite resolves index 23 to some raw code pointer, other callsites that call index 23 also receive a \"hit\" from that warmup. This could be beneficial when there are many callsites but a relatively smaller pool of called functions (e.g., ICs).</p>\n<p>The downside to caching indexed on callee rather than callsite is that if there are a large number of callees, we can expect more cache conflicts and hence misses. (If funcref table indices 1 and 1025 are both frequently called, a 1024-entry direct-mapped cache will thrash.) But I expect with ICs in particular to have a lot of callsites and relatively few (shared) callees.</p>\n<p>On Octane-compiled-to-Wasm with my JS AOT compilation tooling using <code>call_indirect</code> for all ICs, I see: baseline score (higher is better, proportional to runtime speed) of 2406, score with old one-entry-per-callsite scheme of 2479, score with this scheme of<br>\n2509. So it's slightly faster as well, probably due to a combination of the warmup benefit and a smaller cache footprint, even with the more involved logic to compute the slot address. (This also tells me the benefit of this cache is smaller than I had originally measured on a microbenchmark (20%) -- at 5% on all of Octane -- but that's still worth it, IMHO.)</p>\n<p>(*) Note that slots are not actually contiguous: I did a struct-of-arrays trick, separating cache tags from cache values, so that the assembly lowering can use scaling amodes (<code>vmctx + offset + 4*idx</code> for u32 accesses, and <code>8*idx</code> for u64 accesses) for more efficient code.</p>\n<p>&lt;!--<br>\nPlease make sure you include the following information:</p>\n<ul>\n<li>\n<p>If this work has been discussed elsewhere, please include a link to that<br>\n  conversation. If it was discussed in an issue, just mention \"issue #...\".</p>\n</li>\n<li>\n<p>Explain why this change is needed. If the details are in an issue already,<br>\n  this can be brief.</p>\n</li>\n</ul>\n<p>Our development process is documented in the Wasmtime book:<br>\n<a href=\"https://docs.wasmtime.dev/contributing-development-process.html\">https://docs.wasmtime.dev/contributing-development-process.html</a></p>\n<p>Please ensure all communication follows the code of conduct:<br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/CODE_OF_CONDUCT.md\">https://github.com/bytecodealliance/wasmtime/blob/main/CODE_OF_CONDUCT.md</a><br>\n--&gt;</p>\n</blockquote>",
        "id": 446756516,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719280568
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<p>Currently, the indirect-call cache operates on a basis of <em>one slot per callsite</em>: each <code>call_indirect</code> instruction in a module, up to a limit, has its own slot of storage in the VM context struct that caches a called-table-index to called-raw-function-pointer mapping.</p>\n<p>This is fine but means the storage requirement scales with the module size; hence \"up to a limit\" above. It also means that each callsite needs to \"warm up\" separately, whereas we could in theory reuse the resolved index-&gt;code pointer mapping for the same index across callsites.</p>\n<p>This PR switches instead to a \"direct-mapped cache\": that is, we have a fixed number of cache slots per table per instance, of user-configurable count, and we look in a slot selected by the called table index (modulo the cache size). As before, if the \"tag\" (cache key, called table index) matches, we use the \"value\" (raw code pointer).</p>\n<p>The main advantage of this scheme, and my motivation for making the switch, is that the storage size is fixed and quite small, even for arbitrarily-large modules: for example, on a 64-bit platform with 12-byte slots(*) (4-byte key, 8-byte resolved pointer), for a module with one funcptr table, a 1K-entry cache uses 12KiB per instance. That's much smaller than the total VMFuncRef array size in large modules and should be no problem. My goal in getting to this constant size offset is that turning this on by default eventually will be easier to justify, and that we won't have unexpected perf cliffs for callsites beyond a certain index.</p>\n<p>This also means that if one callsite resolves index 23 to some raw code pointer, other callsites that call index 23 also receive a \"hit\" from that warmup. This could be beneficial when there are many callsites but a relatively smaller pool of called functions (e.g., ICs).</p>\n<p>The downside to caching indexed on callee rather than callsite is that if there are a large number of callees, we can expect more cache conflicts and hence misses. (If funcref table indices 1 and 1025 are both frequently called, a 1024-entry direct-mapped cache will thrash.) But I expect with ICs in particular to have a lot of callsites and relatively few (shared) callees.</p>\n<p>On Octane-compiled-to-Wasm with my JS AOT compilation tooling using <code>call_indirect</code> for all ICs, I see: baseline score (higher is better, proportional to runtime speed) of 2406, score with old one-entry-per-callsite scheme of 2479, score with this scheme of 2509. So it's slightly faster as well, probably due to a combination of the warmup benefit and a smaller cache footprint, even with the more involved logic to compute the slot address. (This also tells me the benefit of this cache is smaller than I had originally measured on a microbenchmark (20%) -- at 5% on all of Octane -- but that's still worth it, IMHO.)</p>\n<p>(*) Note that slots are not actually contiguous: I did a struct-of-arrays trick, separating cache tags from cache values, so that the assembly lowering can use scaling amodes (<code>vmctx + offset + 4*idx</code> for u32 accesses, and <code>8*idx</code> for u64 accesses) for more efficient code.</p>\n<p>&lt;!--<br>\nPlease make sure you include the following information:</p>\n<ul>\n<li>\n<p>If this work has been discussed elsewhere, please include a link to that<br>\n  conversation. If it was discussed in an issue, just mention \"issue #...\".</p>\n</li>\n<li>\n<p>Explain why this change is needed. If the details are in an issue already,<br>\n  this can be brief.</p>\n</li>\n</ul>\n<p>Our development process is documented in the Wasmtime book:<br>\n<a href=\"https://docs.wasmtime.dev/contributing-development-process.html\">https://docs.wasmtime.dev/contributing-development-process.html</a></p>\n<p>Please ensure all communication follows the code of conduct:<br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/CODE_OF_CONDUCT.md\">https://github.com/bytecodealliance/wasmtime/blob/main/CODE_OF_CONDUCT.md</a><br>\n--&gt;</p>\n</blockquote>",
        "id": 446756530,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719280583
    },
    {
        "content": "<p>cfallin updated <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 446757655,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719280891
    },
    {
        "content": "<p>cfallin updated <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 446758564,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719281103
    },
    {
        "content": "<p>cfallin updated <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 446758756,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719281158
    },
    {
        "content": "<p>cfallin updated <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 446767004,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719285470
    },
    {
        "content": "<p>github-actions[bot] <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#issuecomment-2188026714\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<h4>Subscribe to Label Action</h4>\n<p>cc @fitzgen</p>\n<p>&lt;details&gt;<br>\nThis issue or pull request has been labeled: \"fuzzing\", \"wasmtime:api\", \"wasmtime:config\", \"wasmtime:ref-types\"</p>\n<p>Thus the following users have been cc'd because of the following labels:</p>\n<ul>\n<li>fitzgen: fuzzing, wasmtime:ref-types</li>\n</ul>\n<p>To subscribe or unsubscribe from this label, edit the &lt;code&gt;.github/subscribe-to-label.json&lt;/code&gt; configuration file.</p>\n<p><a href=\"https://github.com/bytecodealliance/subscribe-to-label-action\">Learn more.</a><br>\n&lt;/details&gt;</p>\n</blockquote>",
        "id": 446782906,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719294291
    },
    {
        "content": "<p>github-actions[bot] <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#issuecomment-2188027056\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<h4>Label Messager: wasmtime:config</h4>\n<p>It looks like you are changing Wasmtime's configuration options. Make sure to<br>\ncomplete this check list:</p>\n<ul>\n<li>\n<p>[ ] If you added a new <code>Config</code> method, you wrote extensive documentation for<br>\n      it.</p>\n<p>&lt;details&gt;</p>\n<p>Our documentation should be of the following form:</p>\n<p>```text<br>\nShort, simple summary sentence.</p>\n<p>More details. These details can be multiple paragraphs. There should be<br>\ninformation about not just the method, but its parameters and results as<br>\nwell.</p>\n<p>Is this method fallible? If so, when can it return an error?</p>\n<p>Can this method panic? If so, when does it panic?</p>\n<h1>Example</h1>\n<p>Optional example here.<br>\n```</p>\n<p>&lt;/details&gt;</p>\n</li>\n<li>\n<p>[ ] If you added a new <code>Config</code> method, or modified an existing one, you<br>\n  ensured that this configuration is exercised by the fuzz targets.</p>\n<p>&lt;details&gt;</p>\n<p>For example, if you expose a new strategy for allocating the next instance<br>\nslot inside the pooling allocator, you should ensure that at least one of our<br>\nfuzz targets exercises that new strategy.</p>\n<p>Often, all that is required of you is to ensure that there is a knob for this<br>\nconfiguration option in [<code>wasmtime_fuzzing::Config</code>][fuzzing-config] (or one<br>\nof its nested <code>struct</code>s).</p>\n<p>Rarely, this may require authoring a new fuzz target to specifically test this<br>\nconfiguration. See [our docs on fuzzing][fuzzing-docs] for more details.</p>\n<p>&lt;/details&gt;</p>\n</li>\n<li>\n<p>[ ] If you are enabling a configuration option by default, make sure that it<br>\n  has been fuzzed for at least two weeks before turning it on by default.</p>\n</li>\n</ul>\n<p>[fuzzing-config]: <a href=\"https://github.com/bytecodealliance/wasmtime/blob/ca0e8d0a1d8cefc0496dba2f77a670571d8fdcab/crates/fuzzing/src/generators.rs#L182-L194\">https://github.com/bytecodealliance/wasmtime/blob/ca0e8d0a1d8cefc0496dba2f77a670571d8fdcab/crates/fuzzing/src/generators.rs#L182-L194</a><br>\n[fuzzing-docs]: <a href=\"https://docs.wasmtime.dev/contributing-fuzzing.html\">https://docs.wasmtime.dev/contributing-fuzzing.html</a></p>\n<hr>\n<p>&lt;details&gt;</p>\n<p>To modify this label's message, edit the &lt;code&gt;.github/label-messager/wasmtime-config.md&lt;/code&gt; file.</p>\n<p>To add new label messages or remove existing label messages, edit the<br>\n&lt;code&gt;.github/label-messager.json&lt;/code&gt; configuration file.</p>\n<p><a href=\"https://github.com/bytecodealliance/label-messager-action\">Learn more.</a></p>\n<p>&lt;/details&gt;</p>\n</blockquote>",
        "id": 446782950,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719294312
    },
    {
        "content": "<p>alexcrichton submitted <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#pullrequestreview-2139465692\">PR review</a>:</p>\n<blockquote>\n<p>Before I dive too deep into this, I don't think that what's implemented is sufficient. For example this code:</p>\n<div class=\"codehilite\" data-code-language=\"wasm\"><pre><span></span><code>(module\n  (func (export \"_start\")\n    i32.const 0\n    call_indirect\n    i32.const 0\n    call_indirect (result i32)\n    drop\n  )\n\n  (func $a)\n\n  (table 10 10 funcref)\n  (elem (offset (i32.const 1)) func $a)\n)\n</code></pre></div>\n<p>runs as:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"cp\">$</span><span class=\"w\"> </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">q</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\">  </span><span class=\"o\">-</span><span class=\"n\">O</span><span class=\"w\"> </span><span class=\"n\">cache</span><span class=\"o\">-</span><span class=\"n\">call</span><span class=\"o\">-</span><span class=\"n\">indirects</span><span class=\"o\">=</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">wat</span>\n<span class=\"n\">Error</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"> </span><span class=\"n\">module</span><span class=\"w\"> </span><span class=\"err\">`</span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">wat</span><span class=\"err\">`</span>\n\n<span class=\"n\">Caused</span><span class=\"w\"> </span><span class=\"n\">by</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">invoke</span><span class=\"w\"> </span><span class=\"n\">command</span><span class=\"w\"> </span><span class=\"n\">default</span>\n<span class=\"w\">    </span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">error</span><span class=\"w\"> </span><span class=\"k\">while</span><span class=\"w\"> </span><span class=\"n\">executing</span><span class=\"w\"> </span><span class=\"n\">at</span><span class=\"w\"> </span><span class=\"n\">wasm</span><span class=\"w\"> </span><span class=\"n\">backtrace</span><span class=\"p\">:</span>\n<span class=\"w\">           </span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"w\">   </span><span class=\"mh\">0x3a</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"n\">unknown</span><span class=\"o\">&gt;!&lt;</span><span class=\"n\">wasm</span><span class=\"w\"> </span><span class=\"n\">function</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">&gt;</span>\n<span class=\"w\">    </span><span class=\"mi\">2</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">wasm</span><span class=\"w\"> </span><span class=\"n\">trap</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">uninitialized</span><span class=\"w\"> </span><span class=\"n\">element</span>\n<span class=\"cp\">$</span><span class=\"w\"> </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">q</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">O</span><span class=\"w\"> </span><span class=\"n\">cache</span><span class=\"o\">-</span><span class=\"n\">call</span><span class=\"o\">-</span><span class=\"n\">indirects</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">wat</span>\n<span class=\"n\">zsh</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">segmentation</span><span class=\"w\"> </span><span class=\"n\">fault</span><span class=\"w\">  </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">q</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">O</span><span class=\"w\"> </span><span class=\"n\">cache</span><span class=\"o\">-</span><span class=\"n\">call</span><span class=\"o\">-</span><span class=\"n\">indirects</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">wat</span>\n</code></pre></div>\n<p>Notably I think that the cache key can't just be the index but must also be the type used as well? I'm not sure how much the load of the VMSharedTypeIndex will affect perf, so I figure it's probably best to figure this out first before diving too deep in review.</p>\n</blockquote>",
        "id": 446964208,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719340272
    },
    {
        "content": "<p>alexcrichton submitted <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#pullrequestreview-2139465692\">PR review</a>:</p>\n<blockquote>\n<p>Before I dive too deep into this, I don't think that what's implemented is sufficient. For example this code:</p>\n<div class=\"codehilite\" data-code-language=\"wasm\"><pre><span></span><code>(module\n  (func (export \"_start\")\n    i32.const 0\n    call_indirect\n    i32.const 0\n    call_indirect (result i32)\n    drop\n  )\n\n  (func $a)\n\n  (table 10 10 funcref)\n  (elem (offset (i32.const 1)) func $a)\n)\n</code></pre></div>\n<p>runs as:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"cp\">$</span><span class=\"w\"> </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">q</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\">  </span><span class=\"o\">-</span><span class=\"n\">O</span><span class=\"w\"> </span><span class=\"n\">cache</span><span class=\"o\">-</span><span class=\"n\">call</span><span class=\"o\">-</span><span class=\"n\">indirects</span><span class=\"o\">=</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">wat</span>\n<span class=\"n\">Error</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"> </span><span class=\"n\">module</span><span class=\"w\"> </span><span class=\"err\">`</span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">wat</span><span class=\"err\">`</span>\n\n<span class=\"n\">Caused</span><span class=\"w\"> </span><span class=\"n\">by</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">invoke</span><span class=\"w\"> </span><span class=\"n\">command</span><span class=\"w\"> </span><span class=\"n\">default</span>\n<span class=\"w\">    </span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">error</span><span class=\"w\"> </span><span class=\"k\">while</span><span class=\"w\"> </span><span class=\"n\">executing</span><span class=\"w\"> </span><span class=\"n\">at</span><span class=\"w\"> </span><span class=\"n\">wasm</span><span class=\"w\"> </span><span class=\"n\">backtrace</span><span class=\"p\">:</span>\n<span class=\"w\">           </span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"w\">   </span><span class=\"mh\">0x3a</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"n\">unknown</span><span class=\"o\">&gt;!&lt;</span><span class=\"n\">wasm</span><span class=\"w\"> </span><span class=\"n\">function</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">&gt;</span>\n<span class=\"w\">    </span><span class=\"mi\">2</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">wasm</span><span class=\"w\"> </span><span class=\"n\">trap</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">uninitialized</span><span class=\"w\"> </span><span class=\"n\">element</span>\n<span class=\"cp\">$</span><span class=\"w\"> </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">q</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">O</span><span class=\"w\"> </span><span class=\"n\">cache</span><span class=\"o\">-</span><span class=\"n\">call</span><span class=\"o\">-</span><span class=\"n\">indirects</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">wat</span>\n<span class=\"n\">zsh</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">segmentation</span><span class=\"w\"> </span><span class=\"n\">fault</span><span class=\"w\">  </span><span class=\"n\">cargo</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">q</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">O</span><span class=\"w\"> </span><span class=\"n\">cache</span><span class=\"o\">-</span><span class=\"n\">call</span><span class=\"o\">-</span><span class=\"n\">indirects</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"p\">.</span><span class=\"n\">wat</span>\n</code></pre></div>\n<p>Notably I think that the cache key can't just be the index but must also be the type used as well? I'm not sure how much the load of the VMSharedTypeIndex will affect perf, so I figure it's probably best to figure this out first before diving too deep in review.</p>\n</blockquote>",
        "id": 446964209,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719340272
    },
    {
        "content": "<p>alexcrichton created <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#discussion_r1653322664\">PR review comment</a>:</p>\n<blockquote>\n<p>This should use <code>ptr_size</code> to handle cross-compiles between 32/64-bit (it's somewhere on <code>VMOffsets</code>, I forget which)</p>\n</blockquote>",
        "id": 446964210,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719340272
    },
    {
        "content": "<p>alexcrichton created <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#discussion_r1653323565\">PR review comment</a>:</p>\n<blockquote>\n<p>Also could this and the above go ahead and use <code>ishl</code> instead of <code>imul_imm</code>?</p>\n</blockquote>",
        "id": 446964211,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719340273
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#issuecomment-2189728751\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<p>Er I actually flubbed the test a bit and discovered a bug I didn't mean to discover. This is what I intended:</p>\n<div class=\"codehilite\" data-code-language=\"wasm\"><pre><span></span><code>(module\n  (func (export \"_start\")\n    i32.const 1 ;; not 0 unlike above\n    call_indirect\n    i32.const 1 ;; not 0 unlike above\n    call_indirect (result i32)\n    drop\n  )\n\n  (func $a)\n\n  (table 10 10 funcref)\n  (elem (offset (i32.const 1)) func $a)\n)\n</code></pre></div>\n<p>which runs successfully with <code>-O cache-call-indirects</code> but fails without it due to a signature mismatch error. This is what I was trying to reproduce above and is my comment about how the cache key needs to take the tag into account as well. Otherwise though I'm not actually sure what the segfault above is, but that might be more benign.</p>\n<hr>\n<p>Out of curiosity I was also curious, had I not caught this, if fuzzing would catch it. Forcing differential execution of Wasmtime-against-Wasmtime and module generation with <code>wasm-smith</code> this isn't showing up after ~10 minutes of 30 cores fuzzing. This might be something where cache-call-indirects isn't getting a hit on the fuzz modules being configured (does it require the table min == max? I forget...). Basically our fuzz coverage of this may not be that great right now (although oss-fuzz is generally better about coverage than local runs)</p>\n</blockquote>",
        "id": 446967780,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719341330
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#issuecomment-2189729690\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<blockquote>\n<p>Notably I think that the cache key can't just be the index but must also be the type used as well? I'm not sure how much the load of the VMSharedTypeIndex will affect perf, so I figure it's probably best to figure this out first before diving too deep in review.</p>\n</blockquote>\n<p>Ah, could you say more why the type is necessary? In my (possibly naive) view, we are choosing the cache slot to look at a different way, but nothing else should change (obviously there is some breakage here which I'll take a look at -- team meeting next two days so slight delay).</p>\n</blockquote>",
        "id": 446967838,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719341346
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#issuecomment-2189739319\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<p>The behavior of <code>call_indirect</code> is dependent on the table being used, the runtime index, and the type ascribed on <code>call_indirect</code>, but the implementation here only takes two of these into account. The caches are per-table (I think? I didn't dive that deep) and the cache key handles the runtime index. Basically the previous behavior would implicitly take all three into account by having a cache-per-callsite. This implementation has where multiple call sites can share the same cache slot (intentional) but if each call site has different types ascribed to the <code>call_indirect</code> then they have to have different behavior.</p>\n<p>The basic tl;dr; though is that this PR as-is violates wasm-semantics and isn't memory safe. (insofar that you can successfully call a function with the wrong signature)</p>\n</blockquote>",
        "id": 446968535,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719341575
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#issuecomment-2189858938\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<p>Ah, I completely missed the dynamic signature check -- will have to incorporate that probably by baking it into the cache key somehow. (I don't think we'll need a load on the hot path at all.) Solution TBD, thanks!</p>\n</blockquote>",
        "id": 446981844,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719345412
    },
    {
        "content": "<p>fitzgen submitted <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#pullrequestreview-2139737015\">PR review</a>:</p>\n<blockquote>\n<p>Overall, modulo Alex's point about types, this looks great! I like this a lot better than having a cliff where call sites beyond the cliff don't get caching.</p>\n<p>In addition to the point Alex made about types, I think it is worth thinking about (and testing!) the case where we have two tables, both sharing the same cache entries, but where one is a table of untyped <code>funcref</code>s and the other is a table of <code>(ref $some_specific_func_type)</code>.</p>\n<p>In general, I think it is also worth adding a few combinatorial-y unit tests where we have <code>cache_size_log2 == 0</code> (i.e. a single, shared cache entry) and we hammer on that cache entry with a variety of different calls from different sites with the same and different expected types and via different tables with different types of <code>funcref</code>s.</p>\n</blockquote>",
        "id": 446985450,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719346484
    },
    {
        "content": "<p>fitzgen submitted <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#pullrequestreview-2139737015\">PR review</a>:</p>\n<blockquote>\n<p>Overall, modulo Alex's point about types, this looks great! I like this a lot better than having a cliff where call sites beyond the cliff don't get caching.</p>\n<p>In addition to the point Alex made about types, I think it is worth thinking about (and testing!) the case where we have two tables, both sharing the same cache entries, but where one is a table of untyped <code>funcref</code>s and the other is a table of <code>(ref $some_specific_func_type)</code>.</p>\n<p>In general, I think it is also worth adding a few combinatorial-y unit tests where we have <code>cache_size_log2 == 0</code> (i.e. a single, shared cache entry) and we hammer on that cache entry with a variety of different calls from different sites with the same and different expected types and via different tables with different types of <code>funcref</code>s.</p>\n</blockquote>",
        "id": 446985451,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719346484
    },
    {
        "content": "<p>fitzgen created <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#discussion_r1653498719\">PR review comment</a>:</p>\n<blockquote>\n<p>Might want to clamp this value to a reasonable range?</p>\n</blockquote>",
        "id": 446985452,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719346484
    },
    {
        "content": "<p>fitzgen created <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#discussion_r1653484647\">PR review comment</a>:</p>\n<blockquote>\n<p>Nitpick: can we use the same variable names in the comments' pseudocode and the builder variables? Annoying to translate between them in my head.</p>\n</blockquote>",
        "id": 446985453,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719346484
    },
    {
        "content": "<p>cfallin updated <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 447390187,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719466381
    },
    {
        "content": "<p>cfallin updated <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 447390334,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719466486
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#issuecomment-2193795211\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<p>Excellent finds on both -- fixes pushed to (i) tag cache entries with signature ID as well, (ii) check for null code pointer (the feature on <code>main</code> also doesn't catch this; not sure why fuzzing hasn't hit it yet). It's late so I'll test perf tomorrow. Hopefully still a win: the key advantage here should be avoiding the dependent cache misses inherent in the two-level pointer chase of the <code>VMFuncRef</code>, but this is an objective question I can answer concretely soon :-)</p>\n</blockquote>",
        "id": 447390591,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719466689
    },
    {
        "content": "<p>cfallin submitted <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#pullrequestreview-2145707820\">PR review</a>.</p>",
        "id": 447504399,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719501033
    },
    {
        "content": "<p>cfallin created <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#discussion_r1657317828\">PR review comment</a>:</p>\n<blockquote>\n<p>Done both!</p>\n</blockquote>",
        "id": 447504401,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719501033
    },
    {
        "content": "<p>cfallin submitted <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#pullrequestreview-2145708462\">PR review</a>.</p>",
        "id": 447504454,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719501048
    },
    {
        "content": "<p>cfallin created <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#discussion_r1657318204\">PR review comment</a>:</p>\n<blockquote>\n<p>Done</p>\n</blockquote>",
        "id": 447504455,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719501049
    },
    {
        "content": "<p>cfallin submitted <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#pullrequestreview-2145723392\">PR review</a>.</p>",
        "id": 447505465,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719501318
    },
    {
        "content": "<p>cfallin created <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#discussion_r1657326140\">PR review comment</a>:</p>\n<blockquote>\n<p>Ah, it's already clamped in the method on <code>Config</code>, so preferred to let fuzzing maximally poke the API surface here (no special limits). I guess we could do something that avoids biasing the randomness (as-is, the <code>Arbitrary</code> range on the <code>u8</code> maps 0..=255 to 0..=16 with 15/16ths of the space going to 16) but I don't think it's too important to evenly spread effort across sizes that way...</p>\n</blockquote>",
        "id": 447505466,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719501318
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870#issuecomment-2195024360\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>:</p>\n<blockquote>\n<p>On testing the newest version of this PR, that correctly keeps a cache tag for signature ID and also correctly checks for null(*), I'm now seeing a 2.5% slowdown on Octane in AOT JS relative to no caching at all. So it seems this has crossed the instruction-count tradeoff threshold (the benchmarks have quite high IPC so it's about bulk of instruction stream, not cache misses).</p>\n<p>Given the smaller delta I'm seeing now with indirect-call caching in full benchmarks, vs. the microbenchmark I was using to guide me at the time, I think I will actually opt to remove indirect-call caching altogether: the null handling in <code>main</code>'s version is also incorrect (see footnote below) and I'd rather not have the complexity, perf cliff on module size, and vmctx size overhead if it's worth relatively little.</p>\n<p>(*) I had previously incorrectly remembered the SIGSEGV-for-call-to-null-funcref mechanism as working by catching a call to address 0; actually it works by catching a SIGSEGV on a load from the <code>VMFuncRef</code>, so the caching mechanism here would not catch that (and would \"safely\" crash wasmtime at least, rather than execute type-unsafely, but that's still wrong).</p>\n</blockquote>",
        "id": 447508656,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719502211
    },
    {
        "content": "<p>cfallin closed without merge <a href=\"https://github.com/bytecodealliance/wasmtime/pull/8870\">PR #8870</a>.</p>",
        "id": 447508661,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1719502213
    }
]