[
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1023644707\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>Benchmarking: the delta is hard to see with memory initialization taking most of the time (pre-#3699); a few percent improvement in the mean with <code>spidermonkey.wasm</code> but Criterion says not statistically significant.</p>\n<p>However, in #3697 I <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020711683\">measured</a> approximately a factor-of-two improvement in the <code>initialize_vmcontext</code> hotpath (which was most of instantiation, in a scenario where we have a memory technique to reuse mappings).</p>\n</blockquote>",
        "id": 269638985,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643318086
    },
    {
        "content": "<p>cfallin edited a <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1023644707\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>Benchmarking: the delta is hard to see with memory initialization taking most of the time (pre-#3697); a few percent improvement in the mean with <code>spidermonkey.wasm</code> but Criterion says not statistically significant.</p>\n<p>However, in #3697 I <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020711683\">measured</a> approximately a factor-of-two improvement in the <code>initialize_vmcontext</code> hotpath (which was most of instantiation, in a scenario where we have a memory technique to reuse mappings).</p>\n</blockquote>",
        "id": 269639026,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643318108
    },
    {
        "content": "<p>github-actions[bot] <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1023673037\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<h4>Subscribe to Label Action</h4>\n<p>cc @peterhuene</p>\n<p>&lt;details&gt;<br>\nThis issue or pull request has been labeled: \"wasmtime:api\"</p>\n<p>Thus the following users have been cc'd because of the following labels:</p>\n<ul>\n<li>peterhuene: wasmtime:api</li>\n</ul>\n<p>To subscribe or unsubscribe from this label, edit the &lt;code&gt;.github/subscribe-to-label.json&lt;/code&gt; configuration file.</p>\n<p><a href=\"https://github.com/bytecodealliance/subscribe-to-label-action\">Learn more.</a><br>\n&lt;/details&gt;</p>\n</blockquote>",
        "id": 269644897,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643320525
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1023683443\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>Thanks for splitting this out! I've mentioned a few things here in other contexts but I think it's good to get them all in once place. The tl;dr; is that I don't personally feel like this is the best approach, but I think we can still get the same wins.</p>\n<p>Due to a number of implementation details I don't think that this PR is really getting all that much of a benefit from laziness. The anyfunc array is used as storage for a module's <code>*mut VMCallerCheckedAnyfunc</code> which is used as the representation of a <code>funcref</code>. This is subsequently used by things like <code>ref.func</code>, tables, and exports. Today the anyfunc array is the size of the module's index space of functions, but this is actually much larger than necessary because we only use it for those items, and not all functions make their way into <code>ref.func</code>, tables, or exports. This PR also doesn't lazily initialize tables, only the anyfunc array, which means we still initialize all anyfuncs used as part of tables.</p>\n<p>Adding all that together we still pay the full cost of anyfunc initialization for all table entries when a module is instantiated. The cost of initializing exports is done lazily and most modules nowadays don't have <code>ref.func</code> instructions so there's not much cost there. Overall this means that this PR is improving the amount of work done, but I think it's doing it in a bit of a roundabout way. Effectively we're immediately initializing a static subset of the anyfunc array on initialization and we're never actually initializing anything else after that (modulo exports but that's such a small set I don't think it really matters all that much).</p>\n<p>Given all that personally think that a better implementation would be to avoid the laziness complications and instead just go ahead and keep doing what we do today, only over the same set of functions that this PR is doing it for. Basically I think we should shrink the anyfunc array to the size of <a href=\"https://github.com/bytecodealliance/wasmtime/blob/7928a3ffb4ddce1d60ef7634a9cf79c68258c1c0/crates/environ/src/module_environ.rs#L64-L74\">the <code>escaped_funcs</code> array</a>. That I think means that the same magnitude of work would be done on instantiation and there wouldn't be any need to deal with laziness anywhere.</p>\n<hr>\n<p>As a few other side things:</p>\n<ul>\n<li>For \"benchmarking\" this I think it might be good to take a module with 1k functions and a table with 100 of those functions and instantiate that. That should get memory out of the picture and should help focus on just the time it takes to deal with all the anyfuncs.</li>\n<li>For the race comment, I'm under the impression that because the writes are non-atomic that any concurrent access to <code>get_caller_checked_anyfunc</code> would result in a data race. I don't think we have concurrent accesses today though so this is probably fine, but if we were to stick with this PR I'd prefer to either make <code>get_caller_checked_anyfunc</code> unsafe or make it take <code>&amp;mut self</code>.</li>\n</ul>\n<hr>\n<p>Ok now all that being said, one thing you mentioned the other day which I'm coming around to is that long-term I think it would be best to lazily initialize tables instead of just this anyfunc array. With CoW and uffd we're already effectively lazily initializing memory (just relying on the kernel to do so), and I think it would be best to do that for tables as well. I don't really know how we'd do this off the top of my head though.</p>\n<p>Alternatively we could try to figure out how to make the table initialization much more \"static\" where work is done at compile time instead of instantiation time. I had one idea <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3697#issuecomment-1020240918\">over here</a> but that's sort of half-baked and only shrinks <code>VMCallerCheckedAnyfunc</code>, it doesn't offload too much to compile time. Overall doing something at compile-time I think would also be better than the lazy approach when we can.</p>\n</blockquote>",
        "id": 269647165,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643321405
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1023703965\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>@alexcrichton thanks for your thoughts here!</p>\n<p>Top-level: I agree that in practice, eagerly initializing a smaller array-of-anyfuncs which is only \"anyfuncs referenced in tables or by ref.func\" is more-or-less equivalent to this PR's lazy approach. (In the presence of many <code>ref.func</code>s referring to functions that are not in tables, this PR should still win a bit, I think.)</p>\n<p>The main attraction I had to this approach was simplicity and small impact -- indexing via <code>escaped_funcs</code> indices is definitely workable too but implies a bit more indirection.</p>\n<p>The other thing about a pervasively lazy approach is that it paves the way for more gains later, as I think you're getting at with your second-to-last paragraph. If we're lazy about <em>requesting</em> anyfunc pointers (ie not initializing table elements until accessed), then the laziness in filling in the pointed-to anyfuncs finally pays off. And I think the change to get to this point is pretty simple -- just a null check when loading from the table and a slowpath that calls <code>get_caller_checked_anyfunc</code> from inside a libcall, then initializes the table with the result.</p>\n<p>Of course we could do both, and that has benefits too -- both shrinking the size of the vmcontext, and maybe doing things eagerly at first, does not preclude us from lazily initializing later...</p>\n<p>A specific note too:</p>\n<blockquote>\n<p>For the race comment, I'm under the impression that because the writes are non-atomic that any concurrent access to get_caller_checked_anyfunc would result in a data race.</p>\n</blockquote>\n<p>Yes, exactly, the goal is to take advantage of a benign race (and come out with correct results regardless). Maybe to make the compiler happy we need to do this all through atomics and <code>Ordering::Relaxed</code>, as long as the bitmap update compiles down to something without the <code>lock</code> prefix on x86-64; the <code>lock or</code> was the major hotspot in my tests otherwise.</p>\n<p>The comment is attempting to give a safety argument why this is still OK in the presence of concurrent accesses; we should do whatever we need to at the language semantics level but I think I've convinced myself that the resulting machine code / access pattern is sound :-)</p>\n</blockquote>",
        "id": 269650456,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643322847
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1023836944\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>@alexcrichton I've reformulated the benign-racy init path now to use atomics for the bitmap accesses at least. There is still a (very intentional!) race, in that the bit-set is not done with an atomic <code>fetch_or</code> but rather by just storing the loaded value with the new bit set, and no compare-exchange or anything like that. This is to avoid any pipeline serializations, bus locking, etc in the hot-path. The comment explains why this is safe; the tl;dr is that it depends on the fact that all initializations write the same values (idempotent writes) so spurious re-initializations are acceptable.</p>\n</blockquote>",
        "id": 269679720,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643338374
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1024292639\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>I would personally not say that this PR is simple with a small impact, due to a few consequences:</p>\n<ul>\n<li>There's a new <code>InstanceAllocationInfo</code> structure to juggle which should theoretically be managed with other module-level resources like <code>Arc&lt;Module&gt;</code> but isn't. This means while it logically has the same lifetime as other metadata it practically doesn't because it was developed at a different time.</li>\n<li>This inflates the size of <code>VMContext</code> with extra flag bits for whether types are initialized.</li>\n<li>This, as currently written, tries to deal with a race that never actually happens in practice. I don't believe that the <code>get_caller_checked_anyfunc</code> function is ever actually called concurrently (in that Wasmtime's APIs I don't think provide any means of doing so) so the extra synchronization/comments aren't actually necessary.</li>\n</ul>\n<p>Overall I continue to feel that this is a fair bit of complication, especially with  <code>InstanceAllocationInfo</code>, for what ends up being not a huge benefit. This current lazy strategy really isn't all that lazy, it basically immediately initializes everything used for tables/exports and then never initializes anything else in the anyfunc array, so the laziness is never actually leveraged (it just makes initial initialization slower since the flags are all checked and written).</p>\n<p>I'm less certain about the idea of paving the way for gains later as well. I understand how this is sort of theoretically applicable but we're talking about nanoseconds-per-element \"laziness\" which feels like too small of a granularity to manage to me. If we get around to implementing a fully-lazily-initialized table approach (which I'm not entirely sure how we'd do that) it seems like it might be at a more bulk-level instead of at a per-element level. I would also expect that whatever laziness mechanism that would use would subsume this one since there's not really much need for cascading laziness.</p>\n<p>Naively I would expect that using <code>escaped_funcs</code> would be workable to implement. It seems like we'd assign entries in <code>escaped_funcs</code> an index (which would probably be a side table stored in <code>Module</code>) and then <code>ref.func</code> would use this index to get to the anyfunc array as well as table element initialization going through this (perhaps pre-computing something else in <code>Module</code> to avoid too many levels of array acceses during initialization).</p>\n<blockquote>\n<p>I've reformulated the benign-racy init path now to use atomics for the bitmap accesses at least</p>\n</blockquote>\n<p>I believe this is still a theoretical data race that Rust disallows. Despite the machine code working just fine at the LLVM layer any concurrent unsynchronized writes are a data race and undefined behavior, so any concurrent writes would have to be atomic. (again though I don't think it's worthwhile trying to fix this because I don't think this overall laziness approach is what we want either)</p>\n</blockquote>",
        "id": 269746614,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643381247
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1024399525\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>@alexcrichton thanks for your comments!</p>\n<blockquote>\n<p>I'm less certain about the idea of paving the way for gains later as well. I understand how this is sort of theoretically applicable but we're talking about nanoseconds-per-element \"laziness\" which feels like too small of a granularity to manage to me. If we get around to implementing a fully-lazily-initialized table approach (which I'm not entirely sure how we'd do that) it seems like it might be at a more bulk-level instead of at a per-element level. I would also expect that whatever laziness mechanism that would use would subsume this one since there's not really much need for cascading laziness.</p>\n</blockquote>\n<p>So, a few things:</p>\n<ol>\n<li>\n<p>I think the per-element lazy initialization is going to be an important part of any lazy table init. As I think @fitzgen mentioned earlier as well, there's not necessarily any spatial correlation between uses of adjacent table elements. So imagine I have 1000 funcs in a table, and I pull out index 553 to return to you, and never touch anything else. Do we initialize the whole table at that point? Or a block of 100 surrounding the accessed index? The larger the \"bulk init\", the greater the waste -- accessing index 553 probably doesn't mean I'm more likely to access 554 or 552 in the future than any other index.</p>\n</li>\n<li>\n<p>Given that, I think that \"cascading laziness\" is not really the right way to characterize this change, plus a lazy-table-init change on top. It's more like this change is a necessary implementation piece of the latter. If the top layer is lazy (at the whole-table, or table-chunk, or individual-element level) but we don't have a way of avoiding setting up the anyfuncs the top layer points to, then we haven't carried through the work savings.</p>\n</li>\n<li>\n<p>Re: \"I'm not entirely sure how we'd do that\" (lazy table init), do you have any thoughts on the scheme I proposed, with a null check on each table.get that branches to a slowpath libcall?</p>\n</li>\n<li>\n<p>Reducing to the list of escaped-funcs only in the anyfuncs table is definitely possible, and I'm happy to go that way if you'd prefer for now. But re: above, I think it doesn't really do anything to prepare us for a lazy-table-init world. This change was designed to sort of push us in that direction, and avoid the need for side-tables and indirections.</p>\n</li>\n</ol>\n<p>Otherwise, simplicity is subjective, I guess :-) The factoring of state here is I think going to be necessary for most approaches that initialize this general state after instantiation -- we probably can't get away from having e.g. signature info at libcall time.</p>\n<p>I won't argue the concurrency bit here is complex -- maybe the better answer is to put together an actual lazy-table-init implementation, and then get rid of the initialization bitmap and go back to the zero-func-ptr-means-uninitialized design I had earlier.</p>\n<p>Maybe it would help if I \"drew the rest of the owl\" here and actually sketched lazy table initialization as well? This would perhaps make the picture a bit clearer how all of this could work together, at least as I'm imagining it.</p>\n</blockquote>",
        "id": 269763944,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643388100
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1024498448\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>Ah so my thinking of a more bulk-style initialization is me being inspired by what we're doing for memories where we fault in by page rather than all at once. I'm sort of naively assuming that some level of granularity would also be similar on tables, but byte access patterns may not really have any correlation to wasm-table-access-patterns so it may just be a flawed assumption on my part.</p>\n<p>Also I think I understand now more how something like this would be required for full laziness. I'm still somewhat skeptical that it would need to look precisely like this but I see how my thinking of cascading laziness being unnecessary is flawed.</p>\n<p>And finally sorry I meant to respond to the table point but I forgot! That does sound workable but I think we still need to be careful. For example if an element segment initializes an imported table I don't think that we can skip that. Additionally we'd have to be careful to implement <code>Table::get</code> properly in the embedding API for lazily-initialized tables (I don't think that all the access to do the lazy initialization is trivially there today).</p>\n<hr>\n<p>I think that my preference at this point is to largely let performance guide us. The performance win in this PR is to not initialize anyfuncs for everything that never needs an anyfunc. There's still some unrealized performance wins though:</p>\n<ul>\n<li>Fully lazily initializating anyfuncs (requires lazy tables since table init today force-initializes the anyfuncs)</li>\n<li>Shrinking the vmcontext to only have anyfuncs for what's necessary</li>\n</ul>\n<p>Given that this is all peformance work effectively I think I'd prefer to be guided by numbers. I would be curious to benchmark this implementation here vs one that shrinks the anyfunc array like I've been thinking. The performance difference may come out in the wash but given how small the numbers are here I could imagine it being significant. I think we could then use a similar benchmark for measuring a fully-lazy-table approach.</p>\n<p>Overall I'm still hesitant to commit to design work which will largely only come to fruition in the future when the future is still somewhat uncertain (e.g. the precise design of a fully-lazy-table approach). In that sense I'd prefer to get the win we'll always want in any case, shrinking the VMContext, here and work on the laziness afterwards. (or doing it all at once is fine, but I'd prefer to not be in an inbetween state where we still have a big VMContext and some of it is initialized at instantiation time)<br>\n</p>\n</blockquote>",
        "id": 269780839,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643394678
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1024812162\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<blockquote>\n<p>I believe this is still a theoretical data race that Rust disallows. Despite the machine code working just fine at the LLVM layer any concurrent unsynchronized writes are a data race and undefined behavior, so any concurrent writes would have to be atomic. (again though I don't think it's worthwhile trying to fix this because I don't think this overall laziness approach is what we want either)</p>\n</blockquote>\n<p>I was almost done with my Friday, then a thought occurred to me -- we can just make the anyfunc fields atomics as well and do relaxed-ordering loads/stores. This satisfies the language's memory model; now all racing accesses are to atomics, and there is a Release-to-Acquire edge on the bitmap update that ensures the relaxed stores during init are seen by relaxed loads during normal use. This part at least should be fully theoretically sound now, I think.</p>\n<p>I'll do the rest of the lazy-table init as it is in my head on Monday, just to sketch it and be able to measure, I think. This work will be compatible with / complementary to any work that reduces the number of anyfuncs as well...</p>\n</blockquote>",
        "id": 269831270,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643423323
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1025962755\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>Er sorry but to reiterate again, I do not think anything _should_ be atomic here. This method is statically not possible to call concurrently (or so I believe) so I do not think we should be \"working around\" the compiler in this case. Instead I think the method should change to <code>&amp;mut self</code> or otherwise be <code>unsafe</code> and indicate that callers must be <code>&amp;mut self</code> or otherwise prevent concurrent calls.</p>\n</blockquote>",
        "id": 270059656,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643646298
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733#issuecomment-1025978091\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/3733\">issue #3733</a>:</p>\n<blockquote>\n<p>Ah, OK; I had been assuming that concurrent calls actually <em>are</em> possible via the \"get an export\" path. Within <code>wasmtime_runtime</code> at least, that's all via <code>&amp;self</code> methods. But I see that at the <code>wasmtime</code> layer this takes a mut Store (or <code>AsContextMut</code> or whatnot) so we actually do have mutual exclusion. I agree that given this, just using \"unsafe\" is way simpler.</p>\n</blockquote>",
        "id": 270062187,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643647196
    }
]