[
    {
        "content": "<p>cfallin opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>After some discussion with @alexcrichton just now, the question of why wasmtime's pooling allocator maintains separate pools for memories, tables, and instance structs (and stacks, but those are special on Windows at least) arose.</p>\n<p>Especially if we're considering using <code>madvise</code> to flash-clear state and make instantiation faster, it might make sense to investigate whether we can \"transpose\" the arrangement and put all of an instance's resources in one contiguous region (with appropriate guards of course).</p>\n<p>For example, one instance slot could contain all of that instance's tables and memories, and the <code>Instance</code> struct, and perhaps the stack on non-Windows platforms.</p>\n<p>The major advantage of this is that we can decommit (flash-clear) the whole region with one <code>madvise</code> syscall. In contrast, today, we need a separate <code>madvise</code> for each resource. This cost is somewhat hidden today because we write ~every byte of the <code>Instance</code> and the tables (with eager init) but if we lazily initialize state in tables and/or <code>Instance</code>s (e.g. lazy anyfunc init) this could matter more.</p>\n</blockquote>",
        "id": 270276864,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643746035
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>After some discussion with @alexcrichton just now, the question of why wasmtime's pooling allocator maintains separate pools for memories, tables, and instance structs (and stacks, but those are special on Windows at least) arose.</p>\n<p>Especially if we're considering using <code>madvise</code> to flash-clear state and make instantiation faster, it might make sense to investigate whether we can \"transpose\" the arrangement and put all of an instance's resources in one contiguous region (with appropriate guards of course).</p>\n<p>For example, one instance slot could contain all of that instance's tables and memories, and the <code>Instance</code> struct, and perhaps the stack on non-Windows platforms.</p>\n<p>The major advantage of this is that we can decommit (flash-clear) the whole region with one <code>madvise</code> syscall. In contrast, today, we need a separate <code>madvise</code> for each resource. This cost is somewhat hidden today because we write ~every byte of the <code>Instance</code> and the tables (with eager init) but if we lazily initialize state in tables and/or <code>Instance</code>s (e.g. lazy anyfunc init) this could matter more.</p>\n</blockquote>",
        "id": 270276916,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643746059
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754#issuecomment-1027245009\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>cc @peterhuene, you probably know the reason for separate mmaps here.</p>\n<p>One idea is that we could possibly have less fragmentation by putting everything in one since we don't technically need guards between the instance and the tables (in theory at least), although we do for sure needs guards on stacks and on memories.</p>\n</blockquote>",
        "id": 270277853,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643746433
    },
    {
        "content": "<p>peterhuene <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754#issuecomment-1027256093\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>The primary reason for the disjoint instance resources is that it greatly simplified the <code>uffd</code> implementation as we could just monitor a single range for all linear memories rather than discrete parts of a range representing a single instance (or have to worry about faults from things like tables or stack pages).</p>\n<p>Indeed, the cost was additional <code>madvise</code> calls to make it simpler. Lucet has the design proposed here where all instance resources remain in a continuous slot (and thus a single <code>madvise</code> call), but its uffd handler has to handle faults in pages not meant for linear memory too.</p>\n<p>A single <code>madvise</code> call is definitely preferable and if we're replacing the uffd implementation, I think this makes sense.</p>\n</blockquote>",
        "id": 270279735,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643747195
    },
    {
        "content": "<p>peterhuene edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754#issuecomment-1027256093\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>The primary reason for the disjoint instance resources is that it greatly simplified the <code>uffd</code> implementation as we could just monitor a single range for all linear memories rather than discrete parts of a range representing a single instance's linear memory pages (or, alternatively, monitor pages meant for tables or stacks).</p>\n<p>Indeed, the cost was additional <code>madvise</code> calls to make it simpler. Lucet has the design proposed here where all instance resources remain in a continuous slot (and thus a single <code>madvise</code> call), but its uffd handler has to handle faults in pages not meant for linear memory too.</p>\n<p>A single <code>madvise</code> call is definitely preferable and if we're replacing the uffd implementation, I think this makes sense.</p>\n</blockquote>",
        "id": 270279978,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643747296
    },
    {
        "content": "<p>peterhuene edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754#issuecomment-1027256093\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>The primary reason for the disjoint instance resources is that it greatly simplified the <code>uffd</code> implementation as we could just monitor a single range for all linear memories rather than multiple discrete parts of a range representing an instance's linear memory pages (or, alternatively, monitor pages meant for tables or stacks).</p>\n<p>Indeed, the cost was additional <code>madvise</code> calls to make it simpler. Lucet has the design proposed here where all instance resources remain in a continuous slot (and thus a single <code>madvise</code> call), but its uffd handler has to handle faults in pages not meant for linear memory too.</p>\n<p>A single <code>madvise</code> call is definitely preferable and if we're replacing the uffd implementation, I think this makes sense.</p>\n</blockquote>",
        "id": 270281720,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643747985
    },
    {
        "content": "<p>bjorn3 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754#issuecomment-1027995686\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>Using separate pools would also make it harder to leak pointers (or even cause interpret attacker controlled bytes as a pointer) in case of bugs I would guess.</p>\n</blockquote>",
        "id": 270391894,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643812146
    },
    {
        "content": "<p>bjorn3 edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754#issuecomment-1027995686\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>Using separate pools would also make it harder to leak pointers (or even cause code to interpret attacker controlled bytes as a pointer) in case of bugs I would guess.</p>\n</blockquote>",
        "id": 270391920,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1643812158
    },
    {
        "content": "<p>alexcrichton labeled <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>After some discussion with @alexcrichton just now, the question of why wasmtime's pooling allocator maintains separate pools for memories, tables, and instance structs (and stacks, but those are special on Windows at least) arose.</p>\n<p>Especially if we're considering using <code>madvise</code> to flash-clear state and make instantiation faster, it might make sense to investigate whether we can \"transpose\" the arrangement and put all of an instance's resources in one contiguous region (with appropriate guards of course).</p>\n<p>For example, one instance slot could contain all of that instance's tables and memories, and the <code>Instance</code> struct, and perhaps the stack on non-Windows platforms.</p>\n<p>The major advantage of this is that we can decommit (flash-clear) the whole region with one <code>madvise</code> syscall. In contrast, today, we need a separate <code>madvise</code> for each resource. This cost is somewhat hidden today because we write ~every byte of the <code>Instance</code> and the tables (with eager init) but if we lazily initialize state in tables and/or <code>Instance</code>s (e.g. lazy anyfunc init) this could matter more.</p>\n</blockquote>",
        "id": 276393256,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1648066438
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754#issuecomment-1133041305\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/3754\">issue #3754</a>:</p>\n<blockquote>\n<p>A bit belated but a month or so I did some investigation into this to see what would happen. My findings at the time was that the cost for an <code>madvise</code> call was proportional to the size of the memory that we're <code>madvise</code>-ing, even if that memory wasn't present. My conclusion at the time was that it was better to do the little <code>madvise</code> calls we do today rather than lumping everything into one giant allocation where one <code>madvise</code> call is all that's necessary to blow away.</p>\n<p>I wasn't super scientific in my findings, though. Additionally it was also awhile ago I collected data for this and I probably don't have the branch around any more. Wanted to note though that I don't think this is necessarily a slam dunk and before trying to proceed with this (if ever), we'll want to carefully evaluate performance.</p>\n</blockquote>",
        "id": 283086456,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1653060700
    }
]