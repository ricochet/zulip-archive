[
    {
        "content": "<p><a href=\"https://github.com/jianjunz\">jianjunz</a> added the bug label to <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391\">Issue #8391</a>.</p>",
        "id": 433705966,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1713344154
    },
    {
        "content": "<p>jianjunz opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391\">issue #8391</a>:</p>\n<blockquote>\n<h3>Test Case</h3>\n<p><a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/crates/test-programs/src/bin/nn_image_classification.rs\">nn_image_classification</a>, <a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/crates/test-programs/src/bin/nn_image_classification_named.rs\">nn_image_classification_named</a> and <a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/crates/test-programs/src/bin/nn_image_classification_onnx.rs\">nn_image_classification_onnx</a> have different inference results. The first two cases are based on openvino backend, while the third one is based on onnxruntime backend.</p>\n<h3>Steps to Reproduce</h3>\n<p>Run the cases above locally or check the output of GitHub Actions.</p>\n<p>An example output:<br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/actions/runs/8716252474/job/23909494323#step:17:3914\">https://github.com/bytecodealliance/wasmtime/actions/runs/8716252474/job/23909494323#step:17:3914</a></p>\n<h3>Expected Results</h3>\n<p>Since both of them use mobilenet v2 and the same input tensor data, they should have similar results. We don't expect them to be exactly the same because of different model formats.</p>\n<h3>Actual Results</h3>\n<p>The result of wasi-nn openvino backend is <code>[InferenceResult(963, 0.7113049), InferenceResult(762, 0.07070768), InferenceResult(909, 0.036356032), InferenceResult(926, 0.015456118), InferenceResult(567, 0.015344023)]</code>.</p>\n<p>The result for onnx backend is <code>[InferenceResult(470, 479.08182), InferenceResult(862, 378.7252), InferenceResult(626, 364.8759), InferenceResult(644, 334.28488), InferenceResult(556, 288.65884)]</code>.</p>\n<p>CI for WinML backend is not enabled yet, but it has the same result as onnx backend.</p>\n<h3>Versions and Environment</h3>\n<p>Wasmtime version or commit: 19.0.1</p>\n<p>Operating system: Windows</p>\n<p>Architecture: x86_64</p>\n<h3>Extra Info</h3>\n<p>Although all these tests use mobilenet v2, openvino model requires input data to be BGR format, with mean values: <a href=\"http://[openvino%20model%20description](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/mobilenet-v2-1.0-224)\">127.5, 127.5, 127.5</a>. ONNX model (used by both onnxruntime backend and winml backend) requires input data to be RGB format, in the range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225] (<a href=\"https://github.com/onnx/models/tree/main/validated/vision/classification/mobilenet\">onnx model description</a>).</p>\n<p>I'm not sure if we missed input data preprocessing, or it was processed somewhere else.</p>\n<p>The test case for WinML backend uses a different input at this time. I'm trying to unifying the inputs for all backends so we can double check the correctness.</p>\n</blockquote>",
        "id": 433705968,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1713344155
    },
    {
        "content": "<p>abrown <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391#issuecomment-2064631528\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391\">issue #8391</a>:</p>\n<blockquote>\n<p>cc: @devigned </p>\n</blockquote>",
        "id": 434206433,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1713460742
    },
    {
        "content": "<p>jianjunz <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391#issuecomment-2068591792\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391\">issue #8391</a>:</p>\n<blockquote>\n<p>Wasi-nn example classification-component-onnx has images pre-processed <a href=\"https://github.com/bytecodealliance/wasmtime/blob/4b9f53a928a8721e9b955da1c50090086bbef112/crates/wasi-nn/examples/classification-component-onnx/src/lib.rs#L114-L131\">here</a>. Applying the same process for test inputs should fix this issue for onnxruntime and winml backends.</p>\n</blockquote>",
        "id": 434662794,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1713767571
    },
    {
        "content": "<p>devigned <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391#issuecomment-2069822065\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391\">issue #8391</a>:</p>\n<blockquote>\n<p>Nice catch, @jianjunz! Thank you for opening this issue. </p>\n<p>I'm happy to open a PR for this, but if you are interested, I'd gladly give it a review. Are you interested in contributing?<br>\n</p>\n</blockquote>",
        "id": 434775958,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1713798620
    },
    {
        "content": "<p>devigned edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391#issuecomment-2069822065\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391\">issue #8391</a>:</p>\n<blockquote>\n<p>Nice catch, @jianjunz! Thank you for opening this issue. </p>\n<p>I'm happy to open a PR for this, but if you are interested, I'd gladly give a PR from you a review. Are you interested in contributing?<br>\n</p>\n</blockquote>",
        "id": 434776050,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1713798642
    },
    {
        "content": "<p>jianjunz <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391#issuecomment-2071879255\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391\">issue #8391</a>:</p>\n<blockquote>\n<p>Thanks, David. #8442 is opened for fixing this issue. It also makes ONNX Runtime backend and WinML backend share the same test code because both of them use ONNX models.</p>\n</blockquote>",
        "id": 434946189,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1713865735
    },
    {
        "content": "<p>abrown closed <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8391\">issue #8391</a>:</p>\n<blockquote>\n<h3>Test Case</h3>\n<p><a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/crates/test-programs/src/bin/nn_image_classification.rs\">nn_image_classification</a>, <a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/crates/test-programs/src/bin/nn_image_classification_named.rs\">nn_image_classification_named</a> and <a href=\"https://github.com/bytecodealliance/wasmtime/blob/main/crates/test-programs/src/bin/nn_image_classification_onnx.rs\">nn_image_classification_onnx</a> have different inference results. The first two cases are based on openvino backend, while the third one is based on onnxruntime backend.</p>\n<h3>Steps to Reproduce</h3>\n<p>Run the cases above locally or check the output of GitHub Actions.</p>\n<p>An example output:<br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/actions/runs/8716252474/job/23909494323#step:17:3914\">https://github.com/bytecodealliance/wasmtime/actions/runs/8716252474/job/23909494323#step:17:3914</a></p>\n<h3>Expected Results</h3>\n<p>Since both of them use mobilenet v2 and the same input tensor data, they should have similar results. We don't expect them to be exactly the same because of different model formats.</p>\n<h3>Actual Results</h3>\n<p>The result of wasi-nn openvino backend is <code>[InferenceResult(963, 0.7113049), InferenceResult(762, 0.07070768), InferenceResult(909, 0.036356032), InferenceResult(926, 0.015456118), InferenceResult(567, 0.015344023)]</code>.</p>\n<p>The result for onnx backend is <code>[InferenceResult(470, 479.08182), InferenceResult(862, 378.7252), InferenceResult(626, 364.8759), InferenceResult(644, 334.28488), InferenceResult(556, 288.65884)]</code>.</p>\n<p>CI for WinML backend is not enabled yet, but it has the same result as onnx backend.</p>\n<h3>Versions and Environment</h3>\n<p>Wasmtime version or commit: 19.0.1</p>\n<p>Operating system: Windows</p>\n<p>Architecture: x86_64</p>\n<h3>Extra Info</h3>\n<p>Although all these tests use mobilenet v2, openvino model requires input data to be BGR format, with mean values: <a href=\"http://[openvino%20model%20description](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/mobilenet-v2-1.0-224)\">127.5, 127.5, 127.5</a>. ONNX model (used by both onnxruntime backend and winml backend) requires input data to be RGB format, in the range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225] (<a href=\"https://github.com/onnx/models/tree/main/validated/vision/classification/mobilenet\">onnx model description</a>).</p>\n<p>I'm not sure if we missed input data preprocessing, or it was processed somewhere else.</p>\n<p>The test case for WinML backend uses a different input at this time. I'm trying to unifying the inputs for all backends so we can double check the correctness.</p>\n</blockquote>",
        "id": 444061007,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1718127043
    }
]