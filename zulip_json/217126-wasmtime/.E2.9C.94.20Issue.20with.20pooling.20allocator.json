[
    {
        "content": "<p>Hey all! I'm having a problem the pooling allocator for wasmtime. What it boils down to is that we can never get the pooling allocator to work on a device that has less than 2GB-ish of memory. I tried using the defaults, lowering the defaults even further for all the options and every time it looked like it tried to allocate the same amount of memory, returning the error:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">Error</span>: <span class=\"nc\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">initialize</span><span class=\"w\"> </span><span class=\"n\">host</span>\n\n<span class=\"n\">Caused</span><span class=\"w\"> </span><span class=\"n\">by</span>:\n    <span class=\"mi\">0</span>: <span class=\"nc\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">build</span><span class=\"w\"> </span><span class=\"n\">runtime</span>\n<span class=\"w\">    </span><span class=\"mi\">1</span>: <span class=\"nc\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">construct</span><span class=\"w\"> </span><span class=\"n\">engine</span>\n<span class=\"w\">    </span><span class=\"mi\">2</span>: <span class=\"nc\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">create</span><span class=\"w\"> </span><span class=\"n\">stack</span><span class=\"w\"> </span><span class=\"n\">pool</span><span class=\"w\"> </span><span class=\"n\">mapping</span>\n<span class=\"w\">    </span><span class=\"mi\">3</span>: <span class=\"nc\">mmap</span><span class=\"w\"> </span><span class=\"n\">failed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">allocate</span><span class=\"w\"> </span><span class=\"mh\">0x7d3e8000</span><span class=\"w\"> </span><span class=\"n\">bytes</span>\n<span class=\"w\">    </span><span class=\"mi\">4</span>: <span class=\"nc\">Cannot</span><span class=\"w\"> </span><span class=\"n\">allocate</span><span class=\"w\"> </span><span class=\"n\">memory</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"w\"> </span><span class=\"n\">error</span><span class=\"w\"> </span><span class=\"mi\">12</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>So no matter what I did it tried to allocate 0x7d3e8000 bytes. So my question is: Do any of the options actually help tweak this? And as a follow up, should we just be using the dynamic allocator on small systems? I keep trying to dig into the code, but it is a bit too low level for me and hard to follow what the actual impact on allocated memory is.</p>\n<p>Just to be clear, here is what I've tried:</p>\n<ol>\n<li>Using <code>PoolingAllocationConfig::default()</code></li>\n<li>Lowering the default numbers (see below for snippet) </li>\n<li>Lowering the static memory guard size to 1GB</li>\n</ol>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">pooling_config</span>\n<span class=\"w\">            </span><span class=\"p\">.</span><span class=\"n\">total_component_instances</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n<span class=\"w\">            </span><span class=\"p\">.</span><span class=\"n\">total_memories</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n<span class=\"w\">            </span><span class=\"p\">.</span><span class=\"n\">total_tables</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n<span class=\"w\">            </span><span class=\"p\">.</span><span class=\"n\">linear_memory_keep_resident</span><span class=\"p\">((</span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">MB</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">usize</span><span class=\"p\">)</span>\n<span class=\"w\">            </span><span class=\"p\">.</span><span class=\"n\">table_keep_resident</span><span class=\"p\">((</span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">MB</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"kt\">usize</span><span class=\"p\">);</span>\n</code></pre></div>\n<p>Anyone have any ideas here?</p>",
        "id": 436760557,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714677357
    },
    {
        "content": "<p>Also, it has failed in both the <code>TablePool</code> allocation and the <code>StackPool</code> allocation, depending on the settings</p>",
        "id": 436760825,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714677468
    },
    {
        "content": "<p>assuming this is a Linux machine -- have you tried tweaking the VM overcommit settings?</p>",
        "id": 436762230,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1714678014
    },
    {
        "content": "<p>in general we're pretty free about mmap'ing large regions and they'll be only sparsely populated; the actual RSS should be close to the sum of all instances' heaps, tables, vmcontexts</p>",
        "id": 436762386,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1714678075
    },
    {
        "content": "<p>Yeah the failures have been on linux. Let me try tweaking the overcommit settings</p>",
        "id": 436762592,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714678147
    },
    {
        "content": "<p>That did it</p>",
        "id": 436763196,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714678363
    },
    {
        "content": "<p>Setting it to 1 that is</p>",
        "id": 436763623,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714678525
    },
    {
        "content": "<p>The default heuristic didn't work</p>",
        "id": 436763666,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714678541
    },
    {
        "content": "<p>it's worth doing the math on maximum actual RSS too to make sure the default heuristic wasn't \"onto something\" (i.e. was actually reasonable)</p>",
        "id": 436764517,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1714678874
    },
    {
        "content": "<p>ballpark math can be as simple as number-of-slots * max-heap-size</p>",
        "id": 436764559,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1714678896
    },
    {
        "content": "<p>Yeah default overcommit (on _some_ distros) is to only allow overcommit up to actual physical ram size</p>",
        "id": 436764579,
        "sender_full_name": "Lann Martin",
        "timestamp": 1714678904
    },
    {
        "content": "<p>So the follow up here is why it had issues even when I lowered all of the max number of components and memories. Am I not tweaking it right?</p>",
        "id": 436779787,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714685371
    },
    {
        "content": "<p>the default guard region size is 2GiB, so each linear memory slot the pooling allocator creates will reserve 6GiB of address space</p>",
        "id": 436781440,
        "sender_full_name": "Peter Huene",
        "timestamp": 1714686179
    },
    {
        "content": "<p>Yeah, I knew that the guard region was configurable but that didn't help either without overcommit being changed</p>",
        "id": 436781558,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714686241
    },
    {
        "content": "<p>Would the proper thing be to lower the <code>static_memory_maximum_size</code> to be something like 2GB instead of 4GB?</p>",
        "id": 436781940,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714686395
    },
    {
        "content": "<p>Well, that seemed to do it for me. Lowered memory size to 2GB and guard to 1GB</p>",
        "id": 436783200,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714687080
    },
    {
        "content": "<p>I'm curious about your other knobs, most particularly the number of slots in the pooling allocator -- if an individual module has a max memory size of 2GiB, and you have more than one slot in use, you'll exceed your system's 2GiB physical memory</p>",
        "id": 436783315,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1714687129
    },
    {
        "content": "<p>(overcommit / optimistic underprovisioning is of course a thing, but at least in some contexts one wants to size for worst-case instead)</p>",
        "id": 436783354,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1714687152
    },
    {
        "content": "<p>By slots do you mean \"number of components\" or something else? I figured with an overprovisioning here, I'd assume that it doesn't actually consume all that memory until it is actually used (since it is just in the virtual address space)</p>",
        "id": 436783578,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714687239
    },
    {
        "content": "<p>related, i'm not sure if a guard page size &gt; 0 make sense for a static memory size &lt; 4GB anyway as bounds checks will always be emitted (i think the pooling allocator might still reserve those pages even though they'll never be hit by an out-of-bounds memory access)</p>",
        "id": 436784208,
        "sender_full_name": "Peter Huene",
        "timestamp": 1714687583
    },
    {
        "content": "<p>and by slots, I think it would be <code>total_memories</code> in this case (there's also <code>total_stacks</code> to think about for async)</p>",
        "id": 436784329,
        "sender_full_name": "Peter Huene",
        "timestamp": 1714687661
    },
    {
        "content": "<p>Oh yep you're right:</p>\n<blockquote>\n<p>For 32-bit wasm memories a 4GB static memory is required to even start removing bounds checks.</p>\n</blockquote>",
        "id": 436784467,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714687736
    },
    {
        "content": "<p>but to Chris' point, I'd expect any static memory size &lt; 4GB in a pooling allocator on a device with constrained memory to be closer to a value that one would expect to be able to support 500 concurrently used linear memories for; like 4 MB or something</p>",
        "id": 436784771,
        "sender_full_name": "Peter Huene",
        "timestamp": 1714687931
    },
    {
        "content": "<p>Yep, this is all starting to make some sense. Might document this or write a blog post so there are some more example out there</p>",
        "id": 436784854,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714687990
    },
    {
        "content": "<p>Thanks all for the help here. I think I have a semi-decent grasp on things now</p>",
        "id": 436788885,
        "sender_full_name": "Taylor Thomas",
        "timestamp": 1714690300
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"280224\">Taylor Thomas</span> has marked this topic as resolved.</p>",
        "id": 436788891,
        "sender_full_name": "Notification Bot",
        "timestamp": 1714690303
    }
]