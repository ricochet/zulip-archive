<html>
<head><meta charset="utf-8"><title>wasmtime / issue #2296 Cranelift AArch64: Support the SIM... · git-wasmtime · Zulip Chat Archive</title></head>
<h2>Stream: <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/index.html">git-wasmtime</a></h2>
<h3>Topic: <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html">wasmtime / issue #2296 Cranelift AArch64: Support the SIM...</a></h3>

<hr>

<base href="https://bytecodealliance.zulipchat.com">

<head><link href="https://bytecodealliance.github.io/zulip-archive/style.css" rel="stylesheet"></head>

<a name="255806967"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/255806967" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#255806967">(Oct 01 2021 at 21:00)</a>:</h4>
<p>akirilov-arm labeled <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">issue #2296</a>:</p>
<blockquote>
<p>The bitmask extraction operations (<a href="https://github.com/WebAssembly/simd/issues/201">WebAssembly/simd#201</a>) have already become a <a href="https://github.com/WebAssembly/simd/blob/master/proposals/simd/SIMD.md#bitmask-extraction">part</a> of the fixed-width SIMD proposal, so we should definitely implement them in Cranelift's AArch64 backend (and elsewhere, of course, but that's a separate discussion), so this issue is going to concentrate on implementation approaches, and in particular the instruction sequences that will be used. That is probably going to be the most challenging part because those operations don't have a straightforward mapping into the A64 instruction set.</p>
<p>Let's start with <code>i8x16.bitmask</code>, since it is the most complicated one - the suggested lowering from the proposal is:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">sshr</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vs</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span>#<span class="mi">7</span><span class="w"></span>
<span class="w">  </span><span class="n">ldr</span><span class="w">   </span><span class="n">qt2</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">and</span><span class="w">   </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">ext</span><span class="w">   </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span>#<span class="mi">8</span><span class="w"></span>
<span class="w">  </span><span class="n">zip1</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addv</span><span class="w">  </span><span class="n">ht</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">8</span><span class="n">h</span><span class="w"></span>
<span class="w">  </span><span class="n">umov</span><span class="w">  </span><span class="n">wd</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>where <code>Vs</code> is the input vector register, <code>Wd</code> is the output general-purpose register, and <code>Vt</code> and <code>Vt2</code> - temporary vector registers.</p>
<p>The main issue with this instruction sequence is that it uses the horizontal reduction <code>ADDV</code>, which is quite expensive on some microarchitectures, so here's an alternative:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">cmlt</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vs</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span>#<span class="mi">0</span><span class="w"></span>
<span class="w">  </span><span class="n">ldr</span><span class="w">   </span><span class="n">qt2</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">and</span><span class="w">   </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">umov</span><span class="w">  </span><span class="n">wd</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>We also use <code>CMLT #0</code> instead of <code>SSHR</code>, since it has better throughput in some cases, but, of course, the best approach would be to avoid it completely. AFAIK the expected usage of the bitmask extraction operations is to work on vector comparison results, which are already in the required form (the bits in each lane are either all 0 or 1), so we could avoid the initial instruction by clever IR pattern matching (bare minimum would be <code>maybe_input_insn(ctx, inputs[0], Opcode::Icmp)</code> and so on).</p>
<p>I wrote a microbenchmark for each instruction sequence by putting it in a loop and completing it with a <code>dup vs.16b, wd</code>, which establishes a loop-carried dependency and makes it possible to measure latency. Here are the results for various microarchitectures in terms of speedup achieved by the alternative sequence, i.e. higher numbers mean that the latter is faster (has lower latency):</p>
<table>
<thead>
<tr>
<th>CPU core</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ampere eMAG</td>
<td>114.71%</td>
</tr>
<tr>
<td>Arm Cortex-A53</td>
<td>90.49%</td>
</tr>
<tr>
<td>Arm Cortex-A72</td>
<td>114.35%</td>
</tr>
<tr>
<td>Arm Cortex-A73</td>
<td>112.50%</td>
</tr>
<tr>
<td>Arm Neoverse N1</td>
<td>126.66%</td>
</tr>
</tbody>
</table>
<p>The results are based on median runtimes from 20 runs; standard errors were 0.02% or less, with one exception on the Neoverse N1, which was 1.95% because the very first run took significantly longer than the rest.</p>
<p>Unfortunately, there isn't a clear winner - the alternative instruction sequence is faster on big cores, while the one from the proposal is faster on the little core, i.e. Cortex-A53. However, the speedups achieved on the big cores are greater than the slowdown experienced on the little core.</p>
<p>Future architectural extensions could enable further simplification; in particular, the bit permute instructions that are part of the second version of the Scalable Vector Extension (SVE2) may reduce the number of instructions and avoid the literal load at the same time.</p>
<p>cc @julian-seward1</p>
</blockquote>



<a name="281231499"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/281231499" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#281231499">(May 04 2022 at 22:54)</a>:</h4>
<p>cfallin <a href="https://github.com/bytecodealliance/wasmtime/issues/2296#issuecomment-1118006472">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">issue #2296</a>:</p>
<blockquote>
<p>We pass all SIMD tests on aarch64, so I believe these operations should be supported; closing issue now.</p>
</blockquote>



<a name="281231500"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/281231500" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#281231500">(May 04 2022 at 22:54)</a>:</h4>
<p>cfallin closed <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">issue #2296</a>:</p>
<blockquote>
<p>The bitmask extraction operations (<a href="https://github.com/WebAssembly/simd/issues/201">WebAssembly/simd#201</a>) have already become a <a href="https://github.com/WebAssembly/simd/blob/master/proposals/simd/SIMD.md#bitmask-extraction">part</a> of the fixed-width SIMD proposal, so we should definitely implement them in Cranelift's AArch64 backend (and elsewhere, of course, but that's a separate discussion), so this issue is going to concentrate on implementation approaches, and in particular the instruction sequences that will be used. That is probably going to be the most challenging part because those operations don't have a straightforward mapping into the A64 instruction set.</p>
<p>Let's start with <code>i8x16.bitmask</code>, since it is the most complicated one - the suggested lowering from the proposal is:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">sshr</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vs</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span>#<span class="mi">7</span><span class="w"></span>
<span class="w">  </span><span class="n">ldr</span><span class="w">   </span><span class="n">qt2</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">and</span><span class="w">   </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">ext</span><span class="w">   </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span>#<span class="mi">8</span><span class="w"></span>
<span class="w">  </span><span class="n">zip1</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addv</span><span class="w">  </span><span class="n">ht</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">8</span><span class="n">h</span><span class="w"></span>
<span class="w">  </span><span class="n">umov</span><span class="w">  </span><span class="n">wd</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>where <code>Vs</code> is the input vector register, <code>Wd</code> is the output general-purpose register, and <code>Vt</code> and <code>Vt2</code> - temporary vector registers.</p>
<p>The main issue with this instruction sequence is that it uses the horizontal reduction <code>ADDV</code>, which is quite expensive on some microarchitectures, so here's an alternative:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">cmlt</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vs</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span>#<span class="mi">0</span><span class="w"></span>
<span class="w">  </span><span class="n">ldr</span><span class="w">   </span><span class="n">qt2</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">and</span><span class="w">   </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">umov</span><span class="w">  </span><span class="n">wd</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>We also use <code>CMLT #0</code> instead of <code>SSHR</code>, since it has better throughput in some cases, but, of course, the best approach would be to avoid it completely. AFAIK the expected usage of the bitmask extraction operations is to work on vector comparison results, which are already in the required form (the bits in each lane are either all 0 or 1), so we could avoid the initial instruction by clever IR pattern matching (bare minimum would be <code>maybe_input_insn(ctx, inputs[0], Opcode::Icmp)</code> and so on).</p>
<p>I wrote a microbenchmark for each instruction sequence by putting it in a loop and completing it with a <code>dup vs.16b, wd</code>, which establishes a loop-carried dependency and makes it possible to measure latency. Here are the results for various microarchitectures in terms of speedup achieved by the alternative sequence, i.e. higher numbers mean that the latter is faster (has lower latency):</p>
<table>
<thead>
<tr>
<th>CPU core</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ampere eMAG</td>
<td>114.71%</td>
</tr>
<tr>
<td>Arm Cortex-A53</td>
<td>90.49%</td>
</tr>
<tr>
<td>Arm Cortex-A72</td>
<td>114.35%</td>
</tr>
<tr>
<td>Arm Cortex-A73</td>
<td>112.50%</td>
</tr>
<tr>
<td>Arm Neoverse N1</td>
<td>126.66%</td>
</tr>
</tbody>
</table>
<p>The results are based on median runtimes from 20 runs; standard errors were 0.02% or less, with one exception on the Neoverse N1, which was 1.95% because the very first run took significantly longer than the rest.</p>
<p>Unfortunately, there isn't a clear winner - the alternative instruction sequence is faster on big cores, while the one from the proposal is faster on the little core, i.e. Cortex-A53. However, the speedups achieved on the big cores are greater than the slowdown experienced on the little core.</p>
<p>Future architectural extensions could enable further simplification; in particular, the bit permute instructions that are part of the second version of the Scalable Vector Extension (SVE2) may reduce the number of instructions and avoid the literal load at the same time.</p>
<p>cc @julian-seward1</p>
</blockquote>



<hr><p>Last updated: Jan 20 2025 at 06:04 UTC</p>
</html>