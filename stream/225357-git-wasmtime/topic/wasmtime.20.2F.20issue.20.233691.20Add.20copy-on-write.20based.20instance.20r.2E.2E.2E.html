<html>
<head><meta charset="utf-8"><title>wasmtime / issue #3691 Add copy-on-write based instance r... · git-wasmtime · Zulip Chat Archive</title></head>
<h2>Stream: <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/index.html">git-wasmtime</a></h2>
<h3>Topic: <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html">wasmtime / issue #3691 Add copy-on-write based instance r...</a></h3>

<hr>

<base href="https://bytecodealliance.zulipchat.com">

<head><link href="https://bytecodealliance.github.io/zulip-archive/style.css" rel="stylesheet"></head>

<a name="268002498"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/268002498" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#268002498">(Jan 14 2022 at 11:38)</a>:</h4>
<p>github-actions[bot] <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1013045420">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<h4>Subscribe to Label Action</h4>
<p>cc @peterhuene</p>
<p>&lt;details&gt;<br>
This issue or pull request has been labeled: "wasmtime:api"</p>
<p>Thus the following users have been cc'd because of the following labels:</p>
<ul>
<li>peterhuene: wasmtime:api</li>
</ul>
<p>To subscribe or unsubscribe from this label, edit the &lt;code&gt;.github/subscribe-to-label.json&lt;/code&gt; configuration file.</p>
<p><a href="https://github.com/bytecodealliance/subscribe-to-label-action">Learn more.</a><br>
&lt;/details&gt;</p>
</blockquote>



<a name="268379907"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/268379907" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#268379907">(Jan 18 2022 at 11:58)</a>:</h4>
<p>koute <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1015341813">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>It looks like <code>qemu</code>'s user mode emulator is not emulating <code>madvise</code> properly and that's why the tests are failing on aarch64; I've just checked and on a bare metal aarch64 system they all pass.</p>
<p>I guess ideally we should disable them when running under qemu-user?</p>
</blockquote>



<a name="268421607"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/268421607" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#268421607">(Jan 18 2022 at 16:53)</a>:</h4>
<p>cfallin <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1015614274">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>Hi @koute -- thanks so much for this PR and for bringing up the ideas behind it (in particular, the memfd mechanism)!</p>
<p>Guilty admission on my part: after you mentioned memfd recently on Zulip, and madvise to reset a private mapping (throw away a CoW overlay), I threw together my own implementation as well and did a lot of internal experimentation. (I've been hacking in the pooling allocator and on performance-related things recently as well and your idea was a huge epiphany for me.) I need to clean it up a bit still but will put it up soon (with due credit to you for memfd/madvise/CoW ideas!). Perhaps we can get the best ideas out of both of these PRs :-)</p>
<p>One additional realization I had was that, for performance, we don't want to do any <code>mprotect()</code> at all on heap growth (it holds the whole-process mmap lock in write mode, so can quickly become a bottleneck). But we can play tricks with ftruncate and a <em>second</em> anonymous memfd, mapped above the initial image, so that in a steady state with instance-slot reuse on reinstantiation, our only syscalls are madvise and ftruncate. (Key bits: mmap can map beyond the actual size of a file; size of a file can be changed after mapping is made, and this doesn't touch any whole-process locks; accesses beyond end of file cause SIGBUS.)</p>
<p>Anyway, a few thoughts on this PR:</p>
<ul>
<li>
<p>The first-class notion of snapshotting, saving globals, etc., is interesting, and I can see how it could be useful in certain scenarios. However I think it actually mixes a few ideas which we probably want to separate and design independently: the notion of instance state, snapshotting, rewinding, etc., and the implementation-level mechanism of CoW-mapping a heap image.</p>
<p>In particular, at least in scenarios I'm more familiar with, we do snapshotting via a separate tool that writes out a Wasm module post-snapshot, <a href="https://github.com/bytecodealliance/wizer">Wizer</a>. IMHO, including a runtime snapshot-and-reuse-of-state-in-memory mechanism muddies the waters a bit -- which should be used when? -- and whether or not Wasmtime should have such an API deserves further discussion, I think.</p>
</li>
<li>
<p>Given that, I think something that we could more easily land would be a completely API-transparent extension of the pooling allocator. Basically a new mode that (i) creates a backing image for a module's memories when it's first seen, (ii) CoW-maps these images into memory slots, with the ftruncate-to-extend trick above, and (iii) tries to reuse slots with existing mappings of the desired images when possible. This is basically what I've put together, so I'll try to get it up ASAP.</p>
</li>
<li>
<p>I'm a bit concerned with the need to read <code>/proc/self/pagemap</code> to determine whether pages are present. This has some interesting permissions implications (see <a href="https://www.kernel.org/doc/Documentation/vm/pagemap.txt">Documentation/vm/pagemap.txt</a> in the Linux kernel) -- some bits need root or a process capability to see, and it seems that under some kernel versions the whole file is inaccessible except by root. In any case, this creates a deep coupling with a Linux-specific interface for snapshotting; ideally, such API functionality should be designed, if it exists, so it is not forever limited to be a Linux-specific feature. (In other words: performance-only features can be OS-specific, because that's the nature of optimizing for a platform; but functionality should not be OS-specific.)</p>
<p>I think though that my concern here is basically subsumed by the higher-level question of where snapshotting should occur: if it is the domain of a separate tool, then that separate tool can do special things that may not be as efficient (e.g., diff a whole memory, or mmap inaccessible and mprotect() in pages one at a time), since snapshotting is probably rare compared to instantiation.</p>
</li>
</ul>
<p>I think these are some things we should talk through after I've put up my PR and we can do comparisons. I'm really grateful for the ton of effort you put into this and look forward to comparing the approaches in more detail!<br>
</p>
</blockquote>



<a name="268431564"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/268431564" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#268431564">(Jan 18 2022 at 17:59)</a>:</h4>
<p>fitzgen <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1015671799">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>Agreed with @cfallin that we should disentangle snapshots and instantiation here, and focus on a relatively transparent extension of the pooling instance allocator for now.</p>
<p>That said, a snapshotting feature could be very useful for doing neat things like <code>rr</code>-style record and replay debugging. But this is a pretty big design space, and I'd want to hash out our motivation/use cases, technical architecture, and API design with an RFC before we dive head first into implementation.</p>
</blockquote>



<a name="268436458"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/268436458" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#268436458">(Jan 18 2022 at 18:37)</a>:</h4>
<p>alexcrichton <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1015701556">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>@koute would using the pooling instance allocator work for your embedding's use case? @cfallin's work right now I believe is entirely focused on that which means that by-default Wasmtime wouldn't have copy-on-write re-instantiation because Wasmtime by default (as you've seen and modified here) uses the on-demand instance allocator. If your embedding doesn't work well with the pooling instance allocator then I think we'll need to brainstorm a solution which "merges" your work here with @cfallin's on the pooling allocator, taking into account the feedback around snapshots (which I personally agree is best to separate and ideally make the copy-on-write business a simple config option of "go faster")</p>
</blockquote>



<a name="268496601"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/268496601" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#268496601">(Jan 19 2022 at 06:20)</a>:</h4>
<p>koute <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1016121824">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<blockquote>
<p>I'm a bit concerned with the need to read <code>/proc/self/pagemap</code> to determine whether pages are present.</p>
</blockquote>
<p>This it technically optional and done entirely for performance, so it could be made allowed to fail. Even without using <code>/proc/self/pagemap</code> my instance reuse mechanism is faster than anything that's currently in <code>wasmtime</code> (in our benchmarks), <em>however</em> it's not always faster than what we're currently using without the <code>/proc/self/pagemap</code> part. (And ideally we don't want to switch to anything that's slower; we want to improve performance.)</p>
<blockquote>
<p>This has some interesting permissions implications (see Documentation/vm/pagemap.txt in the Linux kernel) -- some bits need root or a process capability to see, and it seems that under some kernel versions the whole file is inaccessible except by root.</p>
</blockquote>
<p>That is, AFAIK, only applicable to the lower bits, which we don't need in this case. The higher bits (which we need) should be always readable when reading our own process' pagemap.</p>
<blockquote>
<p>The first-class notion of snapshotting, saving globals, etc., is interesting, and I can see how it could be useful in certain scenarios. However I think it actually mixes a few ideas which we probably want to separate and design independently: the notion of instance state, snapshotting, rewinding, etc., and the implementation-level mechanism of CoW-mapping a heap image.</p>
</blockquote>
<blockquote>
<p>If your embedding doesn't work well with the pooling instance allocator then I think we'll need to brainstorm a solution which "merges" your work here with @cfallin's on the pooling allocator, taking into account the feedback around snapshots (which I personally agree is best to separate and ideally make the copy-on-write business a simple config option of "go faster")</p>
</blockquote>
<p>In my initial prototype implementation I actually tried to do this in the same vein as the current pooling allocator, but in the end decided to go with the current approach. Let me explain.</p>
<p>Basically we have three main requirements:</p>
<ol>
<li>Be as fast as possible. (At least as fast as what we currently use, which is the <code>legacy_instance_reuse</code> you see on the graphs.)</li>
<li>Be robust. (The instantiation can <em>not</em> fail.)</li>
<li>Be simple to use and maintainable. (Although we can live with this not being the case if absolutely necessary.)</li>
</ol>
<p>One of the problems with the current pooling allocator (ignoring how it performs) is that it fails at (2) and somewhat at (3), and isn't a simple "go faster" option that you can just blindly toggle. You have to manually specify module and instance limits (and if the WASM blob changes too significantly you need to modify them), <em>and</em> you need to maintain a separate codepath (basically keep another separate <code>Engine</code>s and the same <code>Module</code> twice) for the case when instantiation fails.</p>
<p>So personally I think it just makes more sense (especially for something so fundamental and low level as <code>wasmtime</code>) to just let the user explicitly do the pooling themselves, which is very easy to do with the approach in this PR - just preinstantiate as many instances as you want into a <code>Vec</code>, pop one on use, push them back when you're done, and if you run out of pooled instances you can just easily instantiate one from scratch with a single extra <code>if</code>.</p>
<p>I also considered integrating this into <code>InstancePre</code> directly (or into a separate type) - that is, there would be no <code>reset</code>, and the instances would automatically reset themselves when dropped and returned into a pool inside of the <code>InstancePre</code> and then automatically and transparently reused when <code>InstancePre::instantiate</code> would be called (so <em>that</em> would actually be a transparent "go faster" option), but decided that it'd be better to just let the user control the pooling themselves since it's so simple to do <em>anyway</em>. (And also the way how everything is stored inside of the <code>Store</code> and not the <code>Instance</code> would make that somewhat awkward, since you want to reuse <em>both</em> the <code>Instance</code> and the <code>Store</code>.)</p>
<p>Basically, in our usecase we don't really care about snapshotting at all (it's just an implementation detail to make things go fast), and all we just need is to be able to instantiate clean instances as fast as possible.</p>
<p>Would this be more acceptable to you if API-wise we'd make it less like it's snapshotting and more like an explicit way to pool instances?</p>
<blockquote>
<p>I think these are some things we should talk through after I've put up my PR and we can do comparisons. I'm really grateful for the ton of effort you put into this and look forward to comparing the approaches in more detail!</p>
</blockquote>
<p>Sounds good to me! I'll hook up your PR into our benchmarks so that we can compare the performance.<br>
</p>
</blockquote>



<a name="271306906"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/271306906" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#271306906">(Feb 09 2022 at 16:40)</a>:</h4>
<p>alexcrichton <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1033963213">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>Ok @koute so to follow up on comments and work from before, <a href="https://github.com/bytecodealliance/wasmtime/pull/3733">https://github.com/bytecodealliance/wasmtime/pull/3733</a> is the final major optimization for instantiation that we know of to implement. There's probably some very minor wins still remaining, but that's the lion's share of improvements that we're going to get into wasmtime (that plus memfd).</p>
<p>Could you try re-running your benchmark numbers with the old strategy y'all are currently using, your proposal in this PR, and then <a href="https://github.com/bytecodealliance/wasmtime/pull/3733">https://github.com/bytecodealliance/wasmtime/pull/3733</a> as a PR? Note that when using <a href="https://github.com/bytecodealliance/wasmtime/pull/3733">https://github.com/bytecodealliance/wasmtime/pull/3733</a> using the on-demand allocator, while maybe a little bit slower than the pooling allocator, should still be fine. The intention with <a href="https://github.com/bytecodealliance/wasmtime/pull/3733">https://github.com/bytecodealliance/wasmtime/pull/3733</a> is that it's fast enough that you won't need to maintain a pool of instances, and each "reuse" of an instance can perform the full re-instantiation process. Note that for re-instantiation it's recommended to start from an <code>InstancePre&lt;T&gt;</code> which is the fastest way today to instantiate something.</p>
<p>My prediction is that the time-to-instantiate <a href="https://github.com/bytecodealliance/wasmtime/pull/3733">https://github.com/bytecodealliance/wasmtime/pull/3733</a> is likely quite close to the strategy outlined in this PR. It will probably look a little different one way or another, but that's what I'm curious to see if <a href="https://github.com/bytecodealliance/wasmtime/pull/3733">https://github.com/bytecodealliance/wasmtime/pull/3733</a> works for your use case in terms of robustness and performance. </p>
<p>If you're up for it then it might be interesting to test the pooling allocator as well. I realize that the pooling allocator as-is isn't a great fit for your use case due to it being too constrained, but as a one-off measurement of numbers it might help give an idea of the performance tradeoff between the pooling allocator and the on-demand allocator. Also unless you're specifically interested in concurrent instantiation performance it's fine to only get single-threaded instantiation numbers. It's expected that <a href="https://github.com/bytecodealliance/wasmtime/pull/3733">https://github.com/bytecodealliance/wasmtime/pull/3733</a> does not scale well with cores (like this PR) due to the IPIs necessary at the kernel level with all the calls to <code>madvise</code>. In that sense the more interesting comparison is probably single-threaded numbers (again unless you're specifically interested in the multi-threaded numbers as well)</p>
</blockquote>



<a name="271307163"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/271307163" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#271307163">(Feb 09 2022 at 16:41)</a>:</h4>
<p>alexcrichton <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1033964677">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>Oh and for now memfd is disabled-by-default, so with #3733 you'll need to execute <code>config.memfd(true)</code> as part of Wasmtime's configuration to be sure that it's enabled. If you're seeing instantiation take more than double-digit microseconds then Wasmtime may not be configured correctly and I can help dig in. It's expected, though, that instantiation is probably in the single-digit microseconds.</p>
</blockquote>



<a name="271563358"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/271563358" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#271563358">(Feb 11 2022 at 11:31)</a>:</h4>
<p>koute <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1036109655">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>@alexcrichton Got it! I'll rerun all of the benchmarks next week and I'll get back to you.</p>
</blockquote>



<a name="272401747"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/272401747" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#272401747">(Feb 18 2022 at 13:33)</a>:</h4>
<p>koute <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1044534500">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>@alexcrichton Sorry for the delay! I updated to the newest <code>main</code> of <code>wasmtime</code>, ported over this PR and reran the benchmarks. Here are the results:</p>
<p>![call_empty_function](<a href="https://user-images.githubusercontent.com/246574/154690173-ada9148f-700c-4913-9eee-503e64ed126a.png">https://user-images.githubusercontent.com/246574/154690173-ada9148f-700c-4913-9eee-503e64ed126a.png</a>)<br>
![dirty_1mb_of_memory](<a href="https://user-images.githubusercontent.com/246574/154690186-99911c50-4eb1-4d1e-aa84-b61d709c9a3a.png">https://user-images.githubusercontent.com/246574/154690186-99911c50-4eb1-4d1e-aa84-b61d709c9a3a.png</a>)</p>
<p>In table form:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="n">call_empty_function</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">legacy_instance_reuse</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">native_instance_reuse</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_memfd_only</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_pooling_memfd</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_pooling_only</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_pooling_uffd</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_vanilla</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">|---------|-----------------------|-----------------------|------------------------------|---------------------------------|--------------------------------|--------------------------------|---------------------------|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w">       </span><span class="o">|</span><span class="w"> </span><span class="mi">48</span><span class="w">                    </span><span class="o">|</span><span class="w"> </span><span class="mi">4</span><span class="w">                     </span><span class="o">|</span><span class="w"> </span><span class="mi">17</span><span class="w">                           </span><span class="o">|</span><span class="w"> </span><span class="mi">8</span><span class="w">                               </span><span class="o">|</span><span class="w"> </span><span class="mi">58</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">25</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">65</span><span class="w">                        </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">2</span><span class="w">       </span><span class="o">|</span><span class="w"> </span><span class="mi">67</span><span class="w">                    </span><span class="o">|</span><span class="w"> </span><span class="mi">11</span><span class="w">                    </span><span class="o">|</span><span class="w"> </span><span class="mi">43</span><span class="w">                           </span><span class="o">|</span><span class="w"> </span><span class="mi">21</span><span class="w">                              </span><span class="o">|</span><span class="w"> </span><span class="mi">88</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">36</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">109</span><span class="w">                       </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">4</span><span class="w">       </span><span class="o">|</span><span class="w"> </span><span class="mi">91</span><span class="w">                    </span><span class="o">|</span><span class="w"> </span><span class="mi">17</span><span class="w">                    </span><span class="o">|</span><span class="w"> </span><span class="mi">98</span><span class="w">                           </span><span class="o">|</span><span class="w"> </span><span class="mi">31</span><span class="w">                              </span><span class="o">|</span><span class="w"> </span><span class="mi">156</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">60</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">195</span><span class="w">                       </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">8</span><span class="w">       </span><span class="o">|</span><span class="w"> </span><span class="mi">160</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">25</span><span class="w">                    </span><span class="o">|</span><span class="w"> </span><span class="mi">305</span><span class="w">                          </span><span class="o">|</span><span class="w"> </span><span class="mi">45</span><span class="w">                              </span><span class="o">|</span><span class="w"> </span><span class="mi">335</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">135</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">434</span><span class="w">                       </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">16</span><span class="w">      </span><span class="o">|</span><span class="w"> </span><span class="mi">228</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">35</span><span class="w">                    </span><span class="o">|</span><span class="w"> </span><span class="mi">634</span><span class="w">                          </span><span class="o">|</span><span class="w"> </span><span class="mi">64</span><span class="w">                              </span><span class="o">|</span><span class="w"> </span><span class="mi">847</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">271</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">1140</span><span class="w">                      </span><span class="o">|</span><span class="w"></span>

<span class="n">dirty_1mb_of_memory</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">legacy_instance_reuse</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">native_instance_reuse</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_memfd_only</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_pooling_memfd</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_pooling_only</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_pooling_uffd</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">recreate_instance_vanilla</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="o">|---------|-----------------------|-----------------------|------------------------------|---------------------------------|--------------------------------|--------------------------------|---------------------------|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w">       </span><span class="o">|</span><span class="w"> </span><span class="mi">185</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">145</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">159</span><span class="w">                          </span><span class="o">|</span><span class="w"> </span><span class="mi">150</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">196</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">293</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">204</span><span class="w">                       </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">2</span><span class="w">       </span><span class="o">|</span><span class="w"> </span><span class="mi">273</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">212</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">274</span><span class="w">                          </span><span class="o">|</span><span class="w"> </span><span class="mi">219</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">289</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">835</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">333</span><span class="w">                       </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">4</span><span class="w">       </span><span class="o">|</span><span class="w"> </span><span class="mi">374</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">310</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">494</span><span class="w">                          </span><span class="o">|</span><span class="w"> </span><span class="mi">311</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">499</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">1305</span><span class="w">                           </span><span class="o">|</span><span class="w"> </span><span class="mi">553</span><span class="w">                       </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">8</span><span class="w">       </span><span class="o">|</span><span class="w"> </span><span class="mi">738</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">520</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">967</span><span class="w">                          </span><span class="o">|</span><span class="w"> </span><span class="mi">531</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">919</span><span class="w">                            </span><span class="o">|</span><span class="w"> </span><span class="mi">1913</span><span class="w">                           </span><span class="o">|</span><span class="w"> </span><span class="mi">1074</span><span class="w">                      </span><span class="o">|</span><span class="w"></span>
<span class="o">|</span><span class="w"> </span><span class="mi">16</span><span class="w">      </span><span class="o">|</span><span class="w"> </span><span class="mi">945</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">734</span><span class="w">                   </span><span class="o">|</span><span class="w"> </span><span class="mi">2233</span><span class="w">                         </span><span class="o">|</span><span class="w"> </span><span class="mi">767</span><span class="w">                             </span><span class="o">|</span><span class="w"> </span><span class="mi">1808</span><span class="w">                           </span><span class="o">|</span><span class="w"> </span><span class="mi">3122</span><span class="w">                           </span><span class="o">|</span><span class="w"> </span><span class="mi">1985</span><span class="w">                      </span><span class="o">|</span><span class="w"></span>
</code></pre></div>
<p>When a lot of memory is dirtied it is indeed competitive now, and when not a lot of memory is touched it also performs quite well now! Of course this is assuming <em>both</em> memfd and pooling is enabled.</p>
<p>So I'd like to ask here - are there any plans to make the pooling less painful to use? Something like this would be ideal:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="n">config</span><span class="p">.</span><span class="n">allocation_strategy</span><span class="p">(</span><span class="n">InstanceAllocationStrategy</span>::<span class="n">Pooling</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">strategy</span>: <span class="nb">Default</span>::<span class="n">default</span><span class="p">(),</span><span class="w"></span>
<span class="w">    </span><span class="n">instance_limit</span>: <span class="mi">8</span><span class="w"></span>
<span class="p">});</span><span class="w"></span>
</code></pre></div>
<p>Basically completely get rid of the <code>module_limits</code> and have it work with modules of any size, and change the <code>instance_limit</code> to be a <em>soft</em> cap instead of a hard cap, that is - instead of returning an error when the maximum concurrent instance limit is reached it would just transparently create a new <code>OnDemand</code> instance. That would make the pooling strategy truly a drop-in replacement without having to hack around it.</p>
</blockquote>



<a name="272817747"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/272817747" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#272817747">(Feb 22 2022 at 15:24)</a>:</h4>
<p>alexcrichton <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1047908534">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>Hm so actually the "only memfd" line, which I'm assuming is using the default on-demand allocator, is performing much worse than expected. The cost of the on-demand allocator is an extra mmap-or-two and it's not quite as efficient on reuse (a few <code>mmap</code> calls instead of one <code>madvise</code>). In that sense I wouldn't expect it to be an almost order of magnitude slower in the measurements, instead what I've been seeing locally is that it's off by some constant factor ish from the pooling allocator. Do you have some code I could read to double-check the on-demand allocator was configured correctly?</p>
<p>Otherwise though I definitely think we can improve the story with the usability of the pooling allocator. The reason that the module limits and such exist are so that we can create appropriate space for the internal <code>*mut VMContext</code> allocation for each instance which is sized to each instance. I think, though, instead of specifically configuring each field and its limits it would be easier to say "each instance gets N bytes of memory" and then during instantiation if that's exceeded instantiation fails with a descriptive error message. That would mean that a generous amount, such as a megabyte or two, could be allocated for instances and you wouldn't have to worry about tweaking specific limits.</p>
<p>Again though I'm surprised that the on-demand allocator is performing as bad as it did in your measurements, as my suspicion was that it would be sufficient for your use case. I think configuring the pooling allocator by allocation size is probably good to do no matter what, though.</p>
</blockquote>



<a name="272833613"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/272833613" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#272833613">(Feb 22 2022 at 17:01)</a>:</h4>
<p>alexcrichton <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1048012116">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>Actually thinking about this some more, my measurements and impression about the relative cost of these is primarily in the single-threaded case, I haven't looked too too closely at the multithreaded bits. Does your use case stress the multi-threaded aspect heavily? If so we can try to dig in some more, but I'm not sure if you're measuring the multi-threaded performance at the behest of an old request of ours or your own project's motivations as well.</p>
<p>As a point of comparison for a 16-threaded scenario I get:</p>
<table>
<thead>
<tr>
<th></th>
<th>on-demand</th>
<th>pooling</th>
</tr>
</thead>
<tbody>
<tr>
<td>instantiate</td>
<td>126us</td>
<td>660us</td>
</tr>
<tr>
<td><code>empty</code></td>
<td>327us</td>
<td>690us</td>
</tr>
<tr>
<td><code>dirty</code></td>
<td>3025us</td>
<td>1691us</td>
</tr>
</tbody>
</table>
<p>(hence some of my surprise, but again I haven't dug into these numbers much myself, especially the discrepancy between pooling/on-demand and how the speedup changes depending on the allocation strategy)</p>
</blockquote>



<a name="272925027"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/272925027" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#272925027">(Feb 23 2022 at 09:46)</a>:</h4>
<p>koute <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1048604419">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<blockquote>
<p>Do you have some code I could read to double-check the on-demand allocator was configured correctly?</p>
</blockquote>
<p>Sure.</p>
<p>This is the crate where we encapsulated our use of <code>wasmtime</code>: (I'm linking to a fork which I used for benchmarking since it has some extra changes which haven't yet landed in production; please excuse the somewhat messy code in places.)</p>
<p><a href="https://github.com/koute/substrate/tree/master_wasmtime_benchmarks_2/client/executor/wasmtime">https://github.com/koute/substrate/tree/master_wasmtime_benchmarks_2/client/executor/wasmtime</a></p>
<p>Let me give you a quick step-by-step walkthrough of the code:</p>
<ol>
<li><a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L331">The engine's configured.</a></li>
<li><a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L387">The pooling's configured. (optional)</a></li>
<li><a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L622">The Engine is created.</a></li>
<li><a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L634">Any potential memory imports are converted into memory exports.</a> (Since we don't really need to support imported memories, and last time I tried there was big performance difference where the pooling allocator got a lot slower when using imported memories; maybe that's why you're seeing the difference here with on-demand vs pooling?)</li>
<li><a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L644">The Module is created.</a></li>
<li><a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L680">The Linker is created.</a></li>
<li><a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L681">Any function imports are registered within the Linker.</a></li>
<li><a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L684">We create an InstancePre.</a> From now we'll exclusively use this <code>InstancePre</code> to instantiate new instances.</li>
<li>Then on each instantiation:<br>
  a. <a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/instance_wrapper.rs#L188">We create a new Store.</a> (Since <code>Store</code>s can't be reused.)<br>
  b. <a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/instance_wrapper.rs#L192">We instantiate a new instance through <code>InstancePre</code>.</a><br>
  c. <a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/instance_wrapper.rs#L68">We call a single function inside of the WASM blob.</a><br>
  d. <a href="https://github.com/koute/substrate/blob/master_wasmtime_benchmarks_2/client/executor/wasmtime/src/runtime.rs#L256">Both the Instance and the Store are then dropped.</a></li>
</ol>
<p>And my benchmarks basically just loop (9) over and over again.</p>
<blockquote>
<p>Does your use case stress the multi-threaded aspect heavily?</p>
</blockquote>
<p>In certain cases we do call into WASM from multiple threads at the same time, so we do care about the multi-threaded aspect, but an instantiated instance never leaves the thread on which the instantiation happened. (Basically the whole number 9 is always executed in one go without the concrete instance being sent to any other thread nor being called twice. [Unless our legacy instance reuse mechanism is used, but we want to get rid of that.])</p>
</blockquote>



<a name="272926693"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/272926693" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#272926693">(Feb 23 2022 at 10:00)</a>:</h4>
<p>koute <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1048615795">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>Okay, so since the current memfd + pooling allocation strategy is fast enough (our primary goal was to remove our legacy instantiation scheme without compromising on performance, and ideally to also get a speedup if possible) I'm going to close this PR now.</p>
<p>As I've said previously I'm not exactly thrilled by the API to be able to use the pooling allocator without introducing arbitrary hard limits, but ultimately that's not a dealbreaker and we can hack around it. (:</p>
<p>(I'm of course still happy to answer questions and help out if necessary, so please feel free to ping me if needed.)</p>
</blockquote>



<a name="272963423"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/272963423" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#272963423">(Feb 23 2022 at 15:26)</a>:</h4>
<p>alexcrichton <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1048900578">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<p>Ok cool thanks for the links, it looks like nothing is amiss there. I also forget  that the machine I'm working on is an 80-core arm64 machine where IPIs are likely more expensive than smaller-core-count machines, so that would likely help explain the discrepancy. </p>
<p>If it helps I put up <a href="https://github.com/bytecodealliance/wasmtime/pull/3837">https://github.com/bytecodealliance/wasmtime/pull/3837</a> which removes <code>ModuleLimits</code> and folds a few things into the <code>InstanceLimits</code> structure. You'll probably want <code>tables</code> and <code>memories</code> set to 1 (since you're only supporting MVP wasm anyway), <code>memory_pages</code> set to maximum 65536 (as it's just maximally allowed memory, not committed memory). You'll need to manually configure <code>size</code> and <code>table_elements</code> though for your desired maximums. Increasing them will increase the committed memory of the pooling allocator (well, mmapped-as-zero I guess and it's not actually committed until it's accessed).</p>
<p>That hopefully makes things a bit more usable!</p>
<p>Oh also for imported memories, it's true that right now for copy-on-write-based initialization we do not apply the optimization to imported memories, only to locally defined memories. All of the "interesting" modules we've been optimizing and care about the runtime for all define their own memory and export it, but it's not impossible to support imported memories and if you've got a use case we can look to implement support for imported memories. </p>
</blockquote>



<a name="273051963"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20issue%20%233691%20Add%20copy-on-write%20based%20instance%20r.../near/273051963" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20issue.20.233691.20Add.20copy-on-write.20based.20instance.20r.2E.2E.2E.html#273051963">(Feb 24 2022 at 06:20)</a>:</h4>
<p>koute <a href="https://github.com/bytecodealliance/wasmtime/pull/3691#issuecomment-1049532123">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/pull/3691">issue #3691</a>:</p>
<blockquote>
<blockquote>
<p>Ok cool thanks for the links, it looks like nothing is amiss there. I also forget that the machine I'm working on is an 80-core arm64 machine where IPIs are likely more expensive than smaller-core-count machines, so that would likely help explain the discrepancy.</p>
</blockquote>
<p>That could potentially affect things, yes; I was testing this on a mere 32-core machine after all. (:</p>
<blockquote>
<p>If it helps I put up #3837 which removes <code>ModuleLimits</code> and folds a few things into the <code>InstanceLimits</code> structure. You'll probably want <code>tables</code> and <code>memories</code> set to 1 (since you're only supporting MVP wasm anyway), <code>memory_pages</code> set to maximum 65536 (as it's just maximally allowed memory, not committed memory). You'll need to manually configure <code>size</code> and <code>table_elements</code> though for your desired maximums. Increasing them will increase the committed memory of the pooling allocator (well, mmapped-as-zero I guess and it's not actually committed until it's accessed).</p>
<p>That hopefully makes things a bit more usable!</p>
</blockquote>
<p>It is indeed a significant improvement! Thanks!</p>
<blockquote>
<p>Oh also for imported memories, it's true that right now for copy-on-write-based initialization we do not apply the optimization to imported memories, only to locally defined memories. All of the "interesting" modules we've been optimizing and care about the runtime for all define their own memory and export it, but it's not impossible to support imported memories and if you've got a use case we can look to implement support for imported memories.</p>
</blockquote>
<p>In our use case at this point we're fine with the way it is currently. We need to support WASM blobs of either type, but we can just easily patch one into the other if necessary so that the memory's always defined internally and exported.</p>
</blockquote>



<hr><p>Last updated: Jan 20 2025 at 06:04 UTC</p>
</html>