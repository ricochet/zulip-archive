<html>
<head><meta charset="utf-8"><title>wasmtime / Issue #2296 Cranelift AArch64: Support the SIM... · git-wasmtime · Zulip Chat Archive</title></head>
<h2>Stream: <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/index.html">git-wasmtime</a></h2>
<h3>Topic: <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html">wasmtime / Issue #2296 Cranelift AArch64: Support the SIM...</a></h3>

<hr>

<base href="https://bytecodealliance.zulipchat.com">

<head><link href="https://bytecodealliance.github.io/zulip-archive/style.css" rel="stylesheet"></head>

<a name="213627526"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/213627526" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#213627526">(Oct 17 2020 at 00:46)</a>:</h4>
<p>akirilov-arm opened <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<p>The bitmask extraction operations (<a href="https://github.com/WebAssembly/simd/issues/201">WebAssembly/simd#201</a>) have already become a <a href="https://github.com/WebAssembly/simd/blob/master/proposals/simd/SIMD.md#bitmask-extraction">part</a> of the fixed-width SIMD proposal, so we should definitely implement them in Cranelift's AArch64 backend (and elsewhere, of course, but that's a separate discussion), so this issue is going to concentrate on implementation approaches, and in particular the instruction sequences that will be used. That is probably going to be the most challenging part because those operations don't have a straightforward mapping into the A64 instruction set.</p>
<p>Let's start with <code>i8x16.bitmask</code>, since it is the most complicated one - the suggested lowering from the proposal is:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">sshr</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vs</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mi">7</span><span class="w"></span>
<span class="w">  </span><span class="n">ldr</span><span class="w">   </span><span class="n">qt2</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">and</span><span class="w">   </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">ext</span><span class="w">   </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mi">8</span><span class="w"></span>
<span class="w">  </span><span class="n">zip1</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addv</span><span class="w">  </span><span class="n">ht</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">8</span><span class="n">h</span><span class="w"></span>
<span class="w">  </span><span class="n">umov</span><span class="w">  </span><span class="n">wd</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>

<p>where <code>Vs</code> is the input vector register, <code>Wd</code> is the output general-purpose register, and <code>Vt</code> and <code>Vt2</code> - temporary vector registers.</p>
<p>The main issue with this instruction sequence is that it uses the horizontal reduction <code>ADDV</code>, which is quite expensive on some microarchitectures, so here's an alternative:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">cmlt</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vs</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mi">0</span><span class="w"></span>
<span class="w">  </span><span class="n">ldr</span><span class="w">   </span><span class="n">qt2</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">and</span><span class="w">   </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">umov</span><span class="w">  </span><span class="n">wd</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>

<p>We also use <code>CMLT #0</code> instead of <code>SSHR</code>, since it has better throughput in some cases, but, of course, the best approach would be to avoid it completely. AFAIK the expected usage of the bitmask extraction operations is to work on vector comparison results, which are already in the required form (the bits in each lane are either all 0 or 1), so we could avoid the initial instruction by clever IR pattern matching (bare minimum would be <code>maybe_input_insn(ctx, inputs[0], Opcode::Icmp)</code> and so on).</p>
<p>I wrote a microbenchmark for each instruction sequence by putting it in a loop and completing it with a <code>dup vs.16b, wd</code>, which establishes a loop-carried dependency and makes it possible to measure latency. Here are the results for various microarchitectures in terms of speedup achieved by the alternative sequence, i.e. higher numbers mean that the latter is faster (has lower latency):</p>
<table>
<thead>
<tr>
<th>CPU core</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ampere eMAG</td>
<td>114.71%</td>
</tr>
<tr>
<td>Arm Cortex-A53</td>
<td>90.49%</td>
</tr>
<tr>
<td>Arm Cortex-A72</td>
<td>114.35%</td>
</tr>
<tr>
<td>Arm Cortex-A73</td>
<td>112.50%</td>
</tr>
<tr>
<td>Arm Neoverse N1</td>
<td>126.66%</td>
</tr>
</tbody>
</table>
<p>The results are based on median runtimes from 20 runs; standard errors were 0.02% or less, with one exception on the Neoverse N1, which was 1.95% because the very first run took significantly longer than the rest.</p>
<p>Unfortunately, there isn't a clear winner - the alternative instruction sequence is faster on big cores, while the one from the proposal is faster on the little core, i.e. Cortex-A53. However, the speedups achieved on the big cores are greater than the slowdown experienced on the little core.</p>
<p>Future architectural extensions could enable further simplification; in particular, the bit permute instructions that are part of the second version of the Scalable Vector Extension (SVE2) may reduce the number of instructions and avoid the literal load at the same time.</p>
<p>cc @julian-seward1</p>
</blockquote>



<a name="213638527"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/213638527" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#213638527">(Oct 17 2020 at 06:44)</a>:</h4>
<p>julian-seward1 <a href="https://github.com/bytecodealliance/wasmtime/issues/2296#issuecomment-710762097">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<p>Anton, thank you for making these measurements.  It's a shame that it isn't a clear win on all targets.  One other variant that I'd like to ask about is formation of the magic constant without doing a load.  Currently what SpiderMonkey has implemented is formation of the constant like this:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">                    </span><span class="c1">// mov   tmp_gpr, #0x0201</span>
<span class="w">                    </span><span class="c1">// movk  tmp_gpr, #0x0804, lsl 16</span>
<span class="w">                    </span><span class="c1">// movk  tmp_gpr, #0x2010, lsl 32</span>
<span class="w">                    </span><span class="c1">// movk  tmp_gpr, #0x8040, lsl 48</span>
<span class="w">                    </span><span class="c1">// dup   tmp_vec.2d, tmp_gpr</span>
</code></pre></div>

<p>I remember reading somewhere (in one of the optimisation guides) that loads have a 3- or 4-cycle latency.  Whereas <code>mov</code>/<code>movk</code> arranged like this can dual-issue in pairs, hence producing the <code>tmp_gpr</code> value in two cycles.  If the <code>dup</code> can execute in one or two cycles then it's not a loss, and has the advantage of not causing extra memory traffic.  But I'm just guessing here.  What do you think?</p>
</blockquote>



<a name="213787718"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/213787718" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#213787718">(Oct 19 2020 at 13:49)</a>:</h4>
<p>akirilov-arm <a href="https://github.com/bytecodealliance/wasmtime/issues/2296#issuecomment-712169950">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<p>I am actually working on improving vector constant materialization right now, and in particular on replicated patterns. While I am thinking mostly about using <code>MOVI</code>/<code>MVNI</code>/<code>FMOV</code> (immediate form), I can add this to what I am focusing on. You are right about the load timing and the possibility of dual-issue, but that is still a chain of dependent instructions, so I'd like to get some microbenchmark data first. The other issue is that in contrast with the literal load approach it is impossible to share the constant value with other instances of the bitmask extraction operation. Yes, we don't support proper constant pools in the AArch64 backend yet (refer to #1549), so sharing is not possible anyway, but that may change in the future.</p>
<p>My suggestion is to use <code>lower_constant_f128()</code> and then we can tackle this separately.</p>
</blockquote>



<a name="214364182"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/214364182" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#214364182">(Oct 23 2020 at 20:48)</a>:</h4>
<p>yurydelendik <a href="https://github.com/bytecodealliance/wasmtime/issues/2296#issuecomment-715582584">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<blockquote>
<p>My suggestion is to use lower_constant_f128() and then we can tackle this separately.</p>
</blockquote>
<p>My understanding that the <code>lower_constant_f128</code> has jump instruction, which might affect the benchmark mentioned in the description. Is it possible to re-run this benchmark that includes loading from <code>pc +</code> and <code>B</code>?</p>
</blockquote>



<a name="214366957"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/214366957" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#214366957">(Oct 23 2020 at 21:12)</a>:</h4>
<p>akirilov-arm <a href="https://github.com/bytecodealliance/wasmtime/issues/2296#issuecomment-715592497">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<p>In PR #2310 <code>lower_constant_f128()</code> no longer generates a literal load for replicated 64-bit patterns and uses the sequence of moves above instead, which is part of the reason I suggested it (I expected to push my changes shortly afterwards, which I did).</p>
<p>The idea is actually to concentrate the handling of constants in a single place, so that we can easily improve code generation in the future. We can also add special cases inside <code>lower_constant_f128()</code> for well-known values like the bitmask extraction ones.</p>
<p>I intend to run some benchmarks again, but they will be slightly different because we are no longer interested in the approach to calculate the bitmask, but in the best way to materialize the magic constants (which is an orthogonal issue).</p>
</blockquote>



<a name="214366995"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/214366995" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#214366995">(Oct 23 2020 at 21:13)</a>:</h4>
<p>akirilov-arm edited a <a href="https://github.com/bytecodealliance/wasmtime/issues/2296#issuecomment-715592497">comment</a> on <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<p>In PR #2310 <code>lower_constant_f128()</code> no longer generates a literal load for replicated 64-bit patterns and uses the sequence of moves above instead, which was part of the reason I suggested it (I expected to push my changes shortly afterwards, which I did).</p>
<p>The idea is actually to concentrate the handling of constants in a single place, so that we can easily improve code generation in the future. We can also add special cases inside <code>lower_constant_f128()</code> for well-known values like the bitmask extraction ones.</p>
<p>I intend to run some benchmarks again, but they will be slightly different because we are no longer interested in the approach to calculate the bitmask, but in the best way to materialize the magic constants (which is an orthogonal issue).</p>
</blockquote>



<a name="224167589"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/224167589" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#224167589">(Jan 27 2021 at 11:20)</a>:</h4>
<p>akirilov-arm <a href="https://github.com/bytecodealliance/wasmtime/issues/2296#issuecomment-768218260">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<p>I have finally managed to run several microbenchmarks for the constant materialization. My base case is the sequence of moves above:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">mov</span><span class="w"> </span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mh">0x0201</span><span class="w"></span>
<span class="w">  </span><span class="n">movk</span><span class="w"> </span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mh">0x0804</span><span class="p">,</span><span class="w"> </span><span class="n">lsl</span><span class="w"> </span><span class="err">#</span><span class="mi">16</span><span class="w"></span>
<span class="w">  </span><span class="n">movk</span><span class="w"> </span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mh">0x2010</span><span class="p">,</span><span class="w"> </span><span class="n">lsl</span><span class="w"> </span><span class="err">#</span><span class="mi">32</span><span class="w"></span>
<span class="w">  </span><span class="n">movk</span><span class="w"> </span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mh">0x8040</span><span class="p">,</span><span class="w"> </span><span class="n">lsl</span><span class="w"> </span><span class="err">#</span><span class="mi">48</span><span class="w"></span>
<span class="w">  </span><span class="n">dup</span><span class="w"> </span><span class="n">vd</span><span class="p">.</span><span class="mi">2</span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="n">xt</span><span class="w"></span>
</code></pre></div>
<p>The first alternative is a straight literal load, which I'll call <code>literal</code>:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">ldr</span><span class="w"> </span><span class="n">qd</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>The second one is a literal load, but only of 64 bits (since we are dealing with a replicated pattern), followed by a move from the first vector element to the other (<code>literal_64</code>):</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">ldr</span><span class="w"> </span><span class="n">dd</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">mov</span><span class="w"> </span><span class="n">vd</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">vd</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>And the third one is similar to the second one, but uses a replicated load instead (so it's called <code>literal_replicate</code>) and requires a temporary register - the instruction count stays the same because <code>LD1R</code> lacks a literal addressing mode:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">adr</span><span class="w"> </span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">ld1r</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">vd</span><span class="p">.</span><span class="mi">2</span><span class="n">d</span><span class="w"> </span><span class="p">},</span><span class="w"> </span><span class="p">[</span><span class="n">xt</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>Each of the alternatives also has a variant in which the literal is positioned immediately after the load, so it has to be branched over - I'll add a <code>_branch</code> suffix to denote it. For example, here's what <code>literal_64_branch</code> looks like:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">ldr</span><span class="w"> </span><span class="n">dd</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="n">f</span><span class="w"></span>
<span class="w">  </span><span class="n">b</span><span class="w"> </span><span class="mi">3</span><span class="n">f</span><span class="w"></span>
<span class="mi">2</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
<span class="mi">3</span>:
  <span class="nc">mov</span><span class="w"> </span><span class="n">vd</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">vd</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
</code></pre></div>
<p>Note that it is not really possible to establish a dependency chain between loop iterations, so I have done an essentially throughput (not latency) measurement, especially on out-or-order processors.</p>
<p>Here are the results for various microarchitectures in terms of speedup achieved by the alternatives, i.e. higher numbers mean that the latter are faster (have higher throughput):</p>
<table>
<thead>
<tr>
<th>CPU core</th>
<th>literal speedup</th>
<th>literal_64 speedup</th>
<th>literal_replicate speedup</th>
<th>literal_branch speedup</th>
<th>literal_64_branch speedup</th>
<th>literal_replicate_branch speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ampere eMAG</td>
<td>109.40%</td>
<td>163.48%</td>
<td>164.41%</td>
<td>51.73%</td>
<td>53.59%</td>
<td>54.91%</td>
</tr>
<tr>
<td>Arm Cortex-A53</td>
<td>249.95%</td>
<td>125.03%</td>
<td>249.72%</td>
<td>166.17%</td>
<td>110.51%</td>
<td>124.40%</td>
</tr>
<tr>
<td>Arm Cortex-A55</td>
<td>150.33%</td>
<td>75.17%</td>
<td>300.00%</td>
<td>99.71%</td>
<td>66.41%</td>
<td>74.92%</td>
</tr>
<tr>
<td>Arm Cortex-A72</td>
<td>200.23%</td>
<td>200.23%</td>
<td>150.00%</td>
<td>66.59%</td>
<td>57.10%</td>
<td>57.10%</td>
</tr>
<tr>
<td>Arm Neoverse N1</td>
<td>346.51%</td>
<td>346.51%</td>
<td>231.01%</td>
<td>77.00%</td>
<td>65.93%</td>
<td>65.93%</td>
</tr>
</tbody>
</table>
<p>The results are based on median runtimes from 20 runs; standard errors are 0.21% or less.</p>
<p>Overall, the literal load approaches without a branch are a clear win, with some microarchitectures (especially the little ones like A53 and A55) preferring the variants that transfer only 64 bits from memory like <code>literal_replicate</code> (explained by the fact that A53 and A55 have a 64-bit read path from the data L1 cache). On the other hand, if execution must branch over the literal, then there is almost always a (significant) regression, so a sequence of moves is preferable.</p>
</blockquote>



<a name="225105155"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/225105155" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#225105155">(Feb 04 2021 at 00:43)</a>:</h4>
<p>akirilov-arm labeled <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<p>The bitmask extraction operations (<a href="https://github.com/WebAssembly/simd/issues/201">WebAssembly/simd#201</a>) have already become a <a href="https://github.com/WebAssembly/simd/blob/master/proposals/simd/SIMD.md#bitmask-extraction">part</a> of the fixed-width SIMD proposal, so we should definitely implement them in Cranelift's AArch64 backend (and elsewhere, of course, but that's a separate discussion), so this issue is going to concentrate on implementation approaches, and in particular the instruction sequences that will be used. That is probably going to be the most challenging part because those operations don't have a straightforward mapping into the A64 instruction set.</p>
<p>Let's start with <code>i8x16.bitmask</code>, since it is the most complicated one - the suggested lowering from the proposal is:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">sshr</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vs</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mi">7</span><span class="w"></span>
<span class="w">  </span><span class="n">ldr</span><span class="w">   </span><span class="n">qt2</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">and</span><span class="w">   </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">ext</span><span class="w">   </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mi">8</span><span class="w"></span>
<span class="w">  </span><span class="n">zip1</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addv</span><span class="w">  </span><span class="n">ht</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">8</span><span class="n">h</span><span class="w"></span>
<span class="w">  </span><span class="n">umov</span><span class="w">  </span><span class="n">wd</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>where <code>Vs</code> is the input vector register, <code>Wd</code> is the output general-purpose register, and <code>Vt</code> and <code>Vt2</code> - temporary vector registers.</p>
<p>The main issue with this instruction sequence is that it uses the horizontal reduction <code>ADDV</code>, which is quite expensive on some microarchitectures, so here's an alternative:</p>
<div class="codehilite" data-code-language="Rust"><pre><span></span><code><span class="w">  </span><span class="n">cmlt</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vs</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="mi">0</span><span class="w"></span>
<span class="w">  </span><span class="n">ldr</span><span class="w">   </span><span class="n">qt2</span><span class="p">,</span><span class="w"> </span><span class="p">.</span><span class="n">Lmask</span><span class="w"></span>
<span class="w">  </span><span class="n">and</span><span class="w">   </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt2</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">addp</span><span class="w">  </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="mi">16</span><span class="n">b</span><span class="w"></span>
<span class="w">  </span><span class="n">umov</span><span class="w">  </span><span class="n">wd</span><span class="p">,</span><span class="w"> </span><span class="n">vt</span><span class="p">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="o">..</span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="n">Lmask</span>:
  <span class="p">.</span><span class="n">byte</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="w"></span>
</code></pre></div>
<p>We also use <code>CMLT #0</code> instead of <code>SSHR</code>, since it has better throughput in some cases, but, of course, the best approach would be to avoid it completely. AFAIK the expected usage of the bitmask extraction operations is to work on vector comparison results, which are already in the required form (the bits in each lane are either all 0 or 1), so we could avoid the initial instruction by clever IR pattern matching (bare minimum would be <code>maybe_input_insn(ctx, inputs[0], Opcode::Icmp)</code> and so on).</p>
<p>I wrote a microbenchmark for each instruction sequence by putting it in a loop and completing it with a <code>dup vs.16b, wd</code>, which establishes a loop-carried dependency and makes it possible to measure latency. Here are the results for various microarchitectures in terms of speedup achieved by the alternative sequence, i.e. higher numbers mean that the latter is faster (has lower latency):</p>
<table>
<thead>
<tr>
<th>CPU core</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ampere eMAG</td>
<td>114.71%</td>
</tr>
<tr>
<td>Arm Cortex-A53</td>
<td>90.49%</td>
</tr>
<tr>
<td>Arm Cortex-A72</td>
<td>114.35%</td>
</tr>
<tr>
<td>Arm Cortex-A73</td>
<td>112.50%</td>
</tr>
<tr>
<td>Arm Neoverse N1</td>
<td>126.66%</td>
</tr>
</tbody>
</table>
<p>The results are based on median runtimes from 20 runs; standard errors were 0.02% or less, with one exception on the Neoverse N1, which was 1.95% because the very first run took significantly longer than the rest.</p>
<p>Unfortunately, there isn't a clear winner - the alternative instruction sequence is faster on big cores, while the one from the proposal is faster on the little core, i.e. Cortex-A53. However, the speedups achieved on the big cores are greater than the slowdown experienced on the little core.</p>
<p>Future architectural extensions could enable further simplification; in particular, the bit permute instructions that are part of the second version of the Scalable Vector Extension (SVE2) may reduce the number of instructions and avoid the literal load at the same time.</p>
<p>cc @julian-seward1</p>
</blockquote>



<a name="234077784"></a>
<h4><a href="https://bytecodealliance.zulipchat.com#narrow/stream/225357-git-wasmtime/topic/wasmtime%20/%20Issue%20%232296%20Cranelift%20AArch64%3A%20Support%20the%20SIM.../near/234077784" class="zl"><img src="https://bytecodealliance.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Wasmtime GitHub notifications bot <a href="https://bytecodealliance.github.io/zulip-archive/stream/225357-git-wasmtime/topic/wasmtime.20.2F.20Issue.20.232296.20Cranelift.20AArch64.3A.20Support.20the.20SIM.2E.2E.2E.html#234077784">(Apr 11 2021 at 21:29)</a>:</h4>
<p>aqrit <a href="https://github.com/bytecodealliance/wasmtime/issues/2296#issuecomment-817375626">commented</a> on <a href="https://github.com/bytecodealliance/wasmtime/issues/2296">Issue #2296</a>:</p>
<blockquote>
<p>Shift and accumulate instructions could be used instead of a constant mask.<br>
<a href="https://stackoverflow.com/a/58381188/5691983">https://stackoverflow.com/a/58381188/5691983</a><br>
<a href="https://bugs.llvm.org/show_bug.cgi?id=49577">https://bugs.llvm.org/show_bug.cgi?id=49577</a></p>
</blockquote>



<hr><p>Last updated: Jan 20 2025 at 06:04 UTC</p>
</html>